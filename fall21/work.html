<!DOCTYPE doctype PUBLIC "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
  <meta http-equiv="Content-Type"
 content="text/html; charset=iso-8859-1">
  <title>Coursework, CSci 39542: Data Science, Hunter College</title>
</head>
<STYLE>A {text-decoration: none;}
th, td { padding: 5px; }
code {
  background-color: #eeeeee;
}
.inline {
  padding: 1px;
}
.blockcode {
  border: 1px solid #999999;
  display: block;
  padding-left: 10px;
  padding-top : 2px;
  padding-bottom : 2px;
  margin: 5px;
}
.datablock {
  border: 1px solid #eeeeee;
  display: block;
  padding: 7px;
  padding-top : 0px;
  margin: 5px;
}
</STYLE>
<body>


<div style="margin: 15px;width=100%;">
    <span style= "float: left;font-size:larger"><a href="index.html">CSci 39542</a></span>
    <span style= "float: right">
      <a href="syl.html">Syllabus</a>&nbsp;&nbsp;&nbsp;
      <a href="resources.html">Resources</a>&nbsp;&nbsp;&nbsp;
      <a href="work.html">Coursework</a><!--&nbsp;&nbsp;&nbsp;
      <a href="faq.html">FAQ</a>-->
    </span>
</div>

<br>
<br>
<hr>

<div style="margin:50px">


<h2>Coursework
  <br>CSci 39542: Introduction to Data Science<br>
<a href="http://www.hunter.</i>cuny.edu/csci">Department of Computer Science</a><br>
<a href="https://hunter.</i>cuny.edu">Hunter College</a>, <a href="https://www.cuny.edu">City University of New York</a><br>
Fall 2021<br><br>
</h2>


<hr>
<a href="work.html#quizzes">Quizzes</a>&nbsp;&nbsp;&nbsp;
<a href="work.html#hw">Homework</a>&nbsp;&nbsp;&nbsp;
<a href="work.html#project">Project</a>&nbsp;&nbsp;&nbsp;
<hr>

<a name="quizzes">
<h2>Quizzes</h2>
</a>
Unless otherwise noted, quizzes are available on Blackboard for the 24 hours after lecture.  Blackboard quizzes are 15 minutes long and can be repeated up to the deadline.  The highest score earned on a Blackboard quiz will be reported.  Blackboard access is generated automatically from the registrar.  See the ICIT <a href="http://www.hunter.</i>cuny.edu/it/blackboard/student-documentation-and-support-for-blackboard">Blackboard</a> page for resources and tutorials for using the system.

<p> Five of the quizzes assess your programming skill using HackerRank.  These quizzes are 30 minutes long and cannot be repeated.  Links will be available on Blackboard to access the quiz.

<p>
There are no make-up quizzes. Instead, your score on the final exam will replace missing quiz grades (the final exam will also replace a quiz grade when you take the quiz but do better on the final exam).  See the <a href="syl.html">syllabus</a> for additional information on how grades are calculated.

<p><a name="q1"><b>Quiz 1: </b> &emsp; <i>Due 4pm, Friday, 27 August.</i>
  The first quiz asks that you confirm that you have read the Hunter College's
  <a href="http://www.hunter.</i>cuny.edu/studentaffairs/student-conduct/academic-integrity/welcome-academic-integrity/?searchterm=integrity">Academic Integrity Policy</a>:
  <blockquote>
  <i>
  Hunter College regards acts of academic dishonesty (e.g., plagiarism, cheating on examinations, obtaining unfair advantage, and falsification of records and official documents) as serious offenses against the values of intellectual honesty. The College is committed to enforcing the CUNY Policy on Academic Integrity and will pursue cases of academic dishonesty according to the Hunter College Academic Integrity Procedures.</i>
  </blockquote>
</p>

<p><a name="q2"><b>Quiz 2: </b></b> &emsp; <i>Due 4pm, Tuesday, 31 August. </i>&emsp;
  The second quiz focuses on the Python Recap: basics and standard packages (pandas, numpy,  matplotlib, & seaborn), zips, and list comprehensions from Lecture 1.</p>

<p><a name="q3"><b>Quiz 3: </b></b> &emsp; <i>Due 4pm, Friday, 3 September.</i>&emsp;
  The quiz covers data sampling from the third lecture and the reading: <a href="http://www.textbook.ds100.org/ch/03/theory_intro.html">DS 100: Chapter 2</a> (Theory for Data Design) and includes Python review questions.</p>

<p><a name="q4"><b>Quiz 4: </b></b> &emsp; <i>Due 4pm, Friday, 10 September.</i>&emsp;
  The quiz covers Python string methods and Python data types from the second and third lectures and the reading: <a href="http://www.textbook.ds100.org/ch/13/text_strings.html">DS 100: Section 13.1</a> (Python String Methods)
  and subsetting dataframes from <a href="http://www.textbook.ds100.org/ch/07/pandas_intro.html">DS 100: Chapter 7</a> (Data Tables in Python).
</p>

<strike>
<p><a name="q5"><b>Quiz 5: </b></b> &emsp; <i>Due 4pm, Tuesday, 14 September.</i>&emsp;
  This is a <b>coding quiz on HackerRank</b> focusing on the Python constructs and package from the first two weeks. You will be sent an invitation to the email you use for Gradescope for this quiz.  You have 30 minutes to complete the quiz, and the quiz cannot be repeated.
</strike>

<p><a name="q6"><b>Quiz 6: </b></b> &emsp; <i>Due 4pm, Tuesday, 21 September.</i>&emsp;
  Today's topic is regular expressions from Lecture #3 and <a href="http://www.textbook.ds100.org/ch/13/text_regex.html">DS 100: Sections 13.2-13.3</a> (Regular Expressions).</p>

<p><a name="q7"><b>Quiz 7: </b></b> &emsp; <i>Due 4pm, Friday, 24 September.</i>&emsp;
  This quiz covers SQL from Lectures #4, 5 & 6 and <a href="http://www.textbook.ds100.org/ch/07/sql_intro.html">DS 100: Chapter 7</a> (Relational Databases & SQL).</p>

<p><a name="q8"><b>Quiz 8: </b></b> &emsp; <i>Due 4pm, Tuesday, 28 September.</i>&emsp;
  This quiz covers DataFrames from Pandas, covered in Lectures #4, 5 & 6 and <a href="http://www.textbook.ds100.org/ch/06/pandas_intro.html">DS 100: Chapter 6</a> (Data Tables in Python).</p>

<p><a name="q9"><b>Quiz 9: </b></b> &emsp; <i>Due 4pm, Friday, 1 October.</i>&emsp;
    The focus is functions in Python, covered in the code demos in Lectures #5 and #6.</p>

<p><a name="q10"><b>Quiz 10: </b></b> &emsp; <i>Due 4pm, Tuesday, 5 October.</i>&emsp;
  This is a <b>coding quiz on HackerRank</b>. You will be sent an invitation to the email you use for Gradescope for this quiz.  You have 30 minutes to complete the quiz, and the quiz cannot be repeated.</p>

<p><a name="q11"><b>Quiz 11: </b> &emsp; <i>Due 4pm, Friday, 8 October.</i>&emsp;
  The focus in on data visualiation as discussed in Lectures #7, #8 and #9 and <a href="http://www.textbook.ds100.org/ch/11/viz_intro.html">DS 100: Chapter 11</a> (Data Visualization).</p>

<p><a name="q12"><b>Quiz 12: </b> &emsp; <i>Due 4pm, Friday, 15 October.</i>&emsp;
  The quiz covers loss functions, correlation, and regression from Lectures #10, #11 & #12 and the reading: <a href=
  "http://www.textbook.ds100.org/ch/04/modeling_loss_functions.html">DS 100, Sections 4.2-4.4</a> (Loss Functions).



<p><a name="q13"><b>Quiz 13: </b> &emsp; <i>Due 4pm, Tuesday, 19 October.</i>&emsp;
  The focus of this quiz is sampling distributions and the Central Limit Theorem, covered in Lectures #11-12,
  <a href="https://inferentialthinking.com/chapters/09/Randomness.html">DS 8: Chapter 9</a> (Randomness), and
    <a href="http://www.textbook.ds100.org/ch/16/prob_and_gen.html">DS 100: Chapter 16</a> (Probability & Generalization).
</p>

<p><a name="q14"><b>Quiz 14: </b> &emsp; <i>Due 4pm, Friday, 22 October.</i>&emsp;
  The quiz covers linear models, ordinary least squares, and gradient descent from Lectures #11-13 and the reading: <a href= "http://www.textbook.ds100.org/ch/17/gradient_descent.html">DS 100: Chapter 17</a> (Gradient Descent).

</p>
<p><a name="q15"><b>Quiz 15: </b> &emsp; <i>Due 4pm, Tuesday, 26 October.</i>&emsp;
  This is a <b>coding quiz on HackerRank</b> focusing on regular expressions. A link to access the quiz is available on Blackboard 24 hours before the due date.  You have 30 minutes to complete the quiz, and the quiz cannot be repeated.
</p>


<p><a name="q16"><b>Quiz 16: </b> &emsp; <i>Due 4pm, Friday, 29 October.</i>&emsp;
    The quiz reviews topics from the first 15 lectures.
</p>

<p><a name="q17"><b>Quiz 17: </b> &emsp; <i>Due 4pm, Tuesday, 2 November.</i>&emsp;
  The quiz covers the feature engineering from Lectures #15-16 and the reading: <a href= "http://www.textbook.ds100.org/ch/20/feature_engineering.html">DS 100: Chapter 20</a> (Feature Engineering).

</p>
<p><a name="q18"><b>Quiz 18: </b> &emsp; <i>Due 4pm, Friday, 5 November.</i>&emsp;
The quiz covers the logistic model from Lectures #17-18 and the reading: <a href= "http://www.textbook.ds100.org/ch/24/classification_intro.html">DS 100: Chapter 20</a> (Classification).
</p>

<p><a name="q19"><b>Quiz 19: </b> &emsp; <i>Due 4pm, Tuesday, 9 November.</i>&emsp;
  The quiz covers the logistic regression from Lectures #17-19 and the reading: <a href= "http://www.textbook.ds100.org/ch/24/classification_intro.html">DS 100: Chapter 20</a> (Classification).

</p>
<p><a name="q20"><b>Quiz 20: </b> &emsp; <i>Due 4pm, Friday, 12 November.</i>&emsp;
    This is a <b>coding quiz on HackerRank</b>. A link to access the quiz is available on Blackboard 24 hours before the due date.  You have 30 minutes to complete the quiz, and the quiz cannot be repeated.  You have 30 minutes to complete the quiz, and the quiz cannot be repeated.<p>
</p>

<p><a name="q21"><b>Quiz 21: </b> &emsp; <i>Due 4pm, Tuesday, 16 November.</i>&emsp;
    The quiz covers classification from Lectures #17-21 and the reading: <a href= "http://www.textbook.ds100.org/ch/24/classification_intro.html">DS 100: Chapter 20</a> (Classification) and <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html">Python DS Handbook Chapter 5</a> (SVMs).
</p>
<p><a name="q22"><b>Quiz 22: </b> &emsp; <i>Due 4pm, Friday, 19 November.</i>&emsp;
    The quiz covers the linear algebra review from Lecture #21.
</p>
<p><a name="q23"><b>Quiz 23: </b> &emsp; <i>Due 4pm, Tuesday, 23 October.</i>&emsp;
    <strike>The quiz focuses on Principal Components Analysis from Lectures #21-23 and the reading: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html">Python Data Science Handbook: Section 5.9</a> (PCA).</strike>  Since we did not cover all the material yet, this quiz will now be review topics from Lectures 1-20.
</p>

<p><a name="q24"><b>Quiz 24: </b> &emsp; <i>Due 4pm, Tuesday, 30 November.</i>&emsp;
    The quiz covers multidimensional scaling and dimensionality reduction from Lectures #22-23 and the reading:       <a href="https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py">Manifold Learning</a> (sklearn).
</p>

<p><a name="q25"><b>Quiz 25: </b> &emsp; <i>Due 4pm, Friday, 3 December.</i>&emsp;
    This is a <b>coding quiz on HackerRank</b>. You will be sent an invitation to the email you use for Gradescope for this quiz.  You have 30 minutes to complete the quiz, and the quiz cannot be repeated.<p>
</p>

<p><a name="q26"><b>Quiz 26: </b> &emsp; <i>Due 4pm, Tuesday, 7 December.</i>&emsp;
    The quiz covers the K-Means Clustering from Lectures #24-26 and the reading: 	<td><a href="http://www.textbook.ds100.org/ch/28/clustering_intro.html">DS 100: Chapter 28</a> (clustering)  and     <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html">Python Data Science Handbook: Section 5.9</a> (K-Means).
</p>

<p><a name="q27"><b>Quiz 27: </b> &emsp; <i>Due 4pm, Friday, 9 December.</i>&emsp;
    The quiz reviews topics from the first 26 lectures.
</p>
<p><a name="q28"><b>Quiz 28: </b> &emsp; <i>Due 4pm, Tuesday, 14 December.</i>&emsp;
    The last quiz is an end of the semester survey.
</p>



<!--
<p>
  <i>More to come...</i>
-->
<br><br><br><br>
<hr>

<a name="hw">
<h2>Homework</h2>
</a>
Unless otherwise noted, programs are submitted on the course's Gradescope site and are written in Python.
Also, to receive full credit, the code should be compatible with Python 3.6 (the default for the Gradescope autograders).

<p>All students registered by Monday, 23 August were sent a registration invitation to the email on record on their Blackboard account.  If you did not receive the email or would like to use a different account, post to <code class="inline">Help::Individual Questions</code> (on the left hand menu when logged into the course site on Blackboard).  Include in your post that you not receive a Gradescope invitation, your preferred email, and we will manually generate an invitation.  As a default, we use your name as it appears in Blackboard/CUNYFirst (to update CUNYFirst, see <a href="https://hunter.</i>cuny.edu/students/registration/records-and-transcripts/changing-your-personal-information/">changing your personal information</a>).  If you prefer a different name for Gradescope, include it in your post, and we will update the Gradescope registration.



<p>
To get full credit for a program, the file must include in the opening comment:
<ul>
    <li> Your name, as it appears in your Gradescope registration.
    <li> The email you are using for Gradescope.
    <li> A list of any resources you used for the program.  Include classmates and tutors that you worked with, along with any websites or tutorials that you used.  If you used no resources (other than the class notes and textbooks), then you should include the line:  "No resources used."
</ul>

For example, for the student, Thomas Hunter, the opening comment of his first program might be:
<pre><code class="blockcode">
"""
Name:  Thomas Hunter
Email: thomas.hunter.</i>1870@hunter.</i>cuny.edu
Resources:  Used python.org as a reminder of Python 3 print statements.
"""
</pre></code>
and then followed by his Python program.

<br>
<br>
<br>
<hr>
Set 1:  The first set of programs recaps familiar Python constructs and packages.  None are challenging, instead, their purpose is as review and to ensure your Python IDE is functional, has the basic libraries and that you can submit programs to Gradescope.
<hr>

<p></p><p><a name="p1"><b>Program 1: Hello, world.</b></b> &emsp; <i>Due noon, Friday, 27 August.</i>
  <br>(Learning Objective:  students are able to use a Python IDE on their computer and successfully submit the work to the Gradescope system.)

  <p>Submit a Python program that prints:  <code class = "inline">Hello, world</code>
</p>

<p></p><p><a name="p2"><b>Program 2: Senators' Names.</b></b> &emsp; <i>Due noon, Monday, 30 August.</i>
  <br>(Learning Objective: students can successfully read and write CSV files and use the Pandas package to select rows, filtered by boolean expressions.)

  <p>
Write a program, using the <code class = "inline">pandas</code> package, that asks the user for the name of an input CSV file and the name of an output CSV file.  The program should open the file name provided by the user.
Next, the program should select rows where the field <code class = "inline">senate_class</code> is non-empty and write the <code class = "inline">first_name</code> and <code class = "inline">last_name</code> to a file with the output file name provided by the user.

<p>For example, if the file was <a href="https://theunitedstates.io/congress-legislators/legislators-current.csv">legislators-current.csv</a> with the first 3 lines of:
<pre><code class=datablock>
last_name,first_name,middle_name,suffix,nickname,full_name,birthday,gender,type,state,district,senate_class,party,url,address,phone,contact_form,rss_url,twitter,facebook,youtube,youtube_id,bioguide_id,thomas_id,opensecrets_id,lis_id,fec_ids,cspan_id,govtrack_id,votesmart_id,ballotpedia_id,washington_post_id,icpsr_id,wikipedia_id
Brown,Sherrod,,,,Sherrod Brown,1952-11-09,M,sen,OH,,1,Democrat,https://www.brown.senate.gov,503 Hart Senate Office Building Washington DC 20510,202-224-2315,http://www.brown.senate.gov/contact/,http://www.brown.senate.gov/rss/feeds/?type=all&amp;,SenSherrodBrown,SenatorSherrodBrown,SherrodBrownOhio,UCgy8jfERh-t_ixkKKoCmglQ,B000944,00136,N00003535,S307,"H2OH13033,S6OH00163",5051,400050,27018,Sherrod Brown,,29389,Sherrod Brown
Cantwell,Maria,,,,Maria Cantwell,1958-10-13,F,sen,WA,,1,Democrat,https://www.cantwell.senate.gov,511 Hart Senate Office Building Washington DC 20510,202-224-3441,http://www.cantwell.senate.gov/public/index.cfm/email-maria,http://www.cantwell.senate.gov/public/index.cfm/rss/feed,SenatorCantwell,senatorcantwell,SenatorCantwell,UCN52UDqKgvHRk39ncySrIMw,C000127,00172,N00007836,S275,"S8WA00194,H2WA01054",26137,300018,27122,Maria Cantwell,,39310,Maria Cantwell
</code></pre>

Then a sample run of the program:

<pre><code class="blockcode">Enter input file name: legislators-current.csv
Enter output file name:  senatorNames.csv
</code></pre>

And the first three lines of <code class = "inline">senatorNames.csv</code> would be:
<pre><code class=datablock>
first_name,last_name
Sherrod,Brown
Maria,Cantwell
</code></pre>

Note:  if you use the legislators CSV file above, your output file should have 101 lines:  1 line of header information and 100 rows of data.
<br>

<p></p><p><a name="p3"><b>Program 3: Senators' Ages.</b></b> &emsp; <i>Due noon, Wednesday, 1 September.</i>
  <br>(Learning Objective: to refresh students' knowledge of Pandas' functionality to create new columns from existing columns of formatted data.)</p><p>
Write a program that asks the user for the name of an input CSV file and the name of an output CSV file.  The program should open the file name provided by the user.</i>  Next, the program should select rows where the field <code class = "inline">senate_class</code> is non-empty and write the <code class = "inline">first_name</code> and compute the <code class = "inline">age</code> based on the <code class = "inline">birthday</code> field as of the first of the year.  Your program should write out a new CSV file (with the name provided by the user) with the two columns:  <code class = "inline">first_name</code> and <code class = "inline">age</code>.

<p>For example, if the file was <a href="https://theunitedstates.io/congress-legislators/legislators-current.csv">legislators-current.csv</a> with the first 3 lines of:

<pre><code class="datablock">
last_name,first_name,middle_name,suffix,nickname,full_name,birthday,gender,type,state,district,senate_class,party,url,address,phone,contact_form,rss_url,twitter,facebook,youtube,youtube_id,bioguide_id,thomas_id,opensecrets_id,lis_id,fec_ids,cspan_id,govtrack_id,votesmart_id,ballotpedia_id,washington_post_id,icpsr_id,wikipedia_id
Brown,Sherrod,,,,Sherrod Brown,1952-11-09,M,sen,OH,,1,Democrat,https://www.brown.senate.gov,503 Hart Senate Office Building Washington DC 20510,202-224-2315,http://www.brown.senate.gov/contact/,http://www.brown.senate.gov/rss/feeds/?type=all&amp;,SenSherrodBrown,SenatorSherrodBrown,SherrodBrownOhio,UCgy8jfERh-t_ixkKKoCmglQ,B000944,00136,N00003535,S307,"H2OH13033,S6OH00163",5051,400050,27018,Sherrod Brown,,29389,Sherrod Brown
Cantwell,Maria,,,,Maria Cantwell,1958-10-13,F,sen,WA,,1,Democrat,https://www.cantwell.senate.gov,511 Hart Senate Office Building Washington DC 20510,202-224-3441,http://www.cantwell.senate.gov/public/index.cfm/email-maria,http://www.cantwell.senate.gov/public/index.cfm/rss/feed,SenatorCantwell,senatorcantwell,SenatorCantwell,UCN52UDqKgvHRk39ncySrIMw,C000127,00172,N00007836,S275,"S8WA00194,H2WA01054",26137,300018,27122,Maria Cantwell,,39310,Maria Cantwell
</code></pre>

Then a sample run of the program:
<pre><code class="blockcode">Enter input file name: legislators-current.csv
Enter output file name:  senatorAge.csv
</code></pre>

And the first three lines of <code class="inline">senatorAge.csv</code> would be:
<pre><code class="datablock">
first_name,age
Sherrod,68
Maria,62
</code></pre>
since that was their ages as of the start of the year:  January 1, 2021.
<p>
Note:  if you use the legislators CSV file above, your output file should have 101 lines:  1 line of header information and 100 rows of data.
<br>

<p></p><p><a name="p4"><b>Program 4: ELA Proficiency.</b></b> &emsp; <i>Due noon, Thursday, 2 September.</i>
  <br>(Learning Objective: students can successfully filter formatted data using standard Pandas operations for selecting data.)

  <p>
  Write a program that asks the user for the name of an input CSV file and the name of an output CSV file.  The program should open the file name provided by the user.
  Next, the program should select rows where the field <code class="inline">Grade</code> is equal to 3 and the <code class="inline">Year</code> is equal to 2019 and write all rows that match that criteria to a new CSV file.

  <p>
  Then a sample run of the program:
<pre><code class="blockcode">Enter input file name: school-ela-results-2013-2019.csv
Enter output file name:  ela2013.csv
</code></pre>
  where the file <code class="inline">school-ela-results-2013-2019.csv</code> is extracted from <a href="https://infohub.nyced.org/reports/academics/test-results">NYC Schools Test Results</a> (and <a href="school_ELA_2013_2019_truncated.csv">truncated version</a> of roughly the first 1000 lines for testing).  The first lines of the output file would be:

<pre><code class="datablock">
School,Name,Grade,Year,Category,Number Tested,Mean Scale Score,# Level 1,% Level 1,# Level 2,% Level 2,# Level 3,% Level 3,# Level 4,% Level 4,# Level 3+4,% Level 3+4
01M015,P.S. 015 ROBERTO CLEMENTE,3,2019,All Students,27,606,1,3.7,7,25.9,18,66.7,1,3.7,19,70.4
01M019, P.S. 019 ASHER LEVY,3,2019,All Students,24,606,0,0.0,8,33.3,15,62.5,1,4.2,16,66.7
01M020,P.S. 020 ANNA SILVER,3,2019,All Students,57,593,13,22.8,24,42.1,18,31.6,2,3.5,20,35.1
</code></pre>

<br>
<br>
<br>
<hr>
  Set 2:  The second set of programs focuses on incorporating and analyzing rectangular data, in terms of relational databases and data frames.  The goal is familiarity with these canonical representations to use as building blocks for future analysis, programs, and your project.
<hr>

<p></p><p><a name="p5"><b>Program 5: URL Collection.</b></b> &emsp; <i>Due noon, Friday, 3 September.</i>
  <br>(Learning Objective: to use regular expressions with simple patterns to filter column data in a canonical example: scraping a website of URL's.)

  <p>
  Write a program that asks the user for the name of an input HTML file and the name of an output CSV file.  Your program should use regular expressions (see <a href="http://www.textbook.ds100.org/ch/12/text_re.html">Chapter 12.4</a> for using the <code class="inline">re</code> package in Python) to find all links in the input file and store the link text and URL as columns:  <code class="inline">Title</code> and <code class="inline">URL</code> in the CSV file specified by the user.  For the URL, strip off the leading <code class="inline">https://</code> or <code class="inline">http://</code> and any trailing slashes (<code class="inline">/</code>):

  <p>For example, if the input file is:

<pre><code class="datablock">
  &lt;html>
  &lt;head>&lt;title>Simple HTML File&lt;/title>&lt;/head>

  &lt;body>
    &lt;p> Here's a link for &lt;a href="http://www.hunter.</i>cuny.edu/csci">Hunter CS Department&lt;/a>
    and for &lt;a href="https://stjohn.github.io/teaching/data/fall21/index.html">CSci 39542&lt;/a>.  &lt;/p>

    &lt;p> And for &lt;a href="https://www.google.com/">google&lt;/a>
  &lt;/body>
  &lt;/html>
</code></pre>

  Then a sample run of the program:
<pre><code class="blockcode">Enter input file name: simple.html
Enter output file name:  links.csv
</code></pre>

  And the <code class="inline">links.csv</code> would be:
<pre><code class="datablock">
Title,URL
Hunter CS Department,www.hunter.</i>cuny.edu/csci
CSci 39542,stjohn.github.io/teaching/data/fall21/index.html
google,www.google.com</code></pre>
</p>
  <br>

</p>

<i>Program 6 is cancelled.  See announcement on Blackboard.</i>
<strike>
<p></p><p><a name="p6"><b>Program 6: Regex on Restaurant Inspection Data.</b></b> &emsp; <i>Due noon, Thursday, 9 September.</i>
  <br>(Learning Objective: The two learning objectives of this exercise are a) to give the students an opportunity to practice their newfound regular expressions (regex) skills and b) familiarize them with the restaurant inspection dataset which would be used again in the latter SQL programs.)

  <p>
  Use regular expressions (covered in Lecture 3 & <a href="http://www.textbook.ds100.org/ch/12/text_regex.html">DS 100: Sections 12.2-3</a>) to clean <a href="https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j/data">restaurant inspection</a> datasets that we will use in later SQL programs.

  <!--
  <i>Add in small dataset to use for testing</i>
  Add in sample run: either as functions or as the resulting CSV.
  Process:
1. Take input csv_file_name and output_file_name,
2. run requested operations on the input data and save it into a csv file under the output_file_name,
3. save work into new csv.
  -->
  Your program should:
  <ul>
    <li> Ask the user for names of an input file and output file.  You can assume that the input file is a CSV file with column names from the <a href="https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j/data">restaurant inspection</a> datasets.  Your program should open the named CSV input file and save the results in the named output file.

    <li>There are many invalid phone numbers in the data set. Instead of 10 digits, e.g. <code class="inline">2125551212</code>, some entries have extra characters and not enough digits, e.g. <code class="inline">212-555-12</code>.  First, use regex to check that 10 digits were entered.  If not, set the value to <code class="inline">""</code> (empty string) since the current data is corrupt.  If the phone number does have 10 digits, use regex to parse out the given 10 digit phone number <code class="inline">DDDDDDDDDD</code> and re-concatenate it back into a 11 digit format: 1-digit country code (as 1), 3-digit area code, and a 7-digit telephone number, so that the final output looks like: <code class="inline">+1-DDD-DDD-DDDDD</code>.

    <li> With the inspection date column, use regex to break out the year, month, and day component, then re concatenate the <code class="inline">MM/DD/YYYY</code> to <code class="inline">YYYY/MM/DD</code>.

    <li> With the restaurant name column <code class="inline">DBA</code>, please create a new column called "restaurant_name" which reformats the data in the original column from all upper case to camel case. Separately, create a new Boolean column that flags <code class="inline">True</code> if the word <code class="inline">Thai</code> (not case sensitive) appears in the original restaurant name.
  </ul>

  <p>For example, if the file was <a href="restaurants30July.csv">restaurants30July.csv</a> with the first 3 lines of:

  <pre><code class="datablock">
CAMIS,DBA,BORO,BUILDING,STREET,ZIPCODE,PHONE,CUISINE DESCRIPTION,INSPECTION DATE,ACTION,VIOLATION CODE,VIOLATION DESCRIPTION,CRITICAL FLAG,SCORE,GRADE,GRADE DATE,RECORD DATE,INSPECTION TYPE,Latitude,Longitude,Community Board,Council District,Census Tract,BIN,BBL,NTA
41178124,CAFE 57,Manhattan,300,WEST   57 STREET,10019,2126492729,American,7/30/2021,Violations were cited in the following area(s).,09C,Food contact surface not properly maintained.,Not Critical,4,A,7/30/2021,8/1/2021,Cycle Inspection / Initial Inspection,40.76643902,-73.98332508,104,3,13900,1025451,1010477502,MN15
50111450,CASTLE CHICKEN,Bronx,5987A,BROADWAY,10471,9178562047,Chicken,7/30/2021,Violations were cited in the following area(s).,05D,Hand washing facility not provided in or near food preparation area and toilet room. Hot and cold running water at adequate pressure to enable cleanliness of employees not provided at facility. Soap and an acceptable hand-drying device not provided.,Critical,41,N,,8/1/2021,Pre-permit (Operational) / Initial Inspection,40.88993027,-73.89805316,208,11,28500,2084208,2058011033,BX29
40699339,NICK GARDEN COFFEE SHOP,Bronx,2953,WEBSTER AVENUE,10458,7183652277,Coffee/Tea,7/30/2021,Violations were cited in the following area(s).,08A,Facility not vermin proof. Harborage or conditions conducive to attracting vermin to the premises and/or allowing vermin to exist.,Not Critical,31,,,8/1/2021,Cycle Inspection / Initial Inspection,40.86759042,-73.88308647,207,11,41500,2016446,2032800061,BX05
</code></pre>

  Then a sample run of the program:
  <pre><code class="blockcode">
Enter input file name: restaurants30July.csv
Enter output file name:  july30filtered.csv
  </code></pre>

  And the first three lines of <code class="inline">july30filtered.csv</code> would be:
<pre><code class="datablock">
CAMIS,DBA,BORO,BUILDING,STREET,ZIPCODE,PHONE,CUISINE DESCRIPTION,INSPECTION DATE,ACTION,VIOLATION CODE,VIOLATION DESCRIPTION,CRITICAL FLAG,SCORE,GRADE,GRADE DATE,RECORD DATE,INSPECTION TYPE,Latitude,Longitude,Community Board,Council District,Census Tract,BIN,BBL,NTA,restaurant_name,thai_boolean
41178124,CAFE 57,Manhattan,300,WEST  57 STREET,10019,+1-212-649-2729,American,2021/07/30,Violations were cited in the following area(s).,09C,Food contact surface not properly maintained.,Not Critical,4,A,7/30/2021,8/1/2021,Cycle Inspection / Initial Inspection,40.76643902,-73.98332508,104,3,13900,1025451,1010477502,MN15,Cafe 57 ,False
50111450,CASTLE CHICKEN,Bronx,5987A,BROADWAY,10471,+1-917-856-2047,Chicken,2021/07/30,Violations were cited in the following area(s).,05D,Hand washing facility not provided in or near food preparation area and toilet room. Hot and cold running water at adequate pressure to enable cleanliness of employees not provided at facility. Soap and an acceptable hand-drying device not provided.,Critical,41,N,,8/1/2021,Pre-permit (Operational) / Initial Inspection,40.88993027,-73.89805316,208,11,28500,2084208,2058011033,BX29,Castle Chicken ,False
40699339,NICK GARDEN COFFEE SHOP,Bronx,2953,WEBSTER AVENUE,10458,+1-718-365-2277,Coffee/Tea,2021/07/30,Violations were cited in the following area(s).,08A,Facility not vermin proof. Harborage or conditions conducive to attracting vermin to the premises and/or allowing vermin to exist.,Not Critical,31,,,8/1/2021,Cycle Inspection / Initial Inspection,40.86759042,-73.88308647,207,11,41500,2016446,2032800061,BX05,Nick Garden Coffee Shop ,False
</code></pre>
</strike>


<p></p><p><a name="p7"><b>Program 7: Neighborhood Tabulation Areas</b></b> &emsp; <i>Due noon, Friday, 10 September.</i>
  <br>(Learning Objective: The learning objective of this exercise is to give the students an opportunity to practice their newfound SQL skills.)

  <p>
  The package <a href="https://pypi.org/project/pandasql/">pandasql</a> provides an easy way to use SQL queries directly on a Pandas DataFrame.  (You may need to install it in your hierarchy (e.g. <code class="inline">pip install pandasql</code> or <code class="inline">pip install pandasql</code>).

  <p>
  Once installed, you can run queries via the function  <code class="inline">sqldf(queryName)</code>.  For example, you could filter for all students in the <a href="https://raw.githubusercontent.com/DS-100/textbook/master/content/ch/01/roster.csv">roster.csv</a> on the waitlist by:

<pre><code class=blockcode>
import pandas as pd
import pandasql as psql
roster = pd.read_csv('roster.csv')

q = 'SELECT * FROM roster WHERE Role = "Waitlist Student"'
waitList = psql.sqldf(q)

print(waitList)
</code>
</pre>

  <p>For this program, ask the user for the input and output file names.
  You should assume that the input file contains the New York City
  <a href="https://data.cityofnewyork.us/City-Government/2010-Neighborhood-Tabulation-Areas-NTAs-/cpf4-rkhq">Neighborhood Tabulation Areas</a> such as
  <a href="nynta.csv">nynta.csv</a>.
  Use <code class="inline">sqldf(queryName)</code> to filter the dataset to return the <code class="inline">NTACode</code> and
  <code class="inline">NTAName</code> columns, labeled as
  <code class="inline">NTA</code> and <code class="inline">NTA_Name</code>, respectively.  You should save the result as a CSV in the output file named by the user.

<p></p><p><a name="p8"><b>Program 8: Restaurant SQL Queries.</b></b> &emsp; <i>Due noon, Monday, 13 September.</i>

  <p>Your program should ask for the input file name (must include .csv) and then for an output file prefix (must not include any extension). For example, with <code class="inline">restaurantJuly2020.csv</code> for the input and <code class="inline">selected</code> for the output prefix.  The program should create 4 files: <code class="inline">selectedA.csv</code>, <code class="inline">selected70.csv</code>, <code class="inline">selectedZIP.csv</code>, and <code class="inline">selectedAll.csv</code>.
  <p>
  Using SQL (see <a href="http://www.textbook.ds100.org/ch/06/sql_basics.html">DS 100: Section 6.2</a>), extract the following information from a <a href="https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j/data">restaurant inspection</a> dataset (a small file of inspections from 30 July is available: <a href="restaurants30July.csv">restaurants30July.csv</a>):
  <ul>
    <li> Save all rows in table where <code class="inline">Grade = A</code> to the output file <code class="inline">prefix+"A.csv"</code> where <code class="inline">prefix</code> holds the value specified by the user.</i> <!--(note: warm-up for SELECT * FROM TABLE WHERE..)--> </li>
    <li> Save all rows in table where <code class="inline">SCORE > 70</code> to the output file <tt>prefix+"70.csv"</tt>.</li>
    <li> Save all rows in table where <code class="inline">ZIPCODE IN (10002, 10027, 10036)</code> to the output file <code class="inline">prefix+"ZIP.csv"</code></li>
    <li> Save columns
      <code class="inline">DBA</code> renamed as <code class="inline">restaurant_name</code>,
      <code class="inline">CUISINE DESCRIPTION</code> renamed as <code class="inline">cuisine_description</code>,
      <code class="inline">BORO</code> renamed as <code class="inline">borough</code>, and
      <code class="inline">GRADE</code> and select only those entries with:
      <code class="inline">GRADE = A</code> and <code class="inline">ZIPCODE IN (10002, 10027, 10036)</code> and save to the output file <code class="inline">prefix+"All.csv"</code>.
  </ul>

  <p>Note: The file extension names are case-sensitive, so, the autograder will not except <code class="inline">... ALL.csv</code> for <code class="inline">... All.csv</code>.
</p>



<p></p><p><a name="p9"><b>Program 9: Aggregating Restaurant Data (SQL).
  </b></b> &emsp; <i>Due noon, Tuesday, 14 September.</i>


  <br>(Learning Objective: The learning objective of this exercise is to give the students an opportunity to practice more advanced SQL skills (e.g. <code class="inline">GROUP BY</code>'s) on a familiar dataset.)

  <p>
  Using the more advanced SQL commands from <a href="http://www.textbook.ds100.org/ch/05/sql_basics.html">DS 100: Section 5.1</a> (e.g. <code class="inline">GROUP BY</code>'s), this program find distinct restaurant names and distinct cuisines by locale.  For testing, a small file of inspections from 1 August is available: <a href="brooklynJuly2021.csv">brooklynJuly2021.csv</a>.

  <p></p><p>Your program should ask for the input file name (must include .csv) and then for an output file prefix (must not include any extension).
  </p>
    <ul>
      <li> Save a unique column of distinct restaurants (note: return the restaurant names, not the count)  to the output file <code class="inline">prefix+"Restaurants.csv"</code> where <code class="inline">prefix</code> holds the value specified by the user.</li>
      <li> Save a unique column of distinct cuisines (note: return the cuisine names, not the count) where <code class="inline">ZIPCODE = 11224</code> to the output file <code class="inline">prefix+"Cuisines11224.csv"</code> where <code class="inline">prefix</code> holds the value specified by the user. </li>
      <li> Save 2 columns, the cuisine and the count of unique restaurants per cuisine,  where <code class="inline">ZIPCODE = 11224</code> (note: return the cuisine names, not the count) to the output file <code class="inline">prefix+"Counts11224.csv"</code> where <code class="inline">prefix</code> holds the value specified by the user. </li>
      <li> Save 3 columns: the borough name, the unique count of cuisines per borough and the unique count of restaurants per boroughs. The results should be sorted results by the borough name (in ascending order:  <code class="inline">ASC</code>) and saved to the output file <code class="inline">prefix+"Boro.csv"</code> where <code class="inline">prefix</code> holds the value specified by the user.</li>
    </ul>
    <p>
For example, if you entered <code class="inline">brooklynJuly2021.csv</code> and <code class="inline">selected</code> for the output prefix, the program should create 4 files: <code class="inline">selectedRestaurants.csv</code>, <code class="inline">selectedCuisines11224.csv</code>, <code class="inline">selectedCounts11224.csv</code>, and <code class="inline">selectedBoro.csv</code>.
The first several lines of <code class="inline">selectedRestaurants.csv</code> are:
<pre><code class=datablock>DBA
1 HOTEL BROOKLYN BRIDGE
14 OLD FULTON STREET
98K
99 CENT PIZZA
ABURI SUSHI BAR
</code></pre>
</p>
<p>The file <code class="inline">selectedCuisines11224.csv</code> is:
<pre><code class=datablock>cnt
American
</code></pre>
(since our test file only has restaurants that serve American food in the 11224 zipcode)
</p>
<p>The file <code class="inline">selectedCounts11224.csv</code> is:
<pre><code class=datablock>CUISINE DESCRIPTION,COUNT(DISTINCT DBA)
American,3
</code></pre>
</p>
<p>The file <code class="inline">selectedBoro.csv</code> is:
<pre><code class=datablock>borough,cnt_cuisine,cnt_restaurants
Brooklyn,50,384
</code></pre>
</p>

<p></p><p><a name="p10"><b>Program 10: Extracting Districts.</b></b> &emsp; <i>Due noon, Monday, 20 September.</i>
    <br>(Learning Objective: successfully write and apply functions to DataFrames to clean data.)
    <p>
    Write a program that asks the user for the name of an input CSV file and the name of an output CSV file.  The program should open the file name provided by the user. Your program should include a function, <code class="inline">extractDistrict()</code> that takes a string as an input and returns the number represented by the first two characters in the string:
    <pre><code class=blockcode>
    def extractDistrict(name):
    '''
    Extracts the district (first two characters) as an integer.
    Input:  Character string containing district + school num (e.g. "01M015")
    Returns:  The first characters as an integer (e.g. 1)
    '''

      #### Your code goes here ####
    </code>
    </pre>
    Your program should apply the function to each row that takes the first two characters of the <code class="inline">School</code> field, converts those into a digit, and stores the results in a new column, <code class="inline">District</code>.  That is,

    <pre><code class="blockcode">df['District'] = df['DBN'].apply(extractDistrict)</code></pre>

    <p> For example, if the <code class="inline">School</code> is <code class="inline">"01M015"</code>, the entry in the new column would be <code class="inline">1</code> (stored as a number, not a string).

    <p> The results should be written to a new CSV file, name provided by the user.</i></p>
  </p>



<p></p><p><a name="p11"><b>Program 11: Joining Restaurant & NTA Data.</b></b> &emsp; <i>Due noon, Tuesday, 21 September.</i>

  <br>(Learning Objective: The exercises in this program will build up to help students conceptualize and finally create a <code class="inline">JOIN</code> between the health inspection table and the NTA table.  This is to reinforce the learning done in the last 2 SQL lectures.)

  <p>For testing, a small file of inspections from 30 July is available: <a href="restaurants30July.csv">restaurants30July.csv</a> and the Neighborhood Tabulation Areas (NTA):
  <a href="nta.csv">nta.csv</a>.


  <p>Your program should ask for two input file name (must include .csv) and then for an output file prefix (must not include any extension). For example, with <code class="inline">restaurantJuly2020.csv</code> and <code class="inline">nta.csv</code> for the input and <code class="inline">selected</code> for the output prefix.  The program should create 6 files:
    <code class="inline">selected1.csv</code>,
    <code class="inline">selected2.csv</code>,
    <code class="inline">selected3.csv</code>,
    <code class="inline">selected4.csv</code>,
    <code class="inline">selected5.csv</code>, and
    <code class="inline">selected6.csv</code>.
  <p>
    <ol>
      <li> Save the NTA column from the restaurant inspection table to the output file <code class="inline">prefix+"1.csv"</code> where <code class="inline">prefix</code> holds the value specified by the user.</i>
      <li> Save the count of unique NTAs in the restaurant health inspection table to the output file <code class="inline">prefix+"2.csv"</code> where <code class="inline">prefix</code> holds the value specified by the user.  (Note this will have a single column and a single value.)
      <li> Save the NTA column and the count of the distinct restaurants from the restaurant inspection table to the output file <code class="inline">prefix+"3.csv"</code> where <code class="inline">prefix</code> holds the value specified by the user.  (Hint:  how can you use <code class="inline">GROUP BY</code> to organize the output?)
      <li> Save the number of rows in the <b>NTA table</b> and the number of unique NTAs in the NTA table to the output file <code class="inline">prefix+"4.csv"</code> where <code class="inline">prefix</code> holds the value specified by the user.
      (Note this will have a two rows and two columns.)
      <li> Save the names of the restaurant and its NTA which can be found via  a <code class="inline">LEFT JOIN</code> of the restaurant inspection table and NTA table.  Save the results to the output file <code class="inline">prefix+"5.csv"</code> where <code class="inline">prefix</code> holds the value specified by the user.
      (Hint: join on the NTA code found in both (but using slightly different names).  Your output should have two columns.)
      <li> Building on the result from 5) above, keep the <code class="inline">LEFT JOIN</code> as is, do one more level of aggregation, so that the end result contains 3 columns (unique NTA code, unique NTA description, and the count distinct restaurants as grouped by the first 2 columns).  Save result to the output file <code class="inline">prefix+"6.csv"</code> where <code class="inline">prefix</code> holds the value specified by the user.
    </ol>
</p>



<p></p><p><a name="p12"><b>Program 12: MTA Ridership.</b></b> &emsp; <i>Due noon, Thursday, 23 September.</i>


  <br>(Learning Objective: to reinforce Pandas skills via use for data aggregating and data cleaning.)

  <p>In the next lecture, we will be summarizing time-series data and using a
  <a href="https://qri.cloud/nyc-transit-data/">cleaned version</a> of MTA subway and bus ridership, inspired by Oldenburg's <a href="https://observablehq.com/@benoldenburg/nyc-transit-turnstile-data">NYC Transit Turnstile Data</a>.

  <p>Write a program that asks the user for the name of an input CSV file and the name of an output CSV file.  The program should open the file name provided by the user, which you can assume will include the column names: <code class="inline">date</code>, <code class="inline">entries</code>, and <code class="inline">exit</code>.  You should create a new file that has one entry for each date that consists of the sum of all entries and the sum of all exits that occur on the date.  This aggegrate data should be stored in the output CSV and should contain only the three columns:  <code class="inline">date</code>, <code class="inline">entries</code>, and <code class="inline">exits</code>, even if there are additional columns in the input CSV file.</p>

  <p>
    <p>For example, if the file was the 2020 data for Staten Island, <a href="mta_trunc_staten_island.csv">rmta_trunc_staten_island.csv</a> with the first 3 lines of:

    <pre><code class="datablock">stop_name,daytime_routes,division,line,borough,structure,gtfs_longitude,gtfs_latitude,complex_id,date,entries,exits
St George,SIR,SIR,Staten Island,SI,Open Cut,-74.073643,40.643748,501,2020-01-01,2929,0
St George,SIR,SIR,Staten Island,SI,Open Cut,-74.073643,40.643748,501,2020-01-02,13073,0
St George,SIR,SIR,Staten Island,SI,Open Cut,-74.073643,40.643748,501,2020-01-03,11857,23
</code></pre>

    Then a sample run of the program:
    <pre><code class="blockcode">Enter input file name: mta_trunc_staten_island.csv
Enter output file name:  filteredSI.csv</code></pre>

    And the first three lines of <code class="inline">filteredSI.csv</code> would be:
  <pre><code class="datablock">date,entries,exits
2020-01-01,3128,0
2020-01-02,13707,0
2020-01-03,12507,23</code></pre>



<!-- URL of cleaned data:  https://qri.cloud/nyc-transit-data/turnstile_daily_counts_2020 and presented:  https://observablehq.com/@benoldenburg/nyc-transit-turnstile-data  -->



<br>
<br>
<br>
<hr>
Set 3:  The third set of programs integrates visualization techniques with analyzing structured data sets.  While the programs do not cover every visualization technique, the practice these programs provide will be directly relevant to your project.
<hr>

<p></p><p><a name="p13"><b>Program 13: Column Summaries.</b></b> &emsp; <i>Due noon, Friday, 24 September.</i>
  <br>(Learning Objective: to strengthen function-writing skills and examine alternate ways to summarize time-series data.)

  <p>In lecture, we used the Pandas' function, <code class = "inline">rolling()</code> to compute a 7-day average of subway ridership for the visualization of ridership in 2020.  For this program, write three functions that take as input a Pandas' series (e.g. a column of a DataFrame) that highlights different patterns in the data:</p>
  <ul>
    <li> <code class = "inline">cumulativeAverage(column)</code>:  Assumes the input is a Series of numerical data. Returns a Series with the cumulative (running) average of the values.  For example, if the first 5 entries are <code class = "inline">10,20,30,40,20</code>, the Series that is returned would start out as <code class = "inline">10,15,20,25,24</code> (since 10/1 = 10, (10+20)/2 = 15, (10+20+30)/3 = 20, (10+20+30+40)/4 = 25, (10+20+30+40+20)/5 = 24).  Note that it is different that <code class = "inline">rolling()</code> we used in class, since this function creates a value for all entries and averages across all values seen.
    <li> <code class = "inline">cyclicAverage(column)</code>:  Assumes the input is a Series of numerical data. Returns a Series with the average of the current day with, if they exist, the value from 7 days previously and 14 days previously.  That is, if they exist, for entry at index <code class = "inline">i</code>, take the average of the values at indices <code class = "inline">i</code>, <code class = "inline">i-1*offset</code>, and <code class = "inline">i-2*offset</code>, as the computation.  Since ridership is highly dependent on the day of the week, this averages the values of the same day in past weeks.
    <li> <code class = "inline">exponentialSmoothing(column)</code>:  Assumes the input is a Series of numerical data. Returns a Series with a weighted average of the previous values with the most recent values have higher weight and the older ones have lower weights.  The value for the first entry, <code class = "inline">newCol[0]</code> is <code class = "inline">column[0]</code>.  The value for subsequent entries is <code class = "inline">newCol[t+1] = 0.5*column[t+1] + 0.5*newCol[t]</code>.  For example, if we had the same Series starting the same as above, <code class = "inline">10,20,30,40,20</code>, our new column would start <code class = "inline">10,15,22.5,31.25,25.625</code>
    (since the first entry is the same, 0.5*20+0.5*10 = 15, 0.5*30+0.5*15= 22.5, 0.5*40+0.5*22.5 = 31.25, 0.5*20 + 0.5*31.25 = 25.625).
  </ul>
  Note:  you should submit a file with only the standard comments at the top, and these three functions.  The grading scripts will then import the file for testing.
</p>

<p></p><p><a name="p14"><b>Program 14: Library Cleaning.</b></b> &emsp; <i>Due noon, Monday, 27 September.</i>
  <br>(Learning Objective: to strengthen data processing skills using regular expressions and standard string methods.)

  <p>
  Write two functions that will be used to clean the OpenData NYC dataset of <a href="https://data.cityofnewyork.us/Business/Library/p4pf-fyc4">Libraries in New York City</a> (downloaded as CSV file).  The first three lines of the CSV file look like:

<pre><code class="datablock">
the_geom,NAME,STREETNAME,HOUSENUM,CITY,ZIP,URL,BIN,BBL,X,Y,SYSTEM,BOROCODE
POINT (-73.95353074430393 40.80297988196676),115th Street,West 115th Street,203,New York,10026,http://www.nypl.org/locations/115th-street,1055236,1018310026,997115.12977,231827.652864,NYPL,1
POINT (-73.9348475633247 40.80301816141575),125th Street,East 125th Street,224,New York,10035,http://www.nypl.org/locations/125th-street,1054674,1017890037,1002287.604,231844.894956,NYPL,1
</code></pre>



  Each function takes as input a row of the table:
  <ul>
    <li> <code class = "inline">extractLatLon(row)</code>:  This function takes the values from the column <code class = "inline">the_geom</code> and extracts the longitude and latitude from the string (they are surrounded by parenthesis and separated by a space, and returns the two as numerical values.  For example, the function would return -73.95353074430393, 40.80297988196676 when applied to the first row of data.
    <li> <code class = "inline">extractTitle(row)</code>:  This function concatenates the values from the columns <code class = "inline">NAME</code>, <code class = "inline">CITY</code>, and <code class = "inline">ZIP</code> code into a single string, separated by a comma and space, and returns the string (to be used as the title for our visualizations).
    For example, when applying this function to the first data row, the return value would be:
    <code class = "inline">115th Street, New York, 10026</code>.
  </ul>

  <p>
  Note:  you should submit a file with only the standard comments at the top, and these two functions.  The grading scripts will then import the file for testing.  A sample test program that assumes your program is called <code class = "inline">p14.py</code>
  and the CSV file is called <code class = "inline">LIBRARY.csv</code> is <a href="test14.py">test14.py</a>.

  </p>





<p></p><p><a name="p15"><b>Program 15: Plotting Challenge.</b></b> &emsp; <i>Due noon, Tuesday, 28 September.</i>


  <br>(Learning Objective: to explore and master <tt>matplotlib.pyplot</tt> commands to create data visualizations.)

  <p>
  The goal is to create a plot of <a href="https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95">NYC OpenData Motor Vehicle Collisions</a> that follows this <a href="https://www.dataquest.io/blog/making-538-plots/">style</a>.  For example, here is the plot for <a href="Motor_Vehicle_Collisions_Mar_2020.csv">January 2020</a> dataset:

  <p>
  <img src="col_Mar2020.png" height=300>

  <p>
  Your program should begin by asking the user for input and output files.  It should be written to take any dataset from the <a href="https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95">NYC OpenData Motor Vehicle Collisions</a> and produce an image that matches this <a href="https://www.dataquest.io/blog/making-538-plots/">style</a>.  The resulting image should be saved to the output file specified by the user.

  <p><i>Hint:  to transform the data into separate columns (i.e. "unstack"/pivot the groups to be columns) for the daily number of collisions for each borough:</i>
  <pre><code class="blockcode">boroDF = df.groupby(['BOROUGH','CRASH DATE']).count()['CRASH TIME'].unstack().transpose()
</code></pre>
  <i>where <tt>df</tt> is the DataFrame with the collisions data.</i></p>

</p>



<p></p><p><a name="p16"><b>Program 16:   Choropleth Attendance Cleaning.</b></b> &emsp; <i>Due noon, Thursday, 30 September.</i>
<br>(Learning Objective: to gain competency cleaning data using pandas functions.)

<p>In lecture, we wrote a program, <a href="schoolsChoropleth.py">schoolsChoropleth.py</a>, using the school district files used in <a href="#p10">Programs 10 & 11</a> to make a choropleth map of top English Languange Arts scores, by <a href="https://data.cityofnewyork.us/Education/School-Districts/r8nu-ymqj">district</a>, in New York City:

  <p>
  <iframe src="ela2019choropleth.html" style="width: 60%; height: 550px" name="internal"></iframe>


<p> For this program, write a program that will clean <a href="https://infohub.nyced.org/reports/school-quality/information-and-data-overview/end-of-year-attendance-and-chronic-absenteeism-data">district school attendance data</a> so that we can use the same visualization to see attendance for different districts.

<p>Your stand-alone program should ask the user for the input file name, the output file name, as well as the grade and school year to use as filters.  For example,
a sample run of the program on <a href="public-district-attendance-results-2014-2019.csv">public-district-attendance-results-2014-2019.csv</a>:
<pre><code class="blockcode">
Enter input file name: public-district-attendance-results-2014-2019.csv
Enter output file name: attendanceThirdGrade2019.csv
Enter grade: 3
Enter year: 2018-19
</code></pre>
If the input file starts as:
<pre><code class="datablock">
District,Grade,Year,Category,# Total Days,# Days Absent,# Days Present,% Attendance,# Contributing 20+ Total Days,# Chronically Absent,% Chronically Absent
1,All Grades,2013-14,All Students,2088851,187879,1900972,91.0,12617,3472,27.5
1,All Grades,2014-15,All Students,2064610,171200,1893410,91.7,12295,3160,25.7
1,All Grades,2015-16,All Students,1995704,169094,1826610,91.5,12137,3206,26.4
1,All Grades,2016-17,All Students,1946012,161756,1784256,91.7,11916,3110,26.1
1,All Grades,2017-18,All Students,1946527,167998,1778529,91.4,11762,3244,27.6
1,All Grades,2018-19,All Students,1925995,175153,1750842,90.9,11593,3364,29.0
</code></pre>
then the output file would start:
<pre><code class="datablock">
District,Grade,Year,Category,# Total Days,# Days Absent,# Days Present,% Attendance,# Contributing 20+ Total Days,# Chronically Absent,% Chronically Absent
1,3,2018-19,All Students,149871,10601,139270,92.9,876,228,26.0
2,3,2018-19,All Students,491432,21170,470262,95.7,2844,278,9.8
3,3,2018-19,All Students,254506,15395,239111,94.0,1488,274,18.4
</code></pre>


<p><i>Hints:
<ul>
    <li> In the CSV file, <code class="inline">Grade</code> can contain both numbers and strings.  Since <code class="inline">"3"</code> is different from the number <code class="inline">3</code>, you may want to convert to strings before comparison.</li>
    <li> The output CSV should not contain the internal index and should be saved without it.
</ul>
</i>

<!-- Folium tutorial:  https://autogis-site.readthedocs.io/en/latest/notebooks/L5/02_interactive-map-folium.html
Layers on folium:  https://blog.prototypr.io/interactive-maps-with-python-part-1-aa1563dbe5a9
https://automating-gis-processes.github.io/2017/lessons/L5/interactive-map-folium.html
Altair & choropleth:  https://www.districtdatalabs.com/altair-choropleth-viz-->

<p>
</p>

<p></p><p><a name="p17"><b>Program 17: Grouping ELA/Math by Districts.</b></b> &emsp; <i>Due noon, Friday, 1 October.</i>
  <br>(Learning Objective: to successfully combine information from multiple input files and display the results using a pivot table.)




  <p>Your program should build on the classwork from Lectures #6 and #9 to build a pivot table, grouped by district and test subject, of the percentage of students that are proficient in each (i.e. score 3 or 4 on the exam).  Your program should ask the user for two input CSV files and print a pivot table.
</p>


Then a sample run of the program with files truncated to a few schools per district for testing (<a href="ela_trunc.csv">ela_trunc.csv</a> and <a href="math_trunc.csv">math_trunc.csv</a>) starts as:

<pre><code class="blockcode">Enter file containing ELA scores: ela_trunc.csv
Enter file containing MATH scores: math_trunc.csv
                    Proficiency                      School Name
District Subject
01       ELA        91.891892  THE EAST VILLAGE COMMUNITY SCHOOL
         MATH       84.615385               P.S. 184M SHUANG WEN
02       ELA        96.825397           P.S. 77 LOWER LAB SCHOOL
         MATH       98.412698           P.S. 77 LOWER LAB SCHOOL
</code></pre>
and continues with top scoring schools for each test and each district printed.
</p>

<i>
<p>Hints:
  <ul>
    <li> Work first with the math scores and get those scores selected.  Then, repeat with the ELA scores.  Once both have the correct values, combine for the final answer.
    <li> Look at the names used in the row and column indices, and use similar names for your dataframes to avoid having to rename them later.
    <li> A very useful aggregate function is <code class="inline">idxmax()</code> which returns the index of the row where the maximum occurs, instead of just the value itself.  For example, if you had a DataFrame <code class="inline">mathdf</code> with the proficiency column already computed, the following will list all the columns for the school with the maximum profiency for each district:
  <pre><code class="blockcode">mathdf = mathdf.loc[ mathdf.groupby('District')['Proficiency'].idxmax() ]</code></pre>
</ul>
</p>
</i>


<p></p><p><a name="p18"><b>Program 18: Log Scale.</b></b> &emsp; <i>Due noon, Monday, 4 October.</i>
  <br>(Learning Objective: gain competency in scaling data via log transformations.)

  <p>
  In Lecture #9 and <a href="http://www.textbook.ds100.org/ch/10/viz_principles_2.html">Section 11.5</a>, we used log scale to visualize data.  Since the logarithm function is not defined on non-positive data, we are first going to write a function that removes any tuple that has a 0 or negative value.  Our second function transformed the cleaned data to its log values.

  <p>
  Write two functions that to be used to display data on a log-scale.  Each function takes and returns two iterables of numeric values (e.g. a Series, np.array, or list restricted to numeric values).

  Each function takes as input a row of the table:
  <ul>
    <li> <code class = "inline">dropNeg(xS,yS)</code>:  This function takes two iterables, <code class = "inline">xS</code> and <code class = "inline">yS</code> of numeric values.  If any entry is not positive in either iterable, that indexed value is dropped from both series.  The results are returned as two separate iterables.  To do this, first zip the series together, drop all the pairs with zero or negative values, and then unzip to return series with only positive values.

    <p>For example, if <code class = "inline">xS</code> contains
    <code class = "inline">[1,2,0,3,4]</code> and
    <code class = "inline">yS</code> contains <code class = "inline">[0,-1.5,4,3,9]</code>, then the <code class = "inline">zip(xS,yS)</code> has entries
    <code class = "inline">[(1,0),(1,-1.5),(0,4),(3,3),(4,9)]</code>.  Dropping all tuples that contain non-positive values yields
    <code class = "inline">[(3,3),(4,9)]</code>, and the unzipped results, <code class = "inline">[3,4]</code> and
    <code class = "inline">[3,9]</code>, are returned.

    <li> <code class = "inline">logScale(xS,yS)</code>:  This function assumes that the inputted iterables contain numeric values, are positive and not null, and returns the <code class = "inline">np.log</code> of each.
    For example, when applying this function to the inputs <code class = "inline">[3,4]</code> and
    <code class = "inline">[3,9]</code>, the function returns <code class = "inline">[1.098612, 1.386294]</code> and
    <code class = "inline">[1.098612,2.19722458]</code>.
  </ul>

<p>
  Note:  you should submit a file with only the standard comments at the top, and these two functions.  The grading scripts will then import the file for testing.  A sample test program that assumes your program is called
  <code class = "inline">p18.py</code>
  and is <a href="test18.py">test18.py</a>.
</p>


<p></p><p><a name="p19"><b>Program 19: Smoothing with Gaussians.</b></b> &emsp; <i>Due noon, Tuesday, 5 October.</i>
  <br>(Learning Objective: increase understanding of smoothing and gain fluidity with using distributions for smoothing.)


  <p>
  In Lecture #9 and <a href="http://www.textbook.ds100.org/ch/10/viz_principles_2.html">Section 11.5</a>, we used smoothing to visualize data.  For this program, write a function that takes two arguments, an Numpy array of x-axis coordinates, and a list of numeric values, and returns the corresponding y-values for the sum of the gaussian probability distribution functions (pdf's) for each point in the list.

  <ul>
    <li> <code class = "inline">computeSmoothing(xes,points)</code>:  This function takes a numpy array <code class = "inline">xes</code> and a list, <code class = "inline">points</code>, of numeric values.  For each <code class = "inline">p</code> in <code class = "inline">points</code>, the function should compute the normal probability distribution function (<code class = "inline">scipy.norm.pdf</code>)
    centered at <code class = "inline">loc = p</code> with standard deviation <code class = "inline">scale = 0.5</code> for all values in <code class = "inline">xes</code>.  The return value is a numpy array of the sum of these at each point.
  </ul>

  <p>For example, calling the function:
    <pre><code class="blockcode">xes = np.linspace(0, 10, 1000)
density = computeSmoothing(xes,[5])
plt.plot(xes,density)
plt.show()</code></pre>

    would give the plot:

    <p><img height=200 src="density5.png">

    <p>since there is only one point given (namely 5), the returned value is the probability density function centered at 5 (with <code class = "inline">scale =  0.5</code>) computed for each of the <code class = "inline">xes</code>.

<p>For example, calling the function:
  <pre><code class="blockcode">pts = [2,2,5,5,2,3,4,6,7,9]
xes = np.linspace(0, 10, 1000)
density = computeSmoothing(xes,pts)
plt.plot(xes,density)
plt.fill_between(xes,density)
plt.show()</code></pre>

would give the plot:

<p><img height=200 src="density_fillBetween.png">

  <p>since the there are 10 points given, the function computes the probability density function centered at each of the points, across all the values in <code class = "inline">xes</code>.  It then sums up these contributions and returns an array of the same length as <code class = "inline">xes</code>.




<p>
    Note:  you should submit a file with only the standard comments at the top, and this function.  The grading scripts will then import the file for testing.

<p>
  <i>Hint:  Include only the function you need (such as <code class = "inline">numpy</code> and <code class = "inline">scipy.stats</code>) and none of the ones for plotting (such as <code class = "inline">matplotlib.pyplot</code> and <code class = "inline">seaborn</code>) since this function is computing and not plotting.
  </i>
  </p>


  <br>
  <br>
  <br>
  <hr>
  Set 4:  The fourth set of programs introduces modeling and estimation, focusing on loss functions and linear modeling.
  <hr>


<p></p><p><a name="p20"><b>Program 20: Loss Functions for Tips.</b></b> &emsp; <i>Due noon, Thursday, 7 October.</i>
    <br>(Learning Objective: strengthen competency with loss functions by applying the techniques to a dataset of tips.)
    <p>
    </p>
    In Lecture #10 and <a href="http://www.textbook.ds100.org/ch/04/modeling_loss_functions.html">Section 4.2</a>, we introduced loss functions to measure how well our estimates fit the data.

    <p>
    Using the functions mean squared loss function <code class = "inline">mse_loss</code> and mean absolute loss function <code class = "inline">abs_loss</code> (<a href="http://www.textbook.ds100.org/ch/04/modeling_loss_functions.html">Section 4.2</a>),
    write two separate functions that take in estimates and tip data
    and returns the respective loss function for each of the estimates to the data.

    <ul>
      <li> <code class = "inline">mse_estimates(thetas,tips)</code>:  This function takes two iterables of numeric values:
      <ul>
          <li> <code class = "inline">thetas</code>: estimates for the population parameter for the percent tips in <code class = "inline">values</code>, and
          <li> <code class = "inline">tips</code>: the tips observed, assumed to be a  positive percentage, ranging from 0 to 100.
      </ul>
      For each <code class = "inline">theta</code> in <code class = "inline">thetas</code>, it should compute the mean squared error between <code class = "inline">theta</code> and <code class = "inline">tips</code>. Return an iterable of the values computed.

      <li> <code class = "inline">mae_estimates(thetas,tips)</code>:  This function takes two iterables of numeric values:
        <ul>
            <li> <code class = "inline">thetas</code>: estimates for the population parameter for the percent tips in <code class = "inline">values</code>, and
            <li> <code class = "inline">tips</code>: the tips observed, assumed to be a  positive percentage, ranging from 0 to 100.
        </ul>
      For each <code class = "inline">theta</code> in <code class = "inline">thetas</code>, it should compute the mean absolute error between <code class = "inline">theta</code> and <code class = "inline">tips</code>. Return an iterable of the values computed.

    </ul>

    Note:  for each of these functions, your returned value will be an iterable with the same length as <code class = "inline">thetas</code>.

    <p>For example, calling the function:
      <pre><code class="blockcode">thetas = np.array([12, 13, 14, 15, 16, 17])
y_vals = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
mse_losses = p20.mse_estimates(thetas,y_vals)
abs_losses = p20.mae_estimates(thetas,y_vals)
plt.scatter(thetas, mse_losses, label='MSE')
plt.scatter(thetas, abs_losses, label='MAE')
plt.title(r'Loss vs. $ \theta $ when $ \bf{y}$$= [ 12.1, 12.8, 14.9, 16.3, 17.2 ] $')
plt.xlabel(r'$ \theta $ Values')
plt.ylabel('Loss')
plt.legend()
plt.show()</code></pre>

      would give the plot:

      <p><img height=200 src="loss_small_ex.png">

  <p>For example, calling the function:
    <pre><code class="blockcode">thetas = np.arange(30)
tips_df = sns.load_dataset('tips')
tipsPercent = (tips_df['tip']/tips_df['total_bill'])*100
mse_losses = p20.mse_estimates(thetas, tipsPercent)
abs_losses = p20.mae_estimates(thetas, tipsPercent)
plt.plot(thetas, mse_losses, label='MSE')
plt.plot(thetas, abs_losses, label='MAE')
plt.title(r'Loss vs. $ \theta $ for sns tips data')
plt.xlabel(r'$ \theta $ Values')
plt.ylabel('Loss')
plt.legend()
plt.show()</code></pre>

  would give the plot:

  <p><img height=200 src="loss_tips.png">

  <p>
      Note:  you should submit a file with only the standard comments at the top, and this function.  The grading scripts will then import the file for testing.

  <p>
    <i>Hint:  Include only the libraries you need (such as <code class = "inline">numpy</code>) and none of the ones for plotting (such as <code class = "inline">matplotlib.pyplot</code> and <code class = "inline">seaborn</code>) since this function is computing and not plotting.
    </i>
    </p>


<p></p><p><a name="p21"><b>Program 21: Taxi Cleaning.</b></b> &emsp; <i>Due noon, Friday, 8 October.</i>
  <br>(Learning Objective: To build up (or refresh) skills at manipulating tabular data, in particular, to use arithmetic operations on columns to create new columns.)

  <p>
    Write a program, tailored to the NYC OpenData Yellow Taxi Trip Data, that asks the user for the name of an input CSV file and the name of an output CSV file.  The program should open the file name provided by the user. Next, the program should copy the input file and create two new columns:  <code class="inline">percent_tip</code>, which is <code class="inline">100*tip_amount/fare_amount</code> and <code class="inline">percent_fare</code>, which is <code class="inline">100*fare_amount/total_amount</code>.  Your program should write out a new CSV file (with the name provided by the user) with the original columns as well as the two newly computed ones.

    <p>For example, if the file, <a href="taxi_new_years_day_2020.csv">taxi_new_years_day_2020.csv</a>, was the first of January 2020 entries downloaded from <a href="https://data.cityofnewyork.us/Transportation/2020-For-Hire-Vehicles-Trip-Data/m3yx-mvk4">2020 Yellow Taxi Trip Data</a> (about 170,000 entries) with the first 3 lines of:
<pre><code class="datablock">VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount,congestion_surcharge
1,01/01/2020 12:00:00 AM,01/01/2020 12:13:03 AM,1,2.2,1,N,68,170,1,10.5,3,0.5,2.85,0,0.3,17.15,2.5
2,01/01/2020 12:00:00 AM,01/01/2020 01:08:55 AM,5,1.43,1,N,48,239,2,6.5,0.5,0.5,0,0,0.3,10.3,2.5
</code></pre>
    Then a sample run of the program:
<pre><code class="blockcode">Enter input file name: taxi_new_years_day2020.csv
Enter output file name:  taxi_Jan2020_with_percents.csv
</code></pre>

    And the first three lines of <tt>taxi_Jan2020_with_percents.csv</tt> would be:
<pre><code class="datablock">VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount,congestion_surcharge,percent_tip,percent_fare
1.0,01/01/2020 12:00:00 AM,01/01/2020 12:13:03 AM,1.0,2.2,1.0,N,68,170,1.0,10.5,3.0,0.5,2.85,0.0,0.3,17.15,2.5,27.1,61.2
2.0,01/01/2020 12:00:00 AM,01/01/2020 01:08:55 AM,5.0,1.43,1.0,N,48,239,2.0,6.5,0.5,0.5,0.0,0.0,0.3,10.3,2.5,0.0,63.1
</code></pre>
    <p> You should round the values stored in your new columns to the nearest tenth and save your CSV file without the indexing (i.e. <code class="inline">index=False</code>).

  </p>


<p></p><p><a name="p22"><b>Program 22: Dice Simulator.</b></b> &emsp; <i>Due noon, Thursday, 14 October.</i>
    <br>(Learning Objective: students will be able to apply their knowledge of the built-in random package to generate simulations of simple phenomena.)
    <p>

    Write a function:
    <ul>
        <li> <code class="inline">diceSim(D1,D2,trials)</code> that takes as input the number of sides on die 1 (<code class="inline">D1</code>) and
        die2 (<code class="inline">D2</code>) and the number of trials.  Your function should repeatedly sum pairs of random numbers between 1 and <code class="inline">D1</code> and 1 and <code class="inline">D2</code> and keep track of how many times each sum occurs. The function returns a numpy array with the fraction each sum of rolls occured.
      </ul>

      <p>
      Since the numbers are chosen at random, the fractions will differ some from run to run. One run of the function <code class="inline">print(p22.diceSim(6,6,10000))</code> resulted in:
<pre><code class="datablock">
  [0.     0.     0.0259 0.0615 0.0791 0.1086 0.139  0.1633 0.1385 0.114  0.0833 0.0587 0.0281]</code></pre>

      or displayed using the code from <a href="http://www.textbook.ds100.org/ch/16/prob_random_vars.html">Section 16.1.1.</a>:
      <p>
      <img src="dice_6_6.png" height=200>
    </p>
      <p>
      Note:  you should submit a file with only the standard comments at the top and the function.  The grading scripts will then import the file for testing.
      </p>


<p></p><p><a name="p23"><b>Program 23: Correlation Coefficients.</b></b> &emsp; <i>Due noon, Friday, 15 October.</i>
    <br>(Learning Objective: to refresh students' knowledge of Pearson's correlation coefficient and to increase fluidity with using statistical functions in Python.)
    <p>
    Write a function that will find the columns with highest absolute correlation coefficents in a DataFrame.  Your program should take as inputs the column of interest, a list of possible correlated columns, and the DataFrame.  The function should return the name and Pearson's R correlation coefficent (can be computed using the Pandas function <code class="inline">series1.corr(series2)</code> where
    <code class="inline">series1</code> and
    <code class="inline">series2</code> are Pandas Series):

    <ul>
      <li> <code class = "inline">findHighestCorr(colName,colLst,df)</code>:  This function takes three inputs:
        <ul>
            <li> <code class = "inline">colName</code>: a column name of the specified DataFrame,
            <li> <code class = "inline">colList</code>: a list of column names of the specified DataFrame.  It must be a list and assumed to be non-empty.  Can include <code class = "inline">colName</code>, and
            <li> <code class = "inline">df</code>: a DataFrame including the specified columns.
        </ul>
        The function computes the correlation coefficient between <code class = "inline">df[colName]</code> and each column specified in <code class = "inline">colList</code>, and returns the name and the correlation coefficient for the column from the list with the highest absolute value.
      </ul>


    <p>
    For example, assuming your function <code class="inline">findHighestR()</code> was in the <code class="inline">p23.py</code>:
    <pre><code class="blockcode">simpleDF = pd.DataFrame({'c1': [1,2,3,4],\
                         'c2': [0,1,0,1],\
                         'c3': [1,10,3,20],\
                         'c4': [-10,-20,-30,-40],})
print('Testing with c1 and [c3,c4]:')
print(p23.findHighestCorr('c1',['c3','c4'],simpleDF))
print(f'c1 has highest absolute r with {p23.findHighestCorr("c1",simpleDF.columns, simpleDF)}.')
</code></pre>
    Would give output:
<pre><code class="datablock">Testing with c1 and [c3,c4]:
('c4', -1.0)
c1 has highest absolute r with ('c1', 1.0)
</code></pre>
    since the correlation cofficient between <code class="inline">simpleDF['c1']</code>
    and the other 3 columns is <code class="inline">0.4472135954999579</code>, <code class="inline">0.7520710469952336</code>, and <code class="inline">
-1.0</code>, respectively and the largest <i>absolute</i> correlation is with <code class="inline">simpleDF['c4']</code>.

  <p> Using the function on the seaborn tips dataset:
<pre><code class="blockcode">import seaborn as sns
tips = sns.load_dataset('tips')
print(f"Correlation coefficient between tips and size is \
        {tips['tip'].corr(tips['size'])}")
print(f"For tip, the highest correlation is \
        {p23.findHighestCorr('tip',['total_bill','size'],tips)}.")
</code></pre>
  will print
<pre><code class="datablock">Correlation coefficient between tips and size is         0.4892987752303577
For tip, the highest correlation is         ('total_bill', 0.6757341092113641).
</code></pre>
    </p>

<p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import your file for testing.</p>


<p></p><p><a name="p24"><b>Program 24: Enrollments.</b></b> &emsp; <i>Due noon, Monday, 18 October.</i>
    <br>(Learning Objective: to use standard Pandas functions to filter rows, aggregates values and create new columns.)
    <p>

    Write a function, <code class="inline">computeEnrollments()</code>, that takes a DataFrame that contains students' names, number of credits completed, and current courses (a string with the course names separated by ` `), and returns a DataFrame that
    <ol>
        <li> Includes only students taking 3 or more courses, and
        <li> Replaces the column of current courses with three different columns: the first that counts total number of courses the student is taking, the second columns has the number of computer science courses currently taking (all courses that start 'csci') and the third column with the number of other courses the student is taking.
      </ol>

    <p>
    For example, assuming your function <code class="inline">computeEnrollments()</code> was in the <code class="inline">p24.py</code>:
      <pre><code class="blockcode">classDF = pd.DataFrame({'Name': ["Ana","Bao","Cara","Dara","Ella","Fatima"],\
                      '# Credits': [45,50,80,115,30,90],\
                      'Current Courses': ["csci160 csci235 math160 jpn201",\
                                          "csci160 csci235 cla101 germn241",\
                                          "csci265 csci335 csci39542 germn241",\
                                          "csci49362 csci499",\
                                          "csci150 csci235 math160",\
                                          "csci335 csci39542 cla101 dan102"]})
print(f'Starting df:\n {classDF}')
print(f'Ending df:\n {p24.computeEnrollments(classDF)}')
</code></pre>
      Would give output:
  <pre><code class="datablock">Starting df:
      Name  # Credits                     Current Courses
0     Ana         45      csci160 csci235 math160 jpn201
1     Bao         50     csci160 csci235 cla101 germn241
2    Cara         80  csci265 csci335 csci39542 germn241
3    Dara        115                   csci49362 csci499
4    Ella         30             csci150 csci235 math160
5  Fatima         90     csci335 csci39542 cla101 dan102

Ending df:
      Name  # Credits  NumCourses  CS  Other
0     Ana         45           4   2      2
1     Bao         50           4   2      2
2    Cara         80           4   3      1
4    Ella         30           3   2      1
5  Fatima         90           4   2      2
</code></pre>
      <p>The resulting DataFrame has only 5 students, since the student, Dara, has fewer than 3 current courses and that row is dropped.

<p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>

<p><i>Hints:
    <ul>
      <li> While straightforward, this problem is much easier to do in pieces, debugging/printing, as you go.  Add lots of print statements (or work inside a debugger) to make sure each part is correct before building on the next one.
      <li> To compute the number of courses taken, count the number of spaces and add 1.
      <li> Using the string (<code class="inline">.str</code>) methods are an excellent approach, but you may get a <code class="inline">SettingWithCopyWarning</code> since it is concerned that the new column is being set based on a subsetted value (i.e. the particular column).  To explicitly assign the values, use <code class="inline">df.assign</code>.  For example, if you have computed the number of courses and stored them in a Series, <code class="inline">totalCourses</code>, you can then <code class="inline">df = df.assign(NumCourses = totalCourses)</code> to create a new column, <code class="inline">NumCourses</code>, in the DataFrame.
    </ul>
  </i>
    </p>


<p></p><p><a name="p25"><b>Program 25: PMF of Senators' Ages.</b></b> &emsp; <i>Due noon, Tuesday, 19 October.</i>
    <br>(Learning Objective: to build intuition and strengthen competency with probability mass functions by analysing ages of public officials.)
    <p>

    <a href="http://www.textbook.ds100.org/ch/16/prob_random_vars.html">Section 16.1 (Random Variables)</a> of the textbook has a small example of computing the probability mass function of a data set of ages was computed by hand.  Write a function that will automate this process:

    <ul>
      <li> <code class = "inline">pmf(vals)</code>:  This function takes one parameter:
      <ul>
          <li> <code class = "inline">vals</code>: an iterable of numeric values, assumed to be positive numbers.
      </ul>
      <li> The function should return two iterables of positive numbers:
        <ul>
            <li> <code class = "inline">uniqueVals</code>: a sorted list of the unique values from the input parameter <code class = "inline">vals</code>, and
            <li> <code class = "inline">mass</code>: the fraction that each <code class = "inline">uniqueVals</code> occurs in <code class = "inline">vals</code>.
          </ul>
          <li> The order of <code class = "inline">mass</code> should follow that of <code class = "inline">uniqueVals</code>.  That is, the ith element of       <code class = "inline">mass</code> should be the fraction computed for the ith value of <code class = "inline">uniqueVals</code>.
          <li> The <code class = "inline">sum(mass)</code> should equal 1.
      </ul>



    <p>For example, calling the function on the example from the textbook:
      <pre><code class="blockcode">x, y = p25.pmf([50,50,52,54])
print(f'The values are: {x}')
print(f'The pmf is: {y}')
print(f'The sum of the pmf is: {sum(y)}.')
plt.bar(x,y)
plt.show()</code></pre>
      would print:
      <pre><code class="datablock">The values are: (50, 52, 54)
The pmf is: (0.5, 0.25, 0.25)
The sum of the pmf is: 1.0.</code></pre>

      and would give the plot:

      <p><img height=200 src="pmf_small_age.png">

    <p>For example, calling the function on the senators' ages from <a href="#p3">Program 3</a>:
    <pre><code class="blockcode">senators = pd.read_csv('senatorsAges.csv')
xSen,ySen = p25.pmf(senators['age'])
plt.bar(xSen,ySen)
plt.show()</code></pre>

    would give the plot:

    <p><img height=200 src="pmf_senators_age.png">


    <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>

    <p>
        <i>Hint:  Include only the libraries you need (such as <code class = "inline">numpy</code>) and none of the ones for plotting (such as <code class = "inline">matplotlib.pyplot</code> and <code class = "inline">seaborn</code>) since this function is computing and not plotting.
        </i>
    </p>


<p></p><p><a name="p26"><b>Program 26: Weekday Entries.</b></b> &emsp; <i>Due noon, Thursday, 21 October.</i>
    <br>(Learning Objective: to strengthen data cleaning skills and familiarity with standard date/time formats.)
    <p>

    Use the <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#overview">date time functionality</a> of Pandas to write the following functions:
    <ul>
      <li> <code class = "inline">tripTime(start,end)</code>:  This function takes two variables of type <code class = "inline">datetime</code> and returns the difference between them.
      <li> <code class = "inline">weekdays(df,col)</code>:  This function takes a DataFrame, <code class = "inline">df</code>, containing the column name, <code class = "inline">col</code>, and returns a DataFrame containing only times that fall on a weekday (i.e. Monday through Friday).
    </ul>

    Give example with green taxi data from seaborn, use first 10 lines, and create new column as well as filter for weekdays.
    <p>
    For example, using the <a href="https://github.com/mwaskom/seaborn-data">Seaborn's Green Taxi Data Set</a> and assuming your functions are in the <code class="inline">p26.py</code>:
    <pre><code class="blockcode">taxi = sns.load_dataset('taxis')
print(taxi.iloc[0:10])  #Print first 10 lines:
taxi['tripTime'] = taxi.apply(lambda x: p26.tripTime(x['pickup'], x['dropoff']), axis=1)
print(taxi.iloc[0:10])
</code></pre>
    Would give output:
<pre><code class="datablock">                pickup              dropoff  ...  pickup_borough  dropoff_borough
0  2019-03-23 20:21:09  2019-03-23 20:27:24  ...       Manhattan        Manhattan
1  2019-03-04 16:11:55  2019-03-04 16:19:00  ...       Manhattan        Manhattan
2  2019-03-27 17:53:01  2019-03-27 18:00:25  ...       Manhattan        Manhattan
3  2019-03-10 01:23:59  2019-03-10 01:49:51  ...       Manhattan        Manhattan
4  2019-03-30 13:27:42  2019-03-30 13:37:14  ...       Manhattan        Manhattan
5  2019-03-11 10:37:23  2019-03-11 10:47:31  ...       Manhattan        Manhattan
6  2019-03-26 21:07:31  2019-03-26 21:17:29  ...       Manhattan        Manhattan
7  2019-03-22 12:47:13  2019-03-22 12:58:17  ...       Manhattan        Manhattan
8  2019-03-23 11:48:50  2019-03-23 12:06:14  ...       Manhattan        Manhattan
9  2019-03-08 16:18:37  2019-03-08 16:26:57  ...       Manhattan        Manhattan

[10 rows x 14 columns]
                pickup              dropoff  ...  dropoff_borough        tripTime
0  2019-03-23 20:21:09  2019-03-23 20:27:24  ...        Manhattan 0 days 00:06:15
1  2019-03-04 16:11:55  2019-03-04 16:19:00  ...        Manhattan 0 days 00:07:05
2  2019-03-27 17:53:01  2019-03-27 18:00:25  ...        Manhattan 0 days 00:07:24
3  2019-03-10 01:23:59  2019-03-10 01:49:51  ...        Manhattan 0 days 00:25:52
4  2019-03-30 13:27:42  2019-03-30 13:37:14  ...        Manhattan 0 days 00:09:32
5  2019-03-11 10:37:23  2019-03-11 10:47:31  ...        Manhattan 0 days 00:10:08
6  2019-03-26 21:07:31  2019-03-26 21:17:29  ...        Manhattan 0 days 00:09:58
7  2019-03-22 12:47:13  2019-03-22 12:58:17  ...        Manhattan 0 days 00:11:04
8  2019-03-23 11:48:50  2019-03-23 12:06:14  ...        Manhattan 0 days 00:17:24
9  2019-03-08 16:18:37  2019-03-08 16:26:57  ...        Manhattan 0 days 00:08:20

[10 rows x 15 columns]
</code></pre>

  <p> Using the function our second function:
<pre><code class="blockcode">taxi = sns.load_dataset('taxis')
weekdays = p26.weekdays(taxi,'pickup')
print(weekdays.iloc[0:10])
</code></pre>
  will give output:
<pre><code class="datablock">
  pickup              dropoff  ...  pickup_borough  dropoff_borough
1   2019-03-04 16:11:55  2019-03-04 16:19:00  ...       Manhattan        Manhattan
2   2019-03-27 17:53:01  2019-03-27 18:00:25  ...       Manhattan        Manhattan
5   2019-03-11 10:37:23  2019-03-11 10:47:31  ...       Manhattan        Manhattan
6   2019-03-26 21:07:31  2019-03-26 21:17:29  ...       Manhattan        Manhattan
7   2019-03-22 12:47:13  2019-03-22 12:58:17  ...       Manhattan        Manhattan
9   2019-03-08 16:18:37  2019-03-08 16:26:57  ...       Manhattan        Manhattan
11  2019-03-20 19:39:42  2019-03-20 19:45:36  ...       Manhattan        Manhattan
12  2019-03-18 21:27:14  2019-03-18 21:34:16  ...       Manhattan        Manhattan
13  2019-03-19 07:55:25  2019-03-19 08:09:17  ...       Manhattan        Manhattan
14  2019-03-27 12:13:34  2019-03-27 12:25:48  ...       Manhattan        Manhattan

[10 rows x 14 columns]
</code></pre>
    note that rows 0,4,8, and 10 have been dropped from the original DataFrame since those corresponded to weekend days.
    </p>

<p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>


    <i>Hints:
        <ul>
          <li> When read in from the CSV, the columns may be stored as a string.  Cast as a <code class = "inline">datetime</code> object (e.g. <code class = "inline">pd.to_datetime(start)</code>) to use the functionality.
          <li> For datetime objects, you can access properties such as day of the week using <code class = "inline">dt</code> prefix, similar to <code class = "inline">.str</code> similar to .str to use string methods and properties (e.g. <code class = "inline">dt.dayofweek</code>).
          See the Python Docs: <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#overview">date time functionality</a> for more details.
        </ul>
    </i>


    </p>





<p></p><p><a name="p27"><b>Program 27: Fitting OLS.</b></b> &emsp; <i>Due noon, Friday, 22 October.</i>
    <br>(Learning Objective: to build intuition and strengthen competency with least squares method of minimizing functions.)
    <p>


    Write a function, <code class = "inline">compute_r_line()</code>, that takes two iterables of numeric values
    representing the independent variable (<code class = "inline">xes</code>) and the dependent
    variable (<code class = "inline">yes</code>) and computes the slope and y-intercept of the
    linear regression line using ordinary least squares.

    See <a href="https://inferentialthinking.com/chapters/15/2/Regression_Line.html">DS 8:  Chapter 15</a>
    The pseudocode for this:
    <ol>
      <li> Compute the standard deviation of the <code class = "inline">xes</code> and <code class = "inline">yes</code>.  Call these <code class = "inline">sd_x</code> and <code class = "inline">sd_y</code>.
      <li> Compute the correlation, <code class = "inline">r</code>,  of the <code class = "inline">xes</code> and <code class = "inline">yes</code>.
      <li> Compute the slope, <code class = "inline">m</code>, as <code class = "inline">m = r*sd_y/sd_x</code>.
      <li> Compute the y-intercept, <code class = "inline">b</code>, as <code class = "inline">b = yes[0] - m * xes[0]</code>
      <li> Return <code class = "inline">m</code> and <code class = "inline">b</code>.
    </ol>

    <p>For example, calling the function on the example from the textbook:
      <pre><code class="blockcode">s1 = [1,2,3,4,5,6,7,8,9,10]
s2 = [0,1,1,2,2,3,3,4,4,5,]
m, b = p27.compute_r_line(s1,s2)
print(m,b)
xes = np.array([0,10])
yes = m*xes + b
plt.scatter(s1,s2)
plt.plot(xes,yes)
plt.title(f'Regression line with m = {m:{4}.{2}} and y-intercept = {b:{4}.{2}}')
plt.show()</code></pre>
      would give the plot:

      <p><img height=300 src="rline_simple.png">

    <p>For example, calling the function on the senators' ages from <a href="#p3">Program 3</a>:
    <pre><code class="blockcode">taxi = sns.load_dataset('taxis')
m, b = p27.compute_r_line(taxi['total'],taxi['tip'])
print(m,b)
xes = np.array([0,175])
yes = m*xes + b
plt.scatter(taxi['total'],taxi['tip'])
plt.plot(xes,yes,color='red')
plt.title(f'Regression line for total vs. tips with m = {m:{4}.{2}} and y-intercept = {b:{4}.{2}}')
plt.xlabel('Total Paid')
plt.ylabel('Tip')
plt.show()</code></pre>

    would give the plot:

    <p><img height=300 src="rline_taxi.png">


    <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>

    <p>
        <i>Hint:  Include only the libraries you need (such as <code class = "inline">numpy</code>) and none of the ones for plotting (such as <code class = "inline">matplotlib.pyplot</code> and <code class = "inline">seaborn</code>) since this function is computing and not plotting.
        </i>
    </p>



<!--
    Show using the taxi data, green taxi, also plot an estimate of 15% tips in yellow, and use above to give the regression line.

      Fitting LM's to Taxi Data.
         Run the linear model with a single parameter on a parameter specified by user.  Set up as a function?  Use the book's plotting to display.  Give small sample file to test on.

      Use sns to do a jointplot:  https://seaborn.pydata.org/tutorial/regression.html
    </p>
-->
<p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>

<p></p><p><a name="p28"><b>Program 28: CS Courses.</b></b> &emsp; <i>Due noon, Monday, 25 October.</i>
    <br>(Learning Objective: to strengthen data cleaning skills using Pandas.)
    <p>

      In <a href="#p24">Program 24</a>, we wrote a function that counted courses that students are currently taking.  For this program,
      write a function that takes a DataFrame and returns a sorted list of the computer science courses taken.  Each course should occur once in the list, no matter how often it occurs in the list of courses taken by the students
    <ul>
        <li> <code class="inline">csCourses(df):</code>  This function takes as input a data frame, <code class="inline">df</code>, that contains a column <code class="inline">Current Courses</code>.  It returns a sorted list of unique strings. Each string is a computer science course (i.e. starts with <code class="inline">csci</code>) that occurs in some entry of <code class="inline">df['Current Courses']</code>.
      </ul>

      <p>
      For example, assuming your function <code class="inline">csCourses(df)</code> was in the <code class="inline">p28.py</code>:
      <pre><code class="blockcode">classDF = pd.DataFrame({'Name': ["Ana","Bao","Cara","Dara","Ella","Fatima"],\
                         '# Credits': [45,50,80,115,30,90],\
                         'Current Courses': ["csci160 csci235 math160 jpn201",\
                                             "csci160 csci235 cla101 germn241",\
                                             "csci265 csci335 csci39542 germn241",\
                                             "csci49362 csci499",\
                                             "csci150 csci235 math160",\
                                             "csci335 csci39542 cla101 dan102"]})


print(f'Starting df:\n {classDF}\n')
print(f'CS courses:\n {p28.csCourses(classDF)}')
</code></pre>
      Would give output:
  <pre><code class="datablock">Starting df:
      Name  # Credits                     Current Courses
0     Ana         45      csci160 csci235 math160 jpn201
1     Bao         50     csci160 csci235 cla101 germn241
2    Cara         80  csci265 csci335 csci39542 germn241
3    Dara        115                   csci49362 csci499
4    Ella         30             csci150 csci235 math160
5  Fatima         90     csci335 csci39542 cla101 dan102

CS courses:
 ['csci150', 'csci160', 'csci235', 'csci265', 'csci335', 'csci39542', 'csci49362', 'csci499']
</code></pre>

<p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>


      <p><i>Hints:
        <ul>
          <li> A useful function for this task is <code class="inline">unique()</code> which lists all the values in a Series exactly once.  For example, <code class="inline">classDF['course'].unique()</code> yields:
          <pre><code class="datablock">['csci160 csci235 math160 jpn201' 'csci160 csci235 cla101 germn241' 'csci265 csci335 csci39542 germn241'
 'csci49362 csci499' 'csci150 csci235 math160' 'csci335 csci39542 cla101 dan102']</code></pre>

          This list can then be processed to find the individual courses.
        </ul>
      </i>
    </p>



    </p>


<p></p><p><a name="p29"><b>Program 29: Predictions with MLM's.</b></b> &emsp; <i>Due noon, Tuesday, 26 October.</i>
    <br>(Learning Objective: to build intuition and strengthen competency with existing methods for computing multiple linear regression.)
    <p>
    <p>
      Write a program that asks the user for the following inputs:
      <ul>
          <li> the name of a CSV file that will be loaded into a dataframe,
          <li> the name of a column to be used as an independent variable in the multiple linear regression,
          <li> the name of second column to be used as another independent variable in the multiple linear regression,
          <li> the name of a column to be used as an dependent variable in the multiple linear regression (i.e. what is being predicted),
          <li> a value for the first variable, and
          <li> a value for the second variable.
      </ul>
      Your program should build a linear model based on the dataframe and two independent variables that will predict the value of the dependent variable. Your program should then predict the dependent variable, based on the two inputted independent variables.

      <p>Use the <code class=inline>LinearRegression()</code> from
        <code class=inline>scikit-learn</code> to fit the model and predict the value.
      For example, if you were working with the <code class=inline>mpg</code> dataset from <code class=inline>seaborn</code>:

      <pre><code class="blockcode">from sklearn import linear_model
regr = linear_model.LinearRegression()
regr.fit(mpg[['horsepower','weight']], mpg['mpg'])
</code></pre>
      would fit the model to the independent variables horsepower and weight to predict the dependent variable mpg.

      <p>To predict, using this model:
      <pre><code class="blockcode">New_horsepower = 200
New_weight = 3500
print (f'Predicted value: {regr.predict([[New_horsepower,New_weight]])[0]}')
</code></pre>
      would print:
      <pre><code class="blockcode">Predicted value: 15.900087446128559
</code></pre>
      <p>
      A sample run of your program would look like:
      <pre><code class="blockcode">Enter name of CSV:  mpg.csv
Enter name of first independent variable: displacement
Enter name of second independent variable: acceleration
Enter name of the dependent variable: mpg
Enter value for first variable for prediction: 100
Enter value for second variable for prediction: 12.0
</code></pre>
      which would output:
      <pre><code class="blockcode">Predicted mpg:  29.400598924519038
</code></pre>
      Your output should contain the dependent variable name followed by a colon and the predicted value.

      <p>Note: your program should ask separately ask for input 6 times, in the order listed above.  Changing the order or combining the inputs into fewer lines will cause the autograder to crash.




<p></p><p><a name="p30"><b>Program 30: Computing Ranges.</b></b> &emsp; <i>Due noon, Thursday, 28 October.</i>
    <br>(Learning Objective: to increase understanding and skills for manipulating numeric and date data.)
    </p>

    <p>
    Write a function that computes the range of values a column takes (i.e. the difference
    between the maximum and minimum values).  The column contains
    numeric values, unless the flag <code class = "inline">datetime</code> is set to
    <code class = "inline">True</code>.  If the datetime flag is true,
    the input column contains strings representing datetime objects (
    <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#overview">overview of datetime in Pandas</a>) and the function should return the range in seconds.

    <ul>
        <li> <code class = "inline">colRange(df, colName, datetime=False)</code>:
          This function takes three inputs:
          <ul>
              <li> <code class = "inline">df</code>: a DataFrame that
                  including the specified column.
              <li> <code class = "inline">colName</code>: a column name of the specified DataFrame,
              <li> <code class = "inline">datetime</code>: a Boolean variable that indicates if the column contains a datetime string.  The default value is False and that the column contains numeric values.  When set to True, the function inteprets the column values as datestring objects.
          </ul>
          The function computes the range of the values in the Series <code class = "inline">df[colName]</code> and returns a numeric answer.  If the <code class = "inline">datetime</code> flag is set, the total number of seconds between the minimum and maximum values is returned.
        </ul>

    <p>
    For example, assuming your function <code class="inline">colRange()</code> was in the <code class="inline">p30.py</code>:
    <pre><code class="blockcode">simpleDF = pd.DataFrame({'id': [1,2,3,4],\
    'checkin': ["2019-03-23 20:21:09","2019-03-23 20:27:24",\
                "2019-03-22 12:47:13","2019-03-22 12:58:17"],\
    'total': [32.51,19.99,1.05,20.50]})
print(f"Testing colRange(simpleDF,'id'): {p30.colRange(simpleDF,'id')}")
print(f"Testing colRange(simpleDF,'checkin',datetime=True): {p30.colRange(simpleDF,'checkin',datetime=True)}")
</code></pre>
      Would give output:
  <pre><code class="datablock">Testing colRange(simpleDF,'id'): 3
Testing colRange(simpleDF,'checkin',datetime=True): 114011.0</code></pre>

    <p> Using the function on the 10 lines of seaborn taxis dataset:
  <pre><code class="blockcode">import seaborn as sns
taxis = sns.load_dataset('tips').dropna().loc[:10]
print(f"Testing colRange(taxis,'total'): {p30.colRange(taxis,'total')}")
print(f"Testing colRange(taxis,'dropoff',datetime=True): {p30.colRange(taxis,'dropoff',datetime=True):}")
</code></pre>
    will print
  <pre><code class="datablock">Testing colRange(taxis,'distance'): 7.21
Testing colRange(taxis,'dropoff',datetime=True): 2236694.0
</code></pre>
      </p>

  <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>

      <p><i>Hint:  See
        <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#overview">Pandas' datetime overview</a>.
A useful function is <code class="inline">dt.total_seconds()</code>
        which returns the total seconds of a <code class="inline">datetime</code>
        object.</i>


<p></p><p><a name="p31"><b>Program 31: Sampling Distributions.</b></b> &emsp; <i>Due noon, Friday, 29 October.</i>
      <br>(Learning Objective: to refresh understanding of normal distributions and introduce sampling standard techniques in Python.)
    </p>

    <p>
    In Lecture #13, we introduced the <code class="inline">pd.sample()</code>
    function for sampling rows of a DataFrame.
    Echoing the <a href="https://www.zoology.ubc.ca/~whitlock/Kingfisher/CLT.htm">Central Limit Theorem</a>UBC sampling demo</a>,
    write a function that computes sample means for a given column of a DataFrame and returns a <code class="inline">numpy</code> array of those means.

    <ul>
      <li> <code class = "inline">sampleMeans(df, colName, k=10, n=1000)</code>:
        This function takes four inputs:
        <ul>
            <li> <code class = "inline">df</code>: a DataFrame that
                  including the specified column.
            <li> <code class = "inline">colName</code>: a column name of the specified DataFrame,
            <li> <code class = "inline">k</code>: the size of the sample.  It has a default value of 10.
            <li> <code class = "inline">n</code>: the number of samples.  It has a default value of 1000.
        </ul>
        It takes the sample size <code class="inline">k</code> and the numbers of samples,
          of <code class="inline">n</code>, a DataFrame, <code class="inline">df</code>, and
          and generates <code class="inline">n</code>
          samples of size <code class="inline">k</code>, computes the sample mean of each, and returns a numpy array of those means.
        </ul>

      <p>
      For example, assuming your function <code class="inline">sampleMeans()</code> was in the <code class="inline">p31.py</code>:
      <pre><code class="blockcode">nd = [np.random.normal() for i in range(1000)]
ed = [np.random.exponential() for i in range(1000)]
df = pd.DataFrame({ "nd" : nd, "ed" : ed})
print(p31.sampleMeans(df, 'nd', k = 5, n=5))
print(p31.sampleMeans(df, 'nd', k = 10, n=5))
</code></pre>
      would print in a sample run
<pre><code class="datablock">[ 0.18006227 -0.02046562  0.13301251  0.52114451  0.47197969]
[ 0.06028354 -0.48566047  0.02343676 -0.28361692  0.25259547]
</code></pre>

<p>
      Continuing the example:
      <pre><code class="blockcode">k_10 = p31.sampleMeans(df, 'ed', k = 10)
k_20 = p31.sampleMeans(df, 'ed', k = 20)
k_30 = p31.sampleMeans(df, 'ed', k = 30)
sns.histplot([ed,k_10,k_20,k_30],element="poly")
plt.title('Means of 1000 samples of an exponential distribution')
plt.show()
</code></pre>
      would display:
      <p>
      <img src="sample_means.png" height=300>

      <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>


<p></p><p><a name="p32"><b>Program 32: Attendance.</b></b> &emsp; <i>Due noon, Monday, 1 November.</i>
      <br>(Learning Objective: to introduce simple feature engineering and reinforce datetime skills.)
      <p>
      In Lectures #14 and #15, we discussed the hypothesis that NYC public schools have lower attendance on Fridays.  For this program, write a function that takes a DataFrame of school attendance records (following the format from <a href="https://data.cityofnewyork.us/Education/2018-2019-Daily-Attendance/x3bb-kg5j">NYC OpenData</a>) and returns the correlation coefficent between the day of the week and daily attendance (computed as a percentage of students present of those enrolled at the school).
      </p>

      <ul>
          <li> <code class = "inline">attendCorr(df)</code>:
              <ul>
                <li> This function takes a DataFrame <code class = "inline">df</code>, with columns <code class = "inline">School DBN</code>,
                  <code class = "inline">Date</code>,
                  <code class = "inline">Enrolled</code>,
                  <code class = "inline">Absent</code>, and
                  <code class = "inline">Present</code>.
                <li> The function computes the attendance as a percentage of students present over students enrolled, and calculates the day of the week for each date.  The function returns the correlation coefficient of the two.
          </ul>
        </ul>
        <p>
        For example, assuming your function <code class="inline">attendCorr()</code> was in the <code class="inline">p32.py</code>:
        <pre><code class="blockcode">df = pd.read_csv('dailyAttendanceManHunt2018.csv')
print(p32.attendCorr(df))
</code></pre>
        would print <code class="inline">-0.014420727967150241</code> for the <a href="dailyAttendanceManHunt2018.csv">sample data set</a> for Manhattan Hunter High School (see lecture notes for obtaining additional datasets).
        A plot of the data is:

          <p>
          <img src="attend_manhunt.png" height=300>

          <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>


      <p>
      <i>Hints:
          <ul>
            <li> When read in from the CSV, the columns may be stored as a string.  Cast as a <code class = "inline">datetime</code> object (e.g. <code class = "inline">pd.to_datetime()</code>), to use the functionality.  You may need to specify the format, since the DOE stored dates as <code class = "inline">YYYYMMDD</code> (see <a href="https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html">Panda Docs</a>).
            <li> For datetime objects, you can access properties such as day of the week using <code class = "inline">dt</code> prefix, similar to <code class = "inline">.str</code> similar to .str to use string methods and properties (e.g. <code class = "inline">dt.dayofweek</code>).
            See the Python Docs: <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#overview">date time functionality</a> for more details.
            <li> In Lecture #15, we introduced several ways to add features to datasets to aid in the analysis.  For this program, add a new column that indicates the days of the week:  0 for Monday, 1 for Tuesday, ... 6 for Sunday (a useful function for this is: <code class = "inline">dt.dayofweek</code>)).
          </ul>
      </i>

      <!--
              https://www.nycenet.edu/PublicApps/Attendance.aspx
              https://www.schools.nyc.gov/about-us/reports/doe-data-at-a-glance
              https://infohub.nyced.org/reports/school-quality/information-and-data-overview
              https://data.cityofnewyork.us/Education/2018-2019-Daily-Attendance/x3bb-kg5j
      -->
      </p>



<p></p><p><a name="p33"><b>Program 33: Confidence Intervals.</b></b> &emsp; <i>Due noon, Tuesday, 2 November.</i>
      <br>(Learning Objective: to reinforce concepts from prerequisite statistics course and build corresponding facility in Python.)
      <p>

      In Lecture #13, we discussed the <a href="https://www.zoology.ubc.ca/~whitlock/Kingfisher/CIMean.htm">UBC confidence interval demo</a>, where a normal distribution (of length of fish) was repeated sampled.  For each sample, the confidence interval of the sample mean was computed and stored.  And, for each sample, it was checked if the true mean of the distribution was contained in the confidence interval of the sample mean:

      <p>
      <iframe src="https://www.zoology.ubc.ca/~whitlock/Kingfisher/CIMean.htm" style="width: 50%; height: 500px" name="internal"></iframe>

      <p>
      For this program, echo the UBC confidence interval demo.  Write a function
      that computes the confidence intervals and tabulates the running successes of the confidence interval of the sample mean capturing the true mean of the population:
          <ul>
            <li> <code class = "inline">ciRuns(alpha = 0.95, mu = 0, sigma = 1, size = 10, trials = 100)</code>:
              <ul>
              <li>This function takes five inputs:
              <ul>
                  <li> <code class = "inline">alpha</code>: the fraction of the distribution contained in the interval (as in <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html">scipy.stats.t.interval()</a>).  It has a default value of 95.
                  <li> <code class = "inline">mu</code>: the mean of the normal distribution sampled.  It has a default value of 0.
                  <li> <code class = "inline">sigma</code>: the standard deviation of the normal distribution sampled.  It has a default value of 1.
                  <li> <code class = "inline">size</code>: the size of the samples.  It has a default value of 10.
                  <li> <code class = "inline">trials</code>: the number of samples.  It has a default value of 100.
              </ul>
              <li> This function returns:
                <ul>
                    <li> A list of intervals, stored as tuples of their lower and upper values (of length <code class = "inline">trials</code>), and
                    <li> A list of length <code class = "inline">trials</code> containing the percentage of successful predictions after each trial.  That is, the ith entry has the percent of the first i trials for which the true mean (<code class = "inline">mu</code>) is in the confidence interval computed for the sample.
                </ul>
              </ul>
              </ul>

      <p>
      For example, assuming your function <code class="inline">ciRuns()</code> was in the <code class="inline">p33.py</code>, then a possible run is:
      <pre><code class="blockcode">intervals, successes = p33.ciRuns(trials = 20)
print(f"intervals: {intervals}")
print(f"successes: {successes}")
      </code></pre>
            would print in a sample run
      <pre><code class="datablock">intervals: [(0.0843959275632028, 1.3323778628928307), (-0.146668094360358, 1.5546642787617675), (-1.029505009635772, 0.5272177024225991), (-0.5702633299624739, 0.5144718024588405), (-0.3979475729570697, 1.1005279531825056), (-0.9894141075519297, 0.8070447535623141), (-1.0433450932702595, 0.7059405804735273), (-0.8902508132395719, 0.3772852944801963), (-1.1068858052695578, 0.0816760750250739), (-0.3661920360152307, 1.003198126280235)]
successes: [0.0, 50.0, 66.66666666666667, 75.0, 80.0, 83.33333333333333, 85.71428571428571, 87.5, 88.88888888888889, 90.0]</code></pre>
      Since the first inteval doesn't contain the mean <code class = "inline">mu = 0</code>, the first entry in successes is 0.  The next interval does contain the mean, so half or 50 percent of the first two runs have been successful.  Similarly, for each of the remaining runs, the running total of percent successful continues to increase until it reaches 90 percent.  Since we are generating the samples randomly, these numbers will change from run to run, and as we increase the number of trials, the percentage success will converge to <code class = "inline">alpha = 95</code>.

      <p>
      Another possible run, where we plot the values to see the results better:
        <pre><code class="blockcode">import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
intervals, successes = p33.ciRuns(mu=500, sigma=100, alpha = .90, trials = 1000)
xes = np.linspace(1,1000,1000)
yes = 90*np.ones(1000)
plt.scatter(xes,successes)
plt.plot(xes,yes,color='red')
plt.title('alpha=90, mu = 500, sigma=100, & trials=1000')
plt.show()</code></pre>


            would display:
            <p>
            <img src="ciRun_alpha_90.png" height=300>

    <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>


    <p>
      <i>Hints:
        <ul>
          <li> To generate confidence intervals, you can do it by hand (see <a href="https://www.zoology.ubc.ca/~whitlock/Kingfisher/CIMean.htm">UBC confidence interval demo</a>) or use the functions from scipy.stats:
          <pre><code class="blockcode">import scipy.stats as st
interval = st.t.interval(alpha,len(sampData)-1, loc=np.mean(sampData),scale=st.sem(sampData))</code></pre>
          where <code class="inline">alpha</code> is as above, and <code class="inline">sampData</code> is your sample.
          <li> To generate random samples from a normal distribution, we have been using <code class="inline">np.random.normal</code> (see <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html">numpy documentation</a>), but
          you can other packages, such as <code class="inline">sklearn</code> if you prefer.
        </ul>
      </i>

<p></p><p><a name="p34"><b>Program 34: Polynomial Features.</b></b> &emsp; <i>Due noon, Thursday, 4 November.</i>
      <br>(Learning Objective: to strenghten understanding of regression models and employ thresholds to decide model fitness.)

      <p>Following the textbook code demostration in Lecture 16, write a function that takes values of an independent variable and corresponding values of a dependent varaible, and fits polynomial regression models of increasing degree until the MSE falls below error.

      <ul>
        <li> <code class = "inline">fitPoly(df,xes,yes,epsilon=0.01)</code>:
          This function takes four inputs:
            <ul>
              <li> <code class = "inline">df</code>: a DataFrame that
                              including the specified columns.
              <li> <code class = "inline">xes</code>: a column name of the specified DataFrame,
              <li> <code class = "inline">yes</code>: a column name of the specified DataFrame.
              <li> <code class = "inline">epsilon</code>: the size of the sample.  It has a default value of 0.01.
            </ul>
          It returns the smallest intger degree >= 1 for which the model yields a MSE of < the specified epsilon.
      </ul>


      <p>For example, assuming your function <code class="inline">fitPoly()</code> was in the <code class="inline">p34.py</code> and the code is the <a href="https://raw.githubusercontent.com/DS-100/textbook/master/content/ch/20/icecream.csv">ice cream ratings example</a> from <a href="http://www.textbook.ds100.org/ch/20/feature_polynomial.html">Chapter 20</a>,
      <pre><code class="blockcode">df = pd.read_csv('icecream.csv')
print(f'Starting df:\n {df}')
eps = 0.5
deg = p34.fitPoly(df,'sweetness','overall',epsilon=eps)
print(f'For epsilon = {eps}, poly has degree {deg}.')</code></pre>
      would print in a sample run:
      <pre><code class="datablock">Starting df:
    sweetness  overall
0        4.1      3.9
1        6.9      5.4
2        8.3      5.8
3        8.0      6.0
4        9.1      6.5
5        9.8      6.1
6       11.0      5.9
7       11.7      5.5
8       11.9      5.4
For epsilon = 0.5, poly has degree 1.</code></pre>

      <p>
      Continuing the example, if we lower the threshold,
                  <pre><code class="blockcode">eps= 0.1
deg = p34.fitPoly(df,'sweetness','overall',epsilon=eps)
print(f'For epsilon = {eps}, poly has degree: {deg}.')</code></pre>
            would print in a sample run:
            <pre><code class="datablock">For epsilon = 0.1, poly has degree: 2.</code></pre>
      <p>
      If we lower the threshold to the default (0.01),
                              <pre><code class="blockcode">eps= 0.01
deg = p34.fitPoly(df,'sweetness','overall')
print(f'For epsilon = {eps}, poly has degree: {deg}.')</code></pre>
                        would print in a sample run:
                        <pre><code class="datablock">For default epsilon, poly has degree: 8.</code></pre>

    <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>


<p></p><p><a name="p35"><b>Program 35: Parking Tickets.</b></b> &emsp; <i>Due noon, Friday, 5 November.</i>
      <br>(Learning Objective: to build familiarity with different approaches for encoding categorical data.)</p>


      <p> Recent <a href="https://nyc.streetsblog.org/2021/10/13/out-of-state-drivers-are-just-the-worst-at-paying-their-nyc-summonses/">news articles</a> focused on the significantly higher percentage of parking tickets that are unpaid for cars with out-of-state plates:

      <p><img src="streetsblog_unpaid_tickets_20211013.png" height=300px>

      <p>The data is aggregated across the whole city, what happens when focused on individual neighborhoods?  Similarly, a high fraction of motor vehicle collisions involve cars registered out-of-state (see <a href="https://nyc.streetsblog.org/2021/09/21/out-of-state-cars-causing-mayhema-streetsblog-crash-analysis/">crash analysis</a> by Streetsblog NYC).  How does that affect your neighborhood?

      <p>Write a function that will takes a DataFrame and column and returns a new DataFrame with a column that is 1 if the column contains the indicator or 0 otherwise.  Your function should allow the new column name and indicator value to be customized in the parameter list.  The default values are <code class = "inline">Registration State</code> and <code class = "inline">NY</code> for the parameters <code class = "inline">colName</code> and <code class = "inline">indicator</code>.  You can assume that the column contains the indicator as a value and that each row is blank or contains a single categorical value for each row (i.e. will contain <code class = "inline">NY</code> but never two different values such as <code class = "inline">NY, NJ</code>).  The new column should be named by the indicator value
      (i.e. <code class = "inline">NY</code> for the default).

      <ul>
        <li> <code class = "inline">addIndicator(df, colName = "Registration State", indicator = "NY")</code>:
        This function takes three inputs:
              <ul>
                  <li> <code class = "inline">df</code>: a DataFrame that
                        including the specified column.
                  <li> <code class = "inline">colName</code>: a column name of
                        the  specified DataFrame.  The default value is <code class = "inline">Registration State</code>.
                  <li> <code class = "inline">indicator</code>: the value used for
                        the indicator as well as the new column created.
                        The default value is <code class = "inline">NY</code>.
              </ul>
        The function should add a new column, <code class = "inline">indicator</code> to the DataFrame that takes values 1 when <code class = "inline">indicator</code> is in <code class = "inline">df[colName]</code> and 0 if it has a different value and <code class = "inline">nan</code>otherwise.
      </ul>

    <p>
    For example, assuming your function <code class="inline">addIndicator()</code> was in the <code class="inline">p35.py</code>:
    <pre><code class="blockcode">df = pd.read_csv('Parking_Violations_Issued_Precinct_19_2021.csv',low_memory=False)
df['Issue Date'] = pd.to_datetime(df['Issue Date'])
dff = p35.addIndicator(df)
print(dff)
print(f'Of the {len(dff)} violations for first half of 2021 for Upper East Side (PD District 19),\n \
      {len(dff[dff.NY == 1])} are for cars registered in New York.')</code></pre>


      </code></pre>
            would print:
      <pre><code class="datablock">          Summons Number Plate ID  ... Double Parking Violation NY
0           1474094223  KDT3875  ...                      NaN  1
1           1474094600  GTW5034  ...                      NaN  1
2           1474116280  HXM6089  ...                      NaN  1
3           1474116310  HRW4832  ...                      NaN  1
4           1474143209  JPR6583  ...                      NaN  1
...                ...      ...  ...                      ... ..
451504      8954357854  JRF3892  ...                      NaN  1
451505      8955665040   199VP4  ...                      NaN  0
451506      8955665064   196WL7  ...                      NaN  0
451507      8970451729  CNK4113  ...                      NaN  1
451508      8998400418   XJWV98  ...                      NaN  0

[451509 rows x 44 columns]
Of the 451509 violations for first half of 2021 for Upper East Side (PD District 19),
       338282 are for cars registered in New York.</code></pre>
      Continuing the example:
      <pre><code class="blockcode">dfff = p35.addIndicator(dff, colName = 'Vehicle Color', indicator="RED")
print(dfff)
plt.xlim(pd.to_datetime("01/01/21"),pd.to_datetime("06/30/21"))
sns.histplot(data=dfff, x = 'Issue Date', hue = 'RED', binwidth = 7)
plt.title('Parking violations for Upper East Side, Jan-Jul 2021')
plt.show()</code></pre>
      would print:
<pre><code class="datablock">Summons Number Plate ID  ... NY RED
0           1474094223  KDT3875  ...  1   0
1           1474094600  GTW5034  ...  1   0
2           1474116280  HXM6089  ...  1   0
3           1474116310  HRW4832  ...  1   0
4           1474143209  JPR6583  ...  1   0
...                ...      ...  ... ..  ..
451504      8954357854  JRF3892  ...  1   0
451505      8955665040   199VP4  ...  0   0
451506      8955665064   196WL7  ...  0   0
451507      8970451729  CNK4113  ...  1   0
451508      8998400418   XJWV98  ...  0   0

[451509 rows x 45 columns]</code></pre>
      would give the plot:

        <p><img height=400 src="parking_red.png">


  <p>Note: you should submit a file with only the standard comments at the top, this
    function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>

  <i>Hints:
    <ul>
      <li> See classwork from Lecture 17 on setting up indicators as well as downloading from NYC OpenData.
      <li> Parking ticket data can be found at:  <a href="https://data.cityofnewyork.us/City-Government/Parking-Violations-Issued-Fiscal-Year-2021/kvfd-bves">NYC OpenData</a>.
      <li> Collision data can be found at: <a href="https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Vehicles/bm4k-52h4">NYC OpenData</a>.

    </ul>
  </i>


<p></p><p><a name="p36"><b>Program 36: Multiple Locations.</b></b> &emsp; <i>Due noon, Monday, 8 November.</i>
      <br>(Learning Objective: to reinforce data cleaning and aggregation skills for DataFrames.)</p>
        <p>
        The OpenData NYC <a href="https://data.cityofnewyork.us/Transportation/Open-Restaurant-Applications/pitm-atqc
">Open Restaurant Applications</a> dataset of applications from food service establishments seeking authorization to place outdoor seating in front of their business on the sidewalk, roadway, or both.  Some establishments are listed multiple times since they have multiple locations.  Others have duplicate listings for the same location due submitting multiple times or for different kinds of permits (use of sidewalk, use of roadway, etc.)

        <p>Write a program that takes a DataFrame of restaurants and returns a DataFrame with each Restaurant occurring exactly once and two new columns: <code class=inline>Number_Submissions</code> which contains the number of times that restaurant occurs in any entry (smallest value is 1) and <code class=inline>Locations</code>, a list consisting of the unique location addresses.

          <ul>
            <li> <code class = "inline">restaurantLocs(df, dba="Restaurant Name", location="Business Address"):</code>:
              This function takes three inputs:
              <ul>
                  <li> <code class = "inline">df</code>: a DataFrame that
                        including the specified columns.
                  <li> <code class = "inline">dba</code>: a column name of the specified DataFrame containing establishment names.  The default value is "Restaurant Name".
                  <li> <code class = "inline">address</code>:  a column name of the specified DataFrame containing locations.  It has a default value is "Business Address".
              </ul>
              The function returns a DataFrame with three columns:
              <ul>
                  <li> <code class = "inline">dba</code>: where dba is the input parameter above.  This column holds the establishment names and is the column to which <code class = "inline">groupby</code> is applied.
                  <li> <code class = "inline">Num_Submissions</code>: a column that contains a count for each establishment name.
                  <li> <code class = "inline">Locations</code>:  a column name that contains for each establishment, a list of unique locations.
              </ul>
              </ul>

            <p>
            For example, assuming your function <code class="inline">restaurantLocs()</code> was in the <code class="inline">p36.py</code> for the file <a href="applications_coffee_truncated.csv">applications_coffee_truncated.csv</a>, the code:
            <pre><code class="blockcode">df = pd.read_csv('applications_coffee_truncated.csv')
newDF = p36.restaurantLocs(df)
print(newDF)</code></pre>
            <p>would print in:
            <pre><code class="datablock">                                        Num_Submissions                                          Locations
Restaurant Name
BLUESPOON COFFEE                                      1                [76 CHAMBERS STREET, Manhattan, NY]
Black Fox Coffee                                      2  [45 East 45th, Manhattan, NY, 70 Pine Street, ...
Black Press Coffee                                    1                  [274 Columbus Ave, Manhattan, NY]
Blackstone Coffee Roasters                            1                 [502 Hudson Street, Manhattan, NY]
Blank Slate Coffee + Kitchen (Midtown)                1                    [941 2nd Avenue, Manhattan, NY]
Blank Slate Coffee + Kitchen (NoMad)                  1                [121 Madison Avenue, Manhattan, NY]
Blue Bottle Coffee                                    1              [450 West 15th street, Manhattan, NY]
Blue Bottle Coffee Gramercy                           1                    [257 Park Ave S, Manhattan, NY]
Daniels Coffee and more                               1                     [1050  3rd ave, Manhattan, NY]
FOREVER COFFEE BAR                                    1              [714 WEST  181 STREET, Manhattan, NY]
GREGORY'S COFFEE                                      1                   [80 BROAD STREET, Manhattan, NY]
GREGORYS COFFEE                                       2  [551 FASHION AVENUE, Manhattan, NY, 485 LEXING...
GROUND CENTRAL COFFEE COMPANY                         1                      [888 8 AVENUE, Manhattan, NY]
Gregorys Coffee                                      18  [58 West 44th, Manhattan, NY, 649 Broadway, Ma...
JOE: THE ART OF COFFEE                                1              [405 WEST   23 STREET, Manhattan, NY]
Kuro Kuma Espresso & Coffee                           1               [121 La Salle Street, Manhattan, NY]
Lenox Coffee                                          1             [60  West 129th street, Manhattan, NY]
Partners Coffee                                       1                 [44 Charles Street, Manhattan, NY]
Patent Coffee / Patent Pending                        1               [49 West 27th Street, Manhattan, NY]
Ralph's Coffee                                        1                [888 Madison Avenue, Manhattan, NY]
STUMPTOWN COFFEE ROASTERS                             1               [30 WEST    8 STREET, Manhattan, NY]
Starbucks Coffee                                      2                     [605 Third Ave, Manhattan, NY]
Starbucks Coffee Company                              1                     [684  6th ave , Manhattan, NY]
THINK COFFEE                                          1              [500 WEST   30 STREET, Manhattan, NY]
jacks stir brew coffee                                1             [10  10 downing street, Manhattan, NY]
le cafe coffee                                        5  [1440 broadway, Manhattan, NY, 7  east 14 st, ...</code></pre>

        <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>

      <p><i>
      Hints:
      <ul>
          <li>  Read the textbook's <a href="http://www.textbook.ds100.org/ch/06/pandas_aggregating.html">Chapter 6</a> on aggregating data.
          <li>  The ideas in that chapter can be applied to multiple columns, using the the <code class=inline>pd.NamedAgg</code> function inside the aggregation.  In this example from the <a href="https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html">pandas documentation</a>, two new columns, <code class=inline>b_min</code> and<code class=inline>c_sum</code>, based on aggregating existing columns:

            <pre><code class=blockcode>df.groupby("A").agg(
  ...     b_min=pd.NamedAgg(column="B", aggfunc="min"),
  ...     c_sum=pd.NamedAgg(column="C", aggfunc="sum"))

   b_min     c_sum
A
1      1  0.590715
2      3  0.704907</code></pre>
      </ul>
          Think about what functions, built-in or ones that you code, that could be used for <code class=inline>aggfunc</code>.
      </i>






<p></p><p><a name="p37"><b>Program 37: Score Predictor.</b></b> &emsp; <i>Due noon, Tuesday, 9 November.</i>
      <br>(Learning Objective: to introduce logistic regression approaches implemented in sklearn.)</p>

      <p>
      In <a href="http://www.textbook.ds100.org/ch/24/classification_prob.html">Chapter 24</a> and Lectures #17 and #18, we worked through a logistic model to predict scoring attempts based on a single independent variable <code class=inline>shot_distance</code>, as well as a second model that used multiple independent variables, <code class=inline>['shot_distance', 'minute', 'action_type', 'shot_type', 'opponent']</code>.  It was noted that the prediction accurracy increased from 0.6 using just <code class=inline>shot_distance</code> to 0.725 using the entire list.  Are all of those additional variables necessary to get the increased accuracy?

      <p>For this program, write a function that identifies which variable increases the accuracy of the oringal model the most.
        <ul>
          <li> <code class = "inline">def bestForPredict(df, columns, x_col = "shot_distance", y_col = "shot_made", test_size = 40, random_state = 42):</code>:
            This function has six inputs:
            <ul>
                <li> <code class = "inline">df</code>: a DataFrame that
                      including the specified columns.
                <li> <code class = "inline">columns</code>: a list of column names of the specified DataFrame.
                <li> <code class = "inline">x_col</code>:  a column name of the specified DataFrame containing locations.  It is one of the independent variables for the model (the other is from the list <code class = "inline">columns</code>).  It has a default value is "shot_distance".
                <li> <code class = "inline">y_col</code>: a column name of the specified DataFrame containing locations.  This is the dependent variable (what's being predicted) in the model.  It has a default value is "shot_made".
                <li> <code class = "inline">test_size</code>: the size of the test set created when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">40</code>.
                <li> <code class = "inline">random_state</code>: the random seed used when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">42</code>.
            </ul>

            The function returns the highest prediction accuracy found from the columns inputted, as well as the name of the column that increases prediction accuracy the most.
          </ul>

          <p>
          For example, assuming your function <code class="inline">bestForPredict()</code> was in the <code class="inline">p37.py</code> for the file <a href="https://raw.githubusercontent.com/DS-100/textbook/master/content/ch/24/lebron.csv">lebron.csv</a>, the code:
          <pre><code class="blockcode">df = pd.read_csv('lebron.csv')
columns = ['minute', 'action_type', 'shot_type', 'opponent']
acc,col_name = p37.bestForPredict(df,columns)
print(f'The highest accuracy, {acc}, was obtained by including column, {col_name}.')</code></pre>
          would print:
          <pre><code class="datablock">The highest accuracy, 0.725, was obtained by including column, action_type.</code></pre>

          <p>Another example with the same DataFrame:
            <pre><code class="blockcode">columns = ['minute', 'opponent']
acc,col_name = p37.bestForPredict(df,columns, test_size = 100, random_state = 17)
print(f'The highest accuracy, {acc}, was obtained by including column, {col_name}.')
</code></pre>
            would print:
            <pre><code class="datablock">The highest accuracy, 0.6, was obtained by including column, opponent.</code></pre>


      <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>

      <i>
      <p>Hints:
        <ul>
          <li> Some of the code in the textbook is deprecated. In particular, the
            <code class=inline>as_matrix</code> and <code class=inline>orient='row'</code> does not work in newer versions.  Omit the former and replace the latter with:
            <pre><code class="blockcode">rows = df[[x_col,c]].to_dict('records')
onehot = DictVectorizer(sparse=False).fit(rows)</code></pre>
            See Lecture #18 for details.
          <li> If your program stuggles to converge when fitting the model, use the keyword parameter to increase the number of iterations, as we did in lecture:  <code class=inline>clf = LogisticRegression(max_iter = 1000)</code>.

        </ul>
      </i>


<p></p><p><a name="p38"><b>Program 38: Ticket Prep.</b></b> &emsp; <i>Due noon, Thursday, 11 November.</i>
      <br>(Learning Objective: to employ aggregation and data cleaning techniques to prepare data for use in classification.)</p>
      <p>
      </p>

      Can you predict which cars will get excessive number of tickets?  In Lectures #15-18 and <a href="http://www.textbook.ds100.org/ch/24/classification_prob.html">Chapter 24</a>, we focused on the building of the classifers.  This question focuses on the prerequisite step:  preparing the data that is used in building the classifier.

      <p>As a first step, we will group by licence plate number and aggregate the state, vehicle type and color by choosing the first item stored for each:
      <pre><code class=blockcode>newDF =  df.groupby('Plate ID').agg(NumTickets =
    pd.NamedAgg(column = 'Plate ID', aggfunc = 'count'),
    Registration = pd.NamedAgg(column = 'Plate Type', aggfunc = 'first'),
    State = pd.NamedAgg(column = 'Registration State', aggfunc = 'first'),
    VehicleColor = pd.NamedAgg(column = 'Vehicle Color', aggfunc = 'first'))</code></pre>
      <p>While this works well for <code class=inline>State</code>,
        <code class=inline>Registration</code> and <code class=inline>Vehicle Color</code>:
      <pre><code class=blockcode>print(f'Registration: {newDF['Registration'].unique()})
print(f'State: {newDF['State'].unique()})
print(f'VehicleColor: {newDF['VehicleColor'].unique()})</code></pre>
      prints expected values for states but many different types of registrations and abbreviations and mispellings for colors:
      <pre><code class=datablock>Registration: ['PAS' 'COM' 'USC' 'MOT' 'LMB' '999' 'CMB' 'RGL' 'SRF' 'MED' 'APP' 'ORG'
 'ITP' 'OMR' 'TRA' 'BOB' 'SPO' 'LMA' 'VAS' 'OML' 'TOW' 'DLR' 'AMB' 'TRC'
 'STG' 'AGR' 'NLM' 'ORC' 'IRP' 'TRL' 'MCL' 'OMT' 'SCL' 'SPC' 'CHC' 'HIS'
 'SRN' 'RGC' 'PHS' 'PSD' 'MCD' 'NYA' 'JCA' 'SOS' 'CSP' 'OMS' 'CBS' 'OMV'
 'HAM']
State: ['DP' 'NJ' 'PA' 'TX' 'OK' 'NY' 'OH' '99' 'DC' 'AR' 'IL' 'MN' 'NC' 'NV'
 'FL' 'GV' 'CA' 'NH' 'MD' 'CT' 'MO' 'RI' 'MS' 'MA' 'MI' 'TN' 'WV' 'AL'
 'OR' 'KS' 'VA' 'KY' 'AZ' 'WA' 'NM' 'CO' 'SC' 'WI' 'ME' 'DE' 'HI' 'IN'
 'WY' 'MT' 'NE' 'VT' 'GA' 'LA' 'SD' 'ON' 'IA' 'ID' 'ND' 'SK' 'UT' 'AK'
 'QB' 'AB' 'BC' 'MX' 'PR' 'NS' 'MB' 'FO']
VehicleColor: ['BLACK' 'SILVE' 'GREY' 'WHITE' 'RED' 'OTHER' 'BLUE' 'GY' 'BLK' 'BK'
 'PURPL' 'TAN' 'GREEN' 'YELLO' 'ORANG' 'BL' 'SILV' 'GRAY' 'BROWN' nan
 'GRY' 'WH' 'SIL' 'GOLD' 'WT' 'WHT' 'GR' 'RD' 'YW' 'BR' 'LTG' 'WH/' 'OR'
 'WHB' 'TN' 'BRN' 'MR' 'DK/' 'BLW' 'GL' 'PR' 'BU' 'DKB' 'W' 'GRT' 'ORG'
 'RD/' 'LT/' 'NO' 'LTT' 'GRN' 'BN' 'TB' 'BRO' 'B' 'RDW' 'SL' 'BURG' 'BLU'
 'NOC' 'BK/' 'DKG' 'WHG' 'PINK' 'G' 'LAVEN' 'BL/' 'YEL' 'OG' 'GRW' 'WHI'
 'WHTE' 'BUR' 'GY/' 'DKR' 'RDT' 'GN' 'BUN' 'SV' 'BKG' 'YELL' 'WHIT' 'GR/'
 'LTTN' 'SLV' 'BRWN' 'GYB' 'WHTIE' 'WI' 'BUS' 'LTB' 'TN/' 'GD' 'MAROO'
 'BW' 'BLG' 'ORA' 'GRA' 'DKP' 'NAVY' 'GREG' 'GRB' 'BRW' 'BBRN' 'R' 'GRRY'
 'BLA' 'BG' 'MAR' 'BURGA' 'BRWON' 'YLW' 'ORNG' 'HREY' 'DERD' 'YL' 'PLE'
 'BWN' 'BI']</code></pre>

        The first two registration types account are the most common:
        <pre><code class=blockcode>count = len(newDF)
pasCount = len(newDF[newDF['Registration'] == 'PAS'])
comCount = len(newDF[newDF['Registration'] == 'COM'])
print(f'{count} different vehicles, {100*(pasCount+comCount)/count} percent are passenger or commercial plates.')</code></pre>
        <p>And for the Precinct District 19 dataset that contains almost a half million tickets:
  <pre><code class=blockcode>159928 different vehicles, 93.95477965084288 percent are passenger or commercial plates.</code></pre>

       And similarly, 15 of the entries for vehicle color account for most of the entries:
       <pre><code class=blockcode>print(newDF['VehicleColor'].unique())
print(f"The top 15 values account for {100*newDF['VehicleColor'].value_counts()[:15].sum()/len(newDF)} percent.")
print(f"Those values are: {newDF['VehicleColor'].value_counts()[:15]}.")</code></pre>

 <pre><code class=blockcode>The top 15 values account for 95.37291781301586 percent.
Those values are:
WH       27814
GY       24704
WHITE    20817
BK       20778
BLACK    14486
GREY      9629
BL        9249
SILVE     5704
BLUE      5300
RD        4395
RED       3303
OTHER     2678
GR        1674
BROWN     1059
TN         938</code></pre>

      <p>To clean the data, write two functions that can be applied to the DataFrame:
      <ul>
        <li> <code class=inline>cleanReg(reg):</code> If <code class=inline>reg</code> is coded as passenger <code class=inline>'PAS'</code> or commercial
        <code class=inline>'COM'</code>, return those values.  Otherwise, return <code class=inline>'OTHER'</code>.
        <li> <code class=inline>cleanColor(c):</code> Return the following for the values of <code class=inline>c</code>:
        <ul>
          <li> <code class=inline>'GRAY'</code>: for <code class=inline>GY</code>,
            <code class=inline>GRAY</code>,
            <code class=inline>GREY</code>,<code class=inline>SILVE</code>,
              <code class=inline>SIL</code>, <code class=inline>SL</code>,
          <li> <code class=inline>'WHITE'</code>: for <code class=inline>WH</code>,
            <code class=inline>WHITE</code>,
          <li> <code class=inline>'BLACK'</code>: for <code class=inline>BK</code>,
            <code class=inline>BLACK</code>, <code class=inline>BL</code>,
          <li> <code class=inline>'BLUE'</code>: for <code class=inline>BLUE</code>,
          <li> <code class=inline>'RED'</code>: for <code class=inline>RED</code>,
              <code class=inline>RD</code>,
          <li> <code class=inline>'GREEN'</code>: for <code class=inline>GR</code>,
              <code class=inline>GREEN</code>,
          <li> <code class=inline>'BROWN'</code>: for <code class=inline>BROWN</code>,
              <code class=inline>TAN</code>,
          <li> Otherwise, return <code class=inline>'OTHER'</code>.
        </ul>
      </ul>
      <p>After applying these functions, the resulting DataFrame can then be used to build a classifer on how likely a particular car is to be one that has more than a ticket a day (see <a href="#p42">Program 42</a>).


      <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>

  <p></p><p><a name="p39"><b>Program 39: Binary Digit Classification.</b></b> &emsp; <i>Due noon, Friday, 12 November.</i>
    <br>(Learning Objective: to build classifers using sklearn.)</p>
    <p>
    This program uses the canonical <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset of hand-written digits</a> discussed in Lecture #19 and available in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">sklearn digits dataset</a>:
          <br>
    <a href="https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py">
      <img src="sklearn_digits.png" height=400>
    </a>

    <br>
    The dataset has 1797 scans of hand-written digits.
    Each entry has the digit represented (<code class=inline>target</code>) as well as the 64 values representing the gray scale for the 8 x 8 image.  The first 5 entries are:
    <br>
    <img src = "mnist_first5.png", height = 100>
    <br>
    The gray scales for the first 5 entries, flattened to one dimensional array:
    <pre><code class="datablock">[[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3. 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]
 [ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.  3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16. 16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  0. 11. 16. 10.  0.  0.]
 [ 0.  0.  0.  4. 15. 12.  0.  0.  0.  0.  3. 16. 15. 14.  0.  0.  0.  0.  8. 13.  8. 16.  0.  0.  0.  0.  1.  6. 15. 11.  0.  0.  0.  1.  8. 13. 15.  1.  0.  0.  0.  9. 16. 16.  5.  0.  0.  0.  0.  3. 13. 16. 16. 11.  5.  0.  0.  0.  0.  3. 11. 16.  9.  0.]
 [ 0.  0.  7. 15. 13.  1.  0.  0.  0.  8. 13.  6. 15.  4.  0.  0.  0.  2.  1. 13. 13.  0.  0.  0.  0.  0.  2. 15. 11.  1.  0.  0.  0.  0.  0.  1. 12. 12.  1.  0.  0.  0.  0.  0.  1. 10.  8.  0.  0.  0.  8.  4.  5. 14.  9.  0.  0.  0.  7. 13. 13.  9.  0.  0.]
 [ 0.  0.  0.  1. 11.  0.  0.  0.  0.  0.  0.  7.  8.  0.  0.  0.  0.  0.  1. 13.  6.  2.  2.  0.  0.  0.  7. 15.  0.  9.  8.  0.  0.  5. 16. 10.  0. 16.  6.  0.  0.  4. 15. 16. 13. 16.  1.  0.  0.  0.  0.  3. 15. 10.  0.  0.  0.  0.  0.  2. 16.  4.  0.  0.]]</code></pre>

    <br>To start, we will focus on entries that represent 0's and 1's.  The first 10 from the dataset are displayed below:
    <br>
      <img src = "mnist_binary10.png", height = 175>

    <p>
    Write a function that builds a logistic regression model that classifies binary digits:
        <ul>
          <li> <code class = "inline">def binary_digit_clf(data, target, test_size = 0.25, random_state = 21):</code>:
            This function has four inputs:
            <ul>
                <li> <code class = "inline">data</code>: a numpy array that
                      includes rows of equal size flattend arrays,
                <li> <code class = "inline">target</code> a numpy array that takes values 0 or 1 corresponding to the rows of <code class = "inline">data</code>.
                <li> <code class = "inline">test_size</code>: the size of the test set created when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">0.25</code>.
                <li> <code class = "inline">random_state</code>: the random seed used when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">21</code>.
            </ul>

            The function confusion matrix that results.
          </ul>

          <p>
          For example, let's flatten the entries and restrict the dataset to just binary digits, as we did in lecture:
          <pre><code class="blockcode">#Import datasets, classifiers and performance metrics:
from sklearn import datasets, svm, metrics
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
#Using the digits data set from sklearn:
from sklearn import datasets
digits = datasets.load_digits()
print(digits.target)
print(type(digits.target), type(digits.data))
#flatten the images
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))
print(data[0:5])
print(f'The targets for the first 5 entries: {digits.target[:5]}')
#Make a DataFrame with just the binary digits:
binaryDigits = [(d,t) for (d,t) in zip(data,digits.target) if t <= 1]
bd,bt = zip(*binaryDigits)
print(f'The targets for the first 5 binary entries: {bt[:5]}')</code></pre>
          which will print:
<pre><code class="datablock">
  [0 1 2 ... 8 9 8]
<class 'numpy.ndarray'> <class 'numpy.ndarray'>
[[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.
  15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.
   0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.
   0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]
 [ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.
   3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16.
  16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.
   0.  0.  0.  0.  0. 11. 16. 10.  0.  0.]
 [ 0.  0.  0.  4. 15. 12.  0.  0.  0.  0.  3. 16. 15. 14.  0.  0.  0.  0.
   8. 13.  8. 16.  0.  0.  0.  0.  1.  6. 15. 11.  0.  0.  0.  1.  8. 13.
  15.  1.  0.  0.  0.  9. 16. 16.  5.  0.  0.  0.  0.  3. 13. 16. 16. 11.
   5.  0.  0.  0.  0.  3. 11. 16.  9.  0.]
 [ 0.  0.  7. 15. 13.  1.  0.  0.  0.  8. 13.  6. 15.  4.  0.  0.  0.  2.
   1. 13. 13.  0.  0.  0.  0.  0.  2. 15. 11.  1.  0.  0.  0.  0.  0.  1.
  12. 12.  1.  0.  0.  0.  0.  0.  1. 10.  8.  0.  0.  0.  8.  4.  5. 14.
   9.  0.  0.  0.  7. 13. 13.  9.  0.  0.]
 [ 0.  0.  0.  1. 11.  0.  0.  0.  0.  0.  0.  7.  8.  0.  0.  0.  0.  0.
   1. 13.  6.  2.  2.  0.  0.  0.  7. 15.  0.  9.  8.  0.  0.  5. 16. 10.
   0. 16.  6.  0.  0.  4. 15. 16. 13. 16.  1.  0.  0.  0.  0.  3. 15. 10.
   0.  0.  0.  0.  0.  2. 16.  4.  0.  0.]]
The targets for the first 5 entries: [0 1 2 3 4]
The targets for the first 5 binary entries: (0, 1, 0, 1, 0)
</code></pre>

          <p>We can then use the restricted data and targets datasets as input to our function, assuming your function <code class="inline">binary_digit_clf()</code> was in the <code class="inline">p39.py</code>:
  <pre><code class="blockcode">confuse_mx = p39.binary_digit_clf(bd,bt,test_size=0.95)
print(f'Confusion matrix:\n{confuse_mx}')
disp = metrics.ConfusionMatrixDisplay(confusion_matrix=confuse_mx)
#Use a different color map since the default is garish:
disp.plot(cmap = "Purples")
plt.title("Logistic Regression Classifier for Binary Digits")
plt.show()</code></pre>
            which will print:
  <pre><code class="datablock">Confusion matrix:
[[172   0]
 [  4 166]]</code></pre>
          and display:
          <br>
          <img src="logReg_clf_confuse.png" height = 300>

          <p>Another example with the same data, but different size for the data reserved for testing:
            <pre><code class="blockcode">confuse_mx = p39.binary_digit_clf(bd,bt)
print(f'Confusion matrix:\n{confuse_mx}')</code></pre>
            would print:
            <pre><code class="datablock">Confusion matrix:
[[43  0]
 [ 0 47]]</code></pre>


      <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>



<p></p><p><a name="p40"><b>Program 40: Enrollments by Courses.</b></b> &emsp; <i>Due noon, Monday, 15 November.</i>
      <br>(Learning Objective: to reinforce categorical encoding and aggregation techniques.)</p>

      Building on <a href="#p24">Program 24</a> and <a href="#p28">Program 28</a>,
      write a function, <code class="inline">byCourses()</code>, that takes a DataFrame that contains students' names, number of credits completed, and current courses (a string with the course names separated by <code class="inline">' '</code>), and returns the resulting column:
      <ul>
        <li> The column indices are the names of the computer science courses only, and
        <li> The one column is <code class="inline">NumEnrolled</code>, the total number of students enrolled.
            </ul>
        <li> No other rows or columns should be included in the DataFrame.

      </ul>

      <p> For example, assuming your function <code class="inline">byCourses()</code> was in the <code class="inline">p40.py</code>:
          <pre><code class="blockcode">classDF = pd.DataFrame({'Name': ["Ana","Bao","Cara","Dara","Ella","Fatima"],\
                          '# Credits': [45,50,80,115,30,90],\
                          'Current Courses': ["csci160 csci235 math160 jpn201",\
                                              "csci160 csci235 cla101 germn241",\
                                              "csci265 csci335 csci39542 germn241",\
                                              "csci49362 csci499",\
                                              "csci150 csci235 math160",\
                                              "csci335 csci39542 cla101 dan102"]})
print(f'Starting df:\n {classDF}\n')
print(f'CS courses:\n {p40.byCourses(classDF)}')</code></pre>
          Would give output:
      <pre><code class="datablock">Starting df:
      Name  # Credits                     Current Courses
0     Ana         45      csci160 csci235 math160 jpn201
1     Bao         50     csci160 csci235 cla101 germn241
2    Cara         80  csci265 csci335 csci39542 germn241
3    Dara        115                   csci49362 csci499
4    Ella         30             csci150 csci235 math160
5  Fatima         90     csci335 csci39542 cla101 dan102

CS courses:
csci150      1
csci160      2
csci235      3
csci265      1
csci335      2
csci39542    2
csci49362    1
csci499      1</code></pre>


  <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>

      <p><i>Hints:
        <ul>
          <li> Use categorical encoding (i.e. one-hot encoding) on the courses each student is taking.
          <li> A recent addition to Pandas allows multiple values to encoded per entry:  <a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.str.get_dummies.html">Series.str.get_dummies(sep = '|')</a>.  Note the default for the delimiter is not <code class = inline>' '</code> as it is for many Python string methods, instead it is
          <code class = inline>'|'</code>.
          <li> Pandas includes useful functions that manipulate matrices such as <code class=inline>df.transpose</code> that will reflect a DataFrame by writing its rows as columns and columns as rows.  See <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html">panda documentation</a> for details.
        </ul>
      </i>
    </p>


  <p></p><p><a name="p41"><b>Program 41: Classifier Misses.</b></b> &emsp; <i>Due noon, Tuesday, 16 November.</i>
    <br>(Learning Objective: to refresh matrix manipulation skills and strengthen understanding of multiway classification results.)</p>
    <p>
    </p>

    In Lectures #20 and #21 and <a href="http://www.textbook.ds100.org/ch/24/classification_intro.html">DS 100: Chapter 24</a>, we used multiway classification on the canonical iris data set.  For this program, write a function that will take a confusion matrix from such an analysis and return the most common misclassified item in the matrix.

    <ul>
        <li> <code class = "inline">def clf_misses(mx):</code>:
              This function has one input:
              <ul>
                  <li> <code class = "inline">mx</code>: a matrix that has been generated by <code class = "inline">sklearn.metrics.confusion_matrix</code>,
              </ul>

              The function returns the label of the class that is most commonly misidentified.  That is, the row which has the number of entries not on the diagonal of all rows.  If there are multiple classes that share the greatest fraction of mislabels, return the first one, as ordered in the maxtrix.
            </ul>

      <p> In <a href="http://www.textbook.ds100.org/ch/24/classification_intro.html">DS 100: Chapter 24</a>, the confusion matrix computed for the iris dataset was:
      <br>
      <img src="ds100_iris_confuse_mx.png" height = 300>

      <br>The first row has no entries not in its diagonal entry and a total of 19 members of the class, so has value <code class=inline>0</code>.  The second column has <code class=inline>2</code> elements mislabeled.  The third column has <code class=inline>0</code> mislabeled.  So, your function would return the second class, the one labeled <code class=inline>iris-versicolor</code>.

      <p>For example, examining a confusion matrix for the MNIST digits dataset and assuming <code class=inline>clf_misses</code> is in <code class=inline>p41</code> and the appropriate libraries are loaded:
      <pre><code class="blockcode">digits = datasets.load_digits()
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))
X_train, X_test, y_train, y_test = train_test_split(data, digits.target,random_state=42, test_size=.75)
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train,y_train)
y_predict = clf.predict(X_test)
confuse_mx = metrics.confusion_matrix(y_test,y_predict)
disp = metrics.ConfusionMatrixDisplay(confusion_matrix=confuse_mx)
disp.plot(cmap = "Purples")
plt.title("Logistic Regression Classifier for Digits")
plt.show()
print(f'The most misclassified class is {p41.clf_misses(confuse_mx)}.')</code></pre>
<p>would display:
  <br><img src="mnist_confuse_mx_75_test.png" height = 300>
  <br>
      and print:
      <pre><code class="datablock">The most misclassified class is 3.</code></pre>


    Hints:
      <ul>
        <li> Break up this question into parts:  first, print out the diagonal entries of the matrix.  Next, how would sum a row of a matrix?  Once you have both of these pieces, the desired row is the one with the highest difference between the row sum and its diagonal entry.
        <li> A useful function for finding at which index a maximal entry occurs is <code class=inline>argmax</code>.
      </ul>
    </i>

    <p></p><p><a name="p42"><b>Program 42: Ticket Predictor.</b></b> &emsp; <i>Due noon, Thursday, 18 November.</i>
      <br> (Learning Objective: to use the tools provided by sklearn to create a support vector machine.)
      </p>
      <p>
      For this program, we will train a linear regression classifier to predict the number of tickets a vehicle is likely to receive. The data is first cleaned using the functions you wrote in <a href="#p38">Program 38</a> to use standardized spellings of color names as well as one of three vehicle classes.  The first function adds indicators for the specified categorical featuers.  The second function trains a linear regression classifier on the data and returns the accuracy (score) of your classifier on the test data, as well as the classifier.
      <ul>
            <li> <code class = "inline">addIndicators(df,cols=['Registration', 'Color', 'State']):</code>:
                This function has two inputs and returns a DataFrame:
                <ul>
                    <li> <code class = "inline">df</code>: a DataFrame that
                            including the columns specified in <code class = "inline">cols</code>.
                    <li> <code class = "inline">col</code>: a list of names of columns in the DataFrameIt has a default value of ['Registration', 'Color', 'State'].
                </ul>
                Returns the DataFrame with an additional indicator columns generated by <code class=inline>get_dummies</code> for specified columns.  The <code class = "inline">drop_first</code> flag is set to <code class = "inline">True</code> to drop extraneous columns.

            <li> <code class = "inline">build_clf(df, xes, y_col = "NumTickets", test_size = 0.25, random_state=17)</code>:
                This function has five inputs and returns a linear regression classifier:
                <ul>
                    <li> <code class = "inline">df</code>: a DataFrame that includes the specified columns.
                    <li> <code class = "inline">xes</code>: the columns that are the independent variables for the model.
                    <li> <code class = "inline">y_col</code>: a column name containing the dependent variable (what's being predicted) in the model.  It has a default value is <code class = "inline">NumTickets</code>.
                    <li> <code class = "inline">test_size</code>: the size of the test set created when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">0.25</code>.
                    <li> <code class = "inline">random_state</code>: the random seed used when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">17</code>.
                  </ul>

                  The function returns the (score) of the model computed on the test data and the classifier.
                </ul>

                <p>
                For example, let's clean the dataset using the functions from <a href="#p39">Program 38</a>, as we did in lecture on <a href="Parking_Q1_2021_Lexington.csv">Parking_Q1_2021_Lexington.csv</a>:
                <pre><code class="blockcode">import numpy as np
import pandas as pd
import p38
import p42
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

df = pd.read_csv('Parking_Q1_2021_Lexington.csv')
#Focus on features about vehicles:
df = df[['Plate ID','Plate Type','Registration State','Issue Date','Vehicle Color']]
#Drop rows that are missing info:
df = df.dropna()
print(f'Your file contains {len(df)} parking violations.')
#Clean the data, using the functions written for P38:
df['Plate Type'] = df['Plate Type'].apply(p38.cleanReg)
df['Vehicle Color'] = df['Vehicle Color'].apply(p38.cleanColor)
#Count tickets for each vehicle:
newDF =  df.groupby('Plate ID').agg(NumTickets =
    pd.NamedAgg(column = 'Plate ID', aggfunc = 'count'),
    Registration = pd.NamedAgg(column = 'Plate Type', aggfunc = 'first'),
    State = pd.NamedAgg(column = 'Registration State', aggfunc = 'first'),
    Color = pd.NamedAgg(column = 'Vehicle Color', aggfunc = 'first')
)
print(newDF)
</code></pre>
                which will print:
      <pre><code class="datablock">Your file contains 20589 parking violations.
          NumTickets Registration State  Color
Plate ID
00356R2            1          PAS    TX  WHITE
004LSM             1          PAS    TN  OTHER
00574R7            1          PAS    TX  WHITE
0064NQD            1          PAS    DP  BLACK
0107NQD            1          PAS    DP   GRAY
...              ...          ...   ...    ...
ZRB1864            1          PAS    PA  WHITE
ZSA6859            1          PAS    PA   GRAY
ZSE1922            1          PAS    PA  WHITE
ZWF62E             1          PAS    NJ  OTHER
ZWZ35J             1          PAS    NJ  OTHER
      </code></pre>

        <p>We can then use the cleaned data, assuming your functions are in the <code class="inline">p42.py</code>:
        <pre><code class="blockcode">newDF = p42.addIndicators(newDF)
print(newDF)</code></pre>
                  will add the indicator variables:
        <pre><code class="datablock">          NumTickets  Registration_OTHER  ...  State_WI  State_WV
Plate ID                                  ...
00356R2            1                   0  ...         0         0
004LSM             1                   0  ...         0         0
00574R7            1                   0  ...         0         0
0064NQD            1                   0  ...         0         0
0107NQD            1                   0  ...         0         0
...              ...                 ...  ...       ...       ...
ZRB1864            1                   0  ...         0         0
ZSA6859            1                   0  ...         0         0
ZSE1922            1                   0  ...         0         0
ZWF62E             1                   0  ...         0         0
ZWZ35J             1                   0  ...         0         0</code></pre>

        <p>We can then use this in the second function to fit a classifier that will predict tickets based on characteristics of the vehicle:
        <pre><code class="blockcode">xes = ['State_NY','Registration_OTHER', 'Registration_PAS', 'Color_GRAY', 'Color_OTHER', 'Color_WHITE']
y_col = 'NumTickets'
sc,clf = p42.build_clf(newDF, xes)
print(f'Score is {sc}.')
predicted = clf.predict([[1,0,0,0,0,1]])[0]
print(f'NY state, white commercial vehicle (encoded as: [1,0,0,0,0,1])\n\twill get {predicted:.2f} tickets.')
predicted = clf.predict([[1,0,1,1,0,0]])[0]
print(f'NY state, gray passenger vehicle (encoded as: [1,0,1,1,0,0])\n\twill get {predicted:.2f} tickets.')
</code></pre>
                  which will print:
        <pre><code class="datablock">Score is 0.04310334739677757.
NY state, white commercial vehicle (encoded as: [1,0,0,0,0,1])
	will get 2.48 tickets.
NY state, gray passenger vehicle (encoded as: [1,0,1,1,0,0])
	will get 1.16 tickets.</code></pre>

  <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>


<p></p><p><a name="p43"><b>Program 43: Moving.</b></b> &emsp; <i>Due noon, Friday, 19 November.</i>
    <br>(Learning Objective: to reinforce linear algebra concepts from prerequisite course and build corresponding facility in Python.)</p>
    <p>

      In Lecture #21 and <a href="https://setosa.io/ev/eigenvectors-and-eigenvalues/">Explained Visually</a>, we reviewed matrices, eigenvectors and eigenvalues.

      <p>
      <iframe src="https://setosa.io/ev/eigenvectors-and-eigenvalues#steady-states" style="width: 60%; height: 500px" name="internal"></iframe>


      <p>For this program, write a function that extends the moving between information about any number of states, starting populations, and the number of years, and returns an array of the ending population of each state.

      <ul>
            <li> <code class = "inline">moving(transition_mx, starting_pop, num_years = 1)</code>:
                This function has three inputs and returns an array:
                <ul>
                    <li> <code class = "inline">transition_mx</code>: an square
                      array of values between 0 and 1.  Each column sums to 1.
                    <li> <code class = "inline">starting_pop</code>: an 1D array of positive numeric values
                      that has the same length as <code class = "inline">transition_mx</code>.
                    <li> <code class = "inline">num_years</code>: a column name containing the dependent variable (what's being predicted) in the model.  It has a default value is <code class = "inline">1</code>.
                </ul>
                The function returns an array of the population of each state after <code class = "inline">num_years</code>.
                <li> <code class = "inline">steadyState(transition_mx, starting_pop)</code>:
                    This function has two inputs and returns an array:
                    <ul>
                        <li> <code class = "inline">transition_mx</code>: an square
                          array of values between 0 and 1.  Each row sums to 1.
                        <li> <code class = "inline">starting_pop</code>: an 1D array of positive numeric values
                          that has the same length as <code class = "inline">transition_mx</code>.
                        <li> <code class = "inline">num_years</code>: a column name containing the dependent variable (what's being predicted) in the model.  It has a default value is <code class = "inline">1</code>.
                    </ul>
                    The function returns an array of the population of each state at the steady state.  That is, it returns the eigenvector corresponding to the largest eigenvalue, scaled so that it's entries sum to 1, and then multiplied by the sum of the starting populations.
                </ul>

      <p>
      For example, if you have that people move from the following three states with the probabilities of moving each year:
      <ul>
        <li> New Yorkers:
            <ul>
                <li> 70% of New Yorkers stay in New York
                <li> 25% of New Yorkers move to California
                <li> 5% of New Yorkers move to Texas
            </ul>
        <li> Californians:
            <ul>
                <li>7% of Californians move to New York
                <li>90% of Californians stay in California
                <li>3% of Californians move to Texas
            </ul>
        <li> Texans:
            <ul>
              <li>15% of Texans move to New York
              <li>25% of Texans move to California
              <li>60% of Texans stay in Texas
            </ul>
      </ul>
      If the initial populations are New York: 20 million, California: 40 million,  and Texas: 25 million.  Then, the transition matrix is:
<pre><code class="blockcode">t_mx = np.array([[.7, .07, .1],
         [.25,.9,.15],
         [.05,.03,.75]])
</code></pre>
      and the starting populations (in millions) are:
      <code class="blockcode">pop0 = np.array([20, 40, 25])</code>

      we can compute the population after 1 years, by taking the intial populations, computing what fraction move to each of the other states:

      <pre><code class="blockcode">pop1 = t_mx @ pop0
print(f'Population of each state after 1 year: {pop1}.')</code></pre>

      which is
      <pre><code class="blockcode">Population of each state after 1 year: [19.3  44.75 20.95].</code></pre>

      <p> Similarly, the population after 2 years, can be found by multiplying the population after 1 year by the transition matrix.  More generally, the population after k+1 years can be found by multiplying the populations at year k by the transition matrix.

      The steady state population can be found by first finding the eigenvector corresponding to the maximal eigenvalue of 1, scaling it so its entries sum to 1 (i.e. divide through by its sum) to get percentages.  And then, multiplying the percentages by the total population.

        <p>For example, continuing from above, and assuming your functions are in <code class=inline>p43</code> and the appropriate libraries are loaded:
        <pre><code class="blockcode">pop1 = p43.moving(t_mx, pop0)
print(f'Population of each state after 1 year: {pop1}')
pop100 = p43.moving(t_mx, pop0, num_years=100)
print(f'Population of each state after 100 years: {pop1}')
pop_steady = p43.steadyState(t_mx, pop0)
print(f'Steady state population: {pop_steady}')</code></pre>

        and print:
        <pre><code class="datablock">Population of each state after 1 year: [19.3  44.75 20.95]
Population of each state after 100 years: [16.91747573 57.76699029 10.31553398]
Steady state population: [16.91747573 57.76699029 10.31553398]</code></pre>

  <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>


      <i>
      Hints:
        <ul>
            <li> The module numpy has many useful functions for computing determinants and eigenvalues in its linear algebra package. Note that the * for matrices is element-wise (not regular matrix multiplication). To multiply two matrices, a and b together use a @ b.
            <li> A useful function to raise a matrix to a power, is
              <code class=inline>numpy.linalg.matrix_power</code> described in
                the <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_power.html">numpy API reference</a>.
            <li> The <code class=inline>numpy.linalg.eig</code> function returns an array of eigenvalues and the associated eigenvectors as columns of an array.  For the example above,
              <pre><code class="blockcode">import numpy.linalg as LA
w,v = LA.eig(t_mx)
print(f'The eigenvalues are: {w} and eigenvectors are:\n{v}.')</code></pre>

              would print:
              <pre><code class="datablock">The eigenvalues are: [ 1.  -0.2  0.1] and eigenvectors are:
[[-6.67423812e-01 -7.07106781e-01  2.67261242e-01]
 [-5.72077554e-01  6.87552368e-17 -8.01783726e-01]
 [-4.76731295e-01  7.07106781e-01  5.34522484e-01]].</code></pre>
              The eigenvector for eigenvalue = 1 is the first column (not the first row-- the rows are grouped together since we enter/print 2D arrays by rows).  See Lecture #21 notes for scaling the vector to compute the steady state population.
        </ul>
      </i>



<p></p><p><a name="p44"><b>Program 44: Model Comparison.</b></b> &emsp; <i>Due noon, Monday, 22 November.</i>
<br>(Learning Objective: to build facility with fitting multiclass models.)</p>


    In Lectures #21 and #22 and <a href="http://www.textbook.ds100.org/ch/24/classification_multiclass.html">Chapter 24</a>, we build classifiers for the iris dataset.  Write a function that fits a Logistic Regression model and a Support Vector Machine to the same training data and returns the score of each on the same testing data.

    <ul>
    <li> <code class = "inline">compare_clf(xes, y, test_size = 0.20, random_state=66,max_iter=500):</code>
        This function has five inputs and returns a linear regression classifier:
        <ul>
            <li> <code class = "inline">xes</code>: a array of the depedendent variables for the model.
            <li> <code class = "inline">y</code>: the independent variable for the model.
            <li> <code class = "inline">test_size</code>: the size of the test set created when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">0.20</code>.
            <li> <code class = "inline">random_state</code>: the random seed used when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">66</code>.
            <li> <code class = "inline">max_iter</code>: the maximum number of iterations flag used for <code class = "inline">LogisticRegression</code>.The default value is <code class = "inline">500</code>.
          </ul>

          The function returns the score of the model score computed on the test data by each classifier.
      </ul>

      <p>For example, assuming your functions are in <code class=inline>p44</code> and the appropriate libraries are loaded:
      <pre><code class="blockcode">iris = datasets.load_iris()
l_40,s_40 = p44.compare_clf(iris.data,iris.target,test_size=.4)
print(f'With a 40% test set, LogReg classifer has score {l_40}.\nSVM classifier had score {s_40}.')

xes = list(range(5,100,5))
runs = [p44.compare_clf(iris.data,iris.target,test_size=x/100) for x in xes]
lr_runs, svm_runs = zip(*runs)
plt.plot(xes, lr_runs, label="LogReg")
plt.plot(xes, svm_runs, label= "SVM")
plt.xlabel('Test Size (Percent)')
plt.ylabel('Score')
plt.title('Iris Dataset:  Test Size vs Score')
plt.legend()
plt.show()</code></pre>

      would print:
      <pre><code class="datablock">With a 40% test set, LogReg classifer has score 0.9833333333333333.
SVM classifier had score 0.9833333333333333.</code></pre>

      and display:
      <p>
      <img src = "iris_logR_svm.png" height=300>

<p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>


<p></p><p><a name="p45"><b>Program 45: Component Retention.</b></b> &emsp; <i>Due noon, Tuesday, 23 November.</i>
  <br>(Learning Objective: to employ programming skills to evaluate the number of principal components to use in dimensionality reduction.)</p>

  <p>
  In Lecture 22 and (also in <a href="http://www.textbook.ds100.org/ch/25/pca_in_practice.html">Chapter 25</a>), we used scree plots to provide a visualization of the captured variance.  This assignment asks you to implement two other popular ways of determining the number of dimensions to retain.

  <ul>
    <li> <code class="inline">captures85(arr):</code> Takes an array <code class="inline">arr</code> (in decreasing order), computes
            the captured variance (<code class="inline">cv = (arr**2)/sum(arr**2)</code>) and returns the number of elements needed to capture more than 85% of the variance.
        <li> <code class="inline">averageEigenvalue(arr):</code> Takes an array
          <code class="inline">arr</code> (in decreasing order), computes
            the average (<code class="inline">avg = sum(arr)/len(arr)</code>) and returns the number of elements greater than <code class="inline">avg</code>.
      </ul>
      </p>

      Using the example from the textbook, if <code class="inline">arr</code> is
      <pre><code class="datablock">a = np.array([585.57, 261.06, 166.31,  57.14,  48.16,  39.79,  31.71,  28.91,
      24.23,  22.23,  20.51,  18.96,  17.01,  15.73,   7.72,   4.3 ,
      1.95,   0.04])</code></pre>
      Then <code class="inline">cv</code> would be:
      <pre><code class="datablock">array([0.76, 0.15, 0.06, 0.01, 0.01, 0.  , 0.  , 0.  ,   0.  , 0.  , 0.  ,
      0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])</code></pre>
      and the function, <code class="inline">capture85(a)</code>, would return 2 since the first coordinate captures 76% of the variance which is less than 85%, the first 2 coordinates capture 76 + 15 = 91% of the variance.

      <p>
      For the second function, again using the example from the textbook, for the array <code class="inline">a</code>, the <code class="inline">avg</code> would be 75.07,
      and the function, <code class="inline">averageEigenvalue(a)</code> would return 3 since the first three coordinates are larger than the average.

      <p>
      Note:  you should submit a file with only the standard comments at the top, and these two functions.  The grading scripts will then import the file for testing.


  <p></p><p><a name="p46"><b>Program 46: Digits Components.</b></b> &emsp; <i>Due noon, Monday, 29 November.</i>

    <br>(Learning Objective: to strengthen understanding of intrinistic dimensions of data sets via exploration of the classic digits dataset.)</p>
    <p>
    In Lecture #21, we introduced Principal Components Analysis and the number of components needed to capture the intrinistic dimension of the data set.  For this program, write a function that allows the user to explore how many dimensions are needed to see the underlying structure of images from the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">sklearn digits dataset</a> (inspired by <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html">Python Data Science Handbook: Section 5.9</a> (PCA)).

    <p> Write a function that approximates an image by summing up a fixed number of its components:

      <ul>
      <li> <code class = "inline">approxDigits(numComponents, coefficients, mean, components):</code>
          This function has four inputs and returns an array containing the approximation:
          <ul>
              <li> <code class = "inline">numComponents</code>: the number of componets used in the approximation.  Expecting a value between 0 and 64.
              <li> <code class = "inline">coefficients</code>: an array of coefficients, outputted from <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">PCA()</a>.
              <li> <code class = "inline">mean</code>: an array representing the mean of the dataset.
              <li> <code class = "inline">components</code>:  an array of the components computed by <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">PCA()</a> analysis.
            </ul>

            The function returns the approximation image (flattened array) of the mean and sum of the first <code class = "inline">numComponents</code>
            terms (i.e. <code class = "inline">coefficients[i] * components[i]</code>).
        </ul>





    As discussed in <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html">Python Data Science Handbook: Section 5.9</a>, we can view the images as sums of the components.  For our
    flattened images, we have 1D arrays of length 64.  Here's the first one from the dataset:
    <pre><code class="datablock">[[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3. 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]</code></pre>
    <p>
    If we let <code class=inline>x1 = [1 0 ... 0]</code>,
    <code class=inline>x2 = [0 1 0 ... 0]</code>, ...,
    <code class=inline>x64 = [0 ... 0 1]</code> (vectors corresponding to the axis), then we can write our images, <code class=inline>im = [i1 i2 ... i64]</code>, as:
    <pre><code class="datablock">im = x1*i1 + x2*i2 + ... + x64*i64
        x1*0 + x2*0 + x3*5 + ... + x64*0</code></pre>
    plugging in the values of <code class=inline>im</code> into the equation.

    <p>
    In a similar fashion, we can represent the image in terms of the axis,<code class=inline>c1, c2, ... c64</code>,  that the PCA analysis returns:
    <pre><code class="datablock">im = mean + c1*i1 + c2*i2 + ... + c64*i64</code></pre>
    since the axis of PCA are chosen so that the first one captures the most variance, the second the next most, etc.  The later axis capture very little variance and likely add litte to the image.  (For technical reasons, we include the mean.  The reason is similar to when we "center" multidimensional data at 0).

    This can be very useful for reducing the dimension of the data set, for example, here is the first image from above on the left:
    <p>
    <img src = "digit_adding_comps.png" width=90%>
    <br>
    The next image is the overall mean, and each subsequent image is adding another component to the previous.  For this particular scan, the mean plus its first component is enough to see that it's a 0.


    <p> For example, assuming the function  is in <code class=inline>p46</code> and the appropriate libraries are loaded:
      <pre><code class="blockcode">from sklearn.decomposition import PCA
pca = PCA()
Xproj = pca.fit_transform(digits.data)
showDigit(pca.mean_, f"Mean for digits")
plt.imshow(pca.mean_.reshape(8,8),cmap='binary', interpolation='nearest',clim=(0, 16))
plt.title("Mean for digits")
plt.show()
approxAnswer = p46.approxDigits(8,Xproj[1068], pca.mean_, pca.components_)
plt.imshow(approxAnswer.reshape(8,8),cmap='binary', interpolation='nearest',clim=(0, 16))
plt.title("mean + 8 components for digits[1068]")
plt.show()</code></pre>

      would show the mean and summed with the first 8 components for
      <code class=inline>digits[1068]</code>:

      <p>
      <img src="digits_mean.png" height=200>
      <img src="digits_1068_8.png" height=200>
  </p>

        <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>



<p></p><p><a name="p47"><b>Program 47: Voting MDS.</b> &emsp; <i>Due noon, Tuesday, 30 November.</i>
  <br>(Learning Objective: to build intuition and strengthen competency with dimensionality reduction methods.)</p>

  <p>
  In Lecture #22 and <a href="http://www.textbook.ds100.org/ch/26/pca_in_practice.html">DS 100, Chapter 26.3</a>, we explored Principal Components Analysis (PCA) for a <a href="https://raw.githubusercontent.com/DS-100/textbook/master/content/ch/26/vote_pivot.csv">US Representatives voting dataset</a>.  For this program, we will examine the dataset with  Multidimensional Scaling (MDS)
  for different distance matrices.  There are two functions to write:


  <ul>
  <li> <code class = "inline">makeMDS(points,metric='euclidean',random_state=25):</code>
      This function has three inputs, fits the data, and returns the embedded coordinates:
      <ul>
          <li> <code class = "inline">points</code>: a numpy array representing the features/coordinates of the input.  Assumes all values in the array are numeric.
          <li> <code class = "inline">metric</code>: the metric used to compute the distance matrix.  The default value is 'euclidean'.  Other common values are 'hamming' and 'cityblock'.
          <li> <code class = "inline">random_state</code>:  the random seed used when instantiating the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html">MDS analysis</a>.  The default value is 25.
        </ul>

        The function first computes the distance matrix for <code class = "inline">points</code>, using the specified distance as a parameter to
        <code class = "inline">scipy.spatial.distance.cdist</code>.  The distance matrix is used to fit the MDS model and the result of <code class = "inline">fit_transform</code> is returned.

        <li> <code class = "inline">makeDisplayDF(df,md_fit,legs):</code>
            This function has three inputs and returns a DataFrame:
            <ul>
                <li> <code class = "inline">df</code>: a DataFrame that includes the column <code class = "inline">member</code> that contains the legislator ID of the member.
                <li> <code class = "inline">md_fit</code>: an array of fitted coordinates, in the same order as <code class = "inline">df</code>.
                <li> <code class = "inline">legs</code>: a DataFrame containing information about the legislators that contains columns <code class = "inline">leg_id</code> with the legislator's ID and <code class = "inline">party</code> containing the legislator's party.
              </ul>

              The function returns a DataFrame to use for plotting with columns
              <code class = "inline">x</code> (storing <code class = "inline">md_fit[:,0]</code>), <code class = "inline">y</code> (storing <code class = "inline">md_fit[:,1]</code>), <code class = "inline">party</code> (storing the party affiliation of the legislator).
    </ul>

    <p>For example, assuming your functions are in <code class=inline>p47</code> and the appropriate libraries are loaded:
    <pre><code class="blockcode">from sklearn.manifold import MDS
from scipy.spatial.distance import cdist
import p47
#Helper function to display plots:
def displayPlot(vote2d,title):
    sns.scatterplot(data = vote2d,x="x", y="y", hue="party",
                    hue_order=['Democrat', 'Republican', 'Libertarian']);
    plt.title(title)
    plt.show()
df = pd.read_csv('vote_pivot.csv')
votes = df.drop('member',axis=1).to_numpy()
legs = pd.read_csv('legislators.csv')[['leg_id','party']]
#Fit to Euclidean distances:
md_fit = p47.makeMDS(votes)
vote2d = p47.makeDisplayDF(df,md_fit,legs)
displayPlot(vote2d,'MDS of Votes with Eucidean Distances')
#Fit to Hamming distances:
md_fit = p47.makeMDS(votes, metric="hamming")
vote2d = p47.makeDisplayDF(df,md_fit,legs)
displayPlot(vote2d,'MDS of Votes with Hamming Distances')
#Fit to Manhattan distances:
md_fit = p47.makeMDS(votes, metric="cityblock")
vote2d = p47.makeDisplayDF(df,md_fit,legs)
displayPlot(vote2d,'MDS of Votes with Manhattan Distances')
</code></pre>

    <p>would display:

    <p>
    <img src = "votes_mds_euclid.png" height = 300>
    <img src = "votes_mds_hamming.png" height = 300>
    <img src = "votes_mds_manhattan.png" height = 300>

  <p> The above runs used the files from the textbook:
      <a href="https://raw.githubusercontent.com/DS-100/textbook/master/content/ch/26/vote_pivot.csv">vote_pivot.csv</a> and
      <a href="https://raw.githubusercontent.com/DS-100/textbook/master/content/ch/26/legislators.csv">legislators.csv</a>.

  <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>


  <p></p><p><a name="p48"><b>Program 48: Transit Distances.</b> &emsp; <i>Due noon, Thursday, 2 December.</i>
      <br>(Learning Objective: to gain better insight into non-Euclidean distances via dimensionality reduction methods.)</p>

      <p> This program focuses on travel times in NYC and how well they estimate the aerial (Euclidean) distance between points.  To compare various measures of distances, we need several functions:

        <ul>
          <li> <code class = "inline">extractMx(df, dropCols = ['Name','Position']):</code>
            This function has two inputs and returns matrix with all times in minutes.
            <ul>
                <li> <code class = "inline">df</code>: a DataFrame.  Assumes that
                  the columns specificed by <code class = "inline">dropCols</code>
                  are columns of <code class = "inline">df</code> and the remaining entries are expressed as hours and minutes.
                <li> <code class = "inline">dropCols</code>: columns containing non-temporal data and to be dropped before converting to numeric values.
                  The default values is <code class = "inline">['Name','Position']</code>.
              </ul>
              The function returns the array of numeric values.  Each entry is the corresponding number of minutes to the input.  For example, if the entry in the DataFrame is <code class = "inline">59 mins</code>, then
              <code class = "inline">59</code> is placed in the corresponding entry of the matrix.  Similarly, if the entry in the DataFrame is <code class = "inline">2 hours 3 mins</code>, then
              <code class = "inline">123</code> is placed in the corresponding entry of the matrix.
          <li> <code class = "inline">scaleMx(distMx, i=0,j=1):</code>
            This function has three inputs and returns the matrix distMx scaled by the ith, jth entry.
            <ul>
                <li> <code class = "inline">distMx</code>: a distance matrix.  Assumes diagonal values are 0, and all other values are numeric and non-negative.
                <li> <code class = "inline">i</code>: the x coordinate of the entry to be used to scale the matrix.  It has a default value of 0.
                <li> <code class = "inline">j</code>: the y coordinate of the entry to be used to scale the matrix.  It has a default value of 1.
              </ul>
              The function returns the array scaled by the entry at [0,1].  That is, it divides through all entries by that value.  For example, if the specified entry is <code class = "inline">59</code>, then all entries in the returned matrix are divided by <code class = "inline">59</code>.
        </ul>

        <p> Using <a href="https://developers.google.com/maps/documentation/distance-matrix/overview">Google Maps API</a>, we generated the amount of time it would take to travel between the following landmarks:
        <ul>
            <li> Bronx Zoo (Bronx),
            <li> Empire State Building (Manhattan),
            <li> National Lighthouse Museum (Staten Island),
            <li> FDR Four Freedoms Park (Roosevelt Island),
            <li> Citi Field (Queens),
            <li> Coney Island (Brooklyn), and
            <li> Hunter College (Manhattan)
        </ul>
        by driving, transit, and walking
        (files:
        <a href="nyc_landmarks_driving.csv">nyc_landmarks_driving.csv</a>,
        <a href="nyc_landmarks_transit.csv">nyc_landmarks_transit.csv</a>,
        <a href="nyc_landmarks_walking.csv">nyc_landmarks_walking.csv</a>
        ).
        <p>
        <iframe src="nycMap.html" style="height: 400px" name="internal"></iframe>


        <p>
      Each file has the entries listed in hours and minutes.  The first function extracts the time from each and returns a matrix of numeric values representing total minutes for each entry.
      For example, assuming your functions are in <code class=inline>p48</code> and the appropriate libraries are loaded:
      <pre><code class="blockcode">transit = pd.read_csv('nyc_landmarks_transit.csv')
print(transit)
transit_mx = p48.extractMx(transit)
print(transit_mx)</code></pre>

      would print:
      <pre><code class="datablock">                          Name  ...  Hunter College
0        Empire State Building  ...         21 mins
1                    Bronx Zoo  ...   1 hour 2 mins
2   National Lighthouse Museum  ...  1 hour 21 mins
3  FDR Four Freedom State Park  ...         37 mins
4                   Citi Field  ...         35 mins
5                 Coney Island  ...  1 hour 10 mins
6               Hunter College  ...          0 mins

[7 rows x 9 columns]
[[  0  55  79  29  40  61  21]
 [ 55   0 105  72  70 106  62]
 [ 99 113   0  83  98 105  81]
 [ 29  71  92   0  41  84  37]
 [ 39  73 105  41   0  95  35]
 [ 59 123  91  83 102   0  70]
 [ 16  67  86  24  35  71   0]]</code></pre>

      If we normalize by the first non-zero entry (0,1) (note that we can't use (0,0), or any (i,i) since it's a distance matrix so would be dividing through by 0):
      <pre><code class="blockcode">transit_normed = p48.scaleMx(transit_mx)
print(transit_normed)</code></pre>
      would print:
<pre><code class="datablock">[[0.         1.         1.43636364 0.52727273 0.72727273 1.10909091 0.38181818]
 [1.         0.         1.90909091 1.30909091 1.27272727 1.92727273  1.12727273]
 [1.8        2.05454545 0.         1.50909091 1.78181818 1.90909091  1.47272727]
 [0.52727273 1.29090909 1.67272727 0.         0.74545455 1.52727273  0.67272727]
 [0.70909091 1.32727273 1.90909091 0.74545455 0.         1.72727273  0.63636364]
 [1.07272727 2.23636364 1.65454545 1.50909091 1.85454545 0.          1.27272727]
 [0.29090909 1.21818182 1.56363636 0.43636364 0.63636364 1.29090909  0.        ]]</code></pre>


  <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>



<p></p><p><a name="p49"><b>Program 49: Toy Clusters.</b></b> &emsp; <i>Due noon, Friday, 3 December.</i>
    <br>(Learning Objective: to build intuition and facility with k-means clustering.)</p>
    <p>

    In Lecture #25 and <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html">Python Data Science Handbook: Section 5.11</a>, we clustered the digits data set using K-means clustering and used t-SNE to improve accuracy.  The digits data set is one of 7 <a href="https://scikit-learn.org/stable/datasets/toy_dataset.html">toy datasets</a> included in <a href="https://scikit-learn.org/stable/index.html">sklearn</a> that can be quickly loaded to  try different algorithms.

    <p>
    For this program, modify the code from lecture to write a function that allows you to run three different preprocessing of a toy dataset ("none", "TSNE", or "MDS"), applies K-Means clustering, takes the mode of each cluster as the predicted label, and then returns the accuracy of the prediction.

    <ul>
    <li> <code class = "inline">clusterDemo(toy_dataset, n_components = 2, n_clusters = 3, method = "none", random_state=0):</code>
        This function has five inputs and returns the accuracy (value between 0 and 1):
        <ul>
            <li> <code class = "inline">toy_dataset</code>: a dataset from the sklearn toy datasets (see above).  Assumes that the dataset is of type
            <code class = "inline">sklearn.utils.Bunch</code> and has fields
            <code class = "inline">data</code> and
            <code class = "inline">target</code>.
            <li> <code class = "inline">n_components</code>: the number of components for the MDS or t-SNE analysis.  The default value is <code class = "inline">2</code>.
            <li> <code class = "inline">n_clusters</code>: the number of components for the K-Means analysis.  The default value is <code class = "inline">3</code>.
            <li> <code class = "inline">method</code>: the method used for preprocessing the data.  Possible values are <code class = "inline">"none"</code> (which does no preprocessing), <code class = "inline">"MDS"</code> (which uses MDS to preprocess), and
            <code class = "inline">"TSNE"</code>.The default value is <code class = "inline">"none"</code>.
            <li> <code class = "inline">random_state</code>: the random seed for MDS, TSNE, and K-Means clustering methods.  The default value is <code class = "inline">0</code>.
          </ul>
          The function should:
          <ol>
              <li> Preprocess the dataset via the method specified.  If method is <code class = "inline">"none"</code>, then the original data is used.
              <li> Run the K-means clustering method on the data.
              <li> Label each element by the most common label for its cluster (i.e. the mode of the labels).
              <li> Return the accuracy of the predicted labels versus the actual labels in the dataset.
      </ul>

      <p>For example, assuming your functions are in <code class=inline>p49</code> and the appropriate libraries are loaded, we can run the function on the relatively small datasets of iris species and wine classifications:
      <pre><code class="blockcode">iris = datasets.load_iris()
no_preproc = p49.clusterDemo(iris)
print(f'Iris:  The accuracy with no-preprocessing is {no_preproc}.')
tsne_proc = p49.clusterDemo(iris, method = "TSNE")
print(f'Iris: The accuracy with TSNE preprocessing is {tsne_proc}.')
mds_proc = p49.clusterDemo(iris, method = "MDS")
print(f'Iris: The accuracy with MDS preprocessing is {mds_proc}.')

wine = datasets.load_wine()
no_preproc = p49.clusterDemo(wine, n_components = 3, random_state=10)
print(f'Wine:  The accuracy with no-preprocessing is {no_preproc}.')
tsne_proc = p49.clusterDemo(wine, n_components = 3, method = "TSNE", random_state=10)
print(f'Wine: The accuracy with TSNE preprocessing is {tsne_proc}.')</code></pre>

      would print:
      <pre><code class="datablock">Iris:  The accuracy with no-preprocessing is 0.8933333333333333.
Iris: The accuracy with TSNE preprocessing is 0.9133333333333333.
Iris: The accuracy with MDS preprocessing is 0.9.
Wine:  The accuracy with no-preprocessing is 0.702247191011236.
Wine: The accuracy with TSNE preprocessing is 0.6797752808988764.</code></pre>

      <p>We can also run on the digits dataset.  It's larger and the t-SNE and MDS methods will take a bit of time to return their answers:
      <pre><code class="blockcode">digits = datasets.load_digits()
no_preproc = p49.clusterDemo(digits, n_clusters = 10, method = "none", random_state=20)
print(f'Digits: The accuracy with no-preprocessing is {no_preproc}.')
tsne_proc = p49.clusterDemo(digits, n_clusters = 10, method = "TSNE", random_state=20)
print(f'Digits: The accuracy with TSNE preprocessing is {tsne_proc}.')
mds_proc = p49.clusterDemo(digits, n_clusters = 10, method = "MDS", random_state=20)
print(f'Digits: The accuracy with MDS preprocessing is {mds_proc}.')</code></pre>

      would print:
      <pre><code class="datablock">Digits: The accuracy with no-preprocessing is 0.7946577629382304.
Digits: The accuracy with TSNE preprocessing is 0.9432387312186978.
Digits: The accuracy with MDS preprocessing is 0.676126878130217.</code></pre>


    <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>

    <p><i>Hints:
      <ul>
        <li> The import statements needed for the function are:
          <pre><code class="blockcode">import pandas as pd
import numpy as np
from sklearn.manifold import TSNE, MDS
from sklearn.cluster import KMeans
from scipy.stats import mode
from sklearn.metrics import accuracy_score</code></pre>
        <li> Both the MDS and t-SNE methods use the <code class=inline>n_components</code> when instantiating the model.  Not passing this parameter to the functions will give different answers.
        <li> The <code class=inline>random_state</code> parameter is used by both the preprocessing steps (MDS and t-SNE) as well as for the K-Means clustering.  Not passing this parameter to the functions will give different answers.
      </ul>
    </i>

<p></p><p><a name="p50"><b>Program 50: 4-Coloring.</b> &emsp; <i>Due noon, Tuesday, 7 December.</i>
    <br>(Learning Objective: to apply k-means clustering to larger datasets.)</p>
      <p>

    In Lecture #25 and <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html">Python Data Science Handbook: Section 5.11</a>, we used K-Means clustering to display an image using 16 colors.  The color values of the image were treated as 3D vectors, and the chosen colors were the centers of the clusters of those values.  For this program, write a function that takes an image and the number of clusters and returns the image recolored with cluster centers.

    <ul>
      <li> <code class = "inline">coloring(img, n_clusters = 4, random_state=5):</code>
      This function has three inputs and returns an np-array containing the recolored image:
      <ul>
          <li> <code class = "inline">img</code>: an np-array containing an image, from <code class = "inline">sklearn.datasets.load_sample_image()</code>sor <code class = "inline">plt.imread()</code>.
          <li> <code class = "inline">n_clusters</code>: the number of clusters used for <code class = "inline">MiniBatchKMeans</code>. The default value is <code class = "inline">4</code>.
          <li> <code class = "inline">random_state</code>: the random seed used for the <code class = "inline">MiniBatchKMeans</code>. The default value is <code class = "inline">5</code>.
      </ul>
      The function returns a recolored image, where each pixel is assigned the value of its cluster..
    </ul>

    <p>For example, assuming your functions are in <code class=inline>p50</code> and the appropriate libraries are loaded.  Here is the example from lecture:
    <pre><code class="blockcode">from sklearn.datasets import load_sample_image
china = load_sample_image("china.jpg")
china_4col = p50.coloring(china)
fig, ax = plt.subplots(1, 2, figsize=(16, 6),
                       subplot_kw=dict(xticks=[], yticks=[]))
fig.subplots_adjust(wspace=0.05)
ax[0].imshow(china)
ax[0].set_title('Original Image', size=16)
ax[1].imshow(china_4col)
ax[1].set_title('4-color Image', size=16);
plt.show()</code></pre>

    which displays:
    <p>
    <img src = "china_4col.png" height=300>

    <p>We can also run it on <a href="hunterFlag.jpg">hunterFlag.jpg</a> (since it's a larger file it will take a bit longer to run):
    <pre><code class="blockcode">hunter = plt.imread('hunterFlag.jpg')
hunter_4col = p50.coloring(hunter, random_state = 70)
hunter_2col = p50.coloring(hunter, n_clusters = 2, random_state = 70)
fig, ax = plt.subplots(1, 3, figsize=(16, 6),
                       subplot_kw=dict(xticks=[], yticks=[]))
fig.subplots_adjust(wspace=0.05)
ax[0].imshow(hunter)
ax[0].set_title('Original Image', size=16)
ax[1].imshow(hunter_4col)
ax[1].set_title('4-color Image', size=16);
ax[2].imshow(hunter_2col)
ax[2].set_title('2-color Image', size=16);
plt.show()</code></pre>

    and display:
    <p>
    <img src = "hunter_4col.png" height=300>

  <p>Note: you should submit a file with only the standard comments at the top, this function, and any helper functions you have written. The grading scripts will then import the file for testing.</p>


<!--
<br><br>
  <p>
    <i>More to come...</i>
-->

<br><br><br><br><br><br>
<hr>
<a name="project">
<h2>Project</h2>
</a>

<p>
The required final project synthesizes the skills acquired in the course to analyze and visualize data on a topic of your choosing.  It is your chance to demonstrate what you have learned, your creativity, and a project that you are passionate about.  The intended audience for your project is your classmates as well as tech recruiters and potential employers.

</p>

<h3>Milestones</h3>

The project is broken down into smaller pieces that must be submitted by the deadlines below.  For details of each milestone, see the links.  The project is worth 25% of the final grade.  The point breakdown is listed in the right hand column.


<p>
<table class="handouts" border="1">
<tr>
	<th>Deadline:</th><th>Deliverables:</th><th>Points:</th><th>Submitted Via:</th>
</tr>
<tr>
	<td>Wednesday, 6 October, noon</td>
  <td><a href="#preproposal">Pre-Proposal</a></td>
  <td>15</td>
  <td>Gradescope</td>
</tr>
<tr>
	<td>Wednesday, <strike>20 October,</strike> 3 November, noon</td>
  <td><a href="#proposal">Title & Proposal</a></td>
  <td><strike>20</strike> 30</td>
  <td><strike>Blackboard</strike> Gradescope</td>
</tr>
<tr><strike>
	<td><strike>Wednesday, 27 October, noon</strike></td>
  <td><strike><a href="#peerReview">Peer Review #1</a></strike></td>
  <td><strike>15</strike></td>
  <td><strike>Blackboard</strike></td></strike>
</tr>
<tr>
	<td>Wednesday, <strike>3 November</strike> 10 November, noon</td>
  <td><a href="#dataCollection">Check-in #1 (Data Collection)</a></td>
  <td>20</td>
  <td>Gradescope</td>
</tr>
<tr>
	<td>Wednesday, <strike>10 November</strike> 17 November, noon</td>
  <td><a href="#analysis">Check-in #2 (Analysis)</a></td>
  <td>20</td>
  <td>Gradescope</td>
</tr>
<tr>
	<td>Wednesday, <strike>17 November</strike> 24 November, noon</td>
  <td><a href="#visualization">Check-in #3 (Visualization)</a></td>
  <td>20</td>
  <td>Gradescope</td>
</tr>

<tr>
	<td>Wednesday, 1 December, noon</td>
  <td><a href="#draft">Draft Abstract & Website</a></td>
  <td>25</td>
  <td>Gradescope</td>
</tr>
<tr>
	<td>Monday, 6 December, noon</td><td>
  <a href="#peerReview2">Peer Review #2</a></td>
  <td><strike>15</strike> 20</td>
  <td>Gradescope</td>
</tr>

<tr>
	<td>Thursday, 9 December, noon</td>
  <td><a href="#abstract">Abstract</a></td>
  <td>25</td>
  <td>Gradescope</td>
</tr>

<tr>
	<td>Friday, 10 December, noon</td>
  <td><a href="#completeSite">Complete Project & Website</a></td>
  <td>50</td>
  <td>Gradescope</td>
</tr>


<tr>
	<td>Monday, 13 December, noon</td><td><strike>Project Video</strike><br><a href="#presentation">Presentation Slides</a></td><td>25</td>
  <td>Gradescope</td>
</tr>
<tr>
	<th colspan=2>Total Points:</th>
  <td>250</td>
  <td></td>
</tr>


</table>


<br><br><br><br>
<h3>Pre-Proposal</h3>
<a name="preproposal"></a>

This pre-proposal is meant to guide you as you brainstorm about your project.  It will also lead up to a more formal and structured project proposal later on.  The window for submitting pre-proposals opens Wednesday, 29 September.  If you would like feedback and the opportunity to resubmit for a higher grade, submit early in the window. Feel free to re-submit as many times as you like, up until the assignment deadline.  The instructing team will work hard to give feedback on your submission as quickly as possible, and we will grade them in the order they were received.


<p> In the pre-proposal, answer each question with 1 to 2 sentences:

<ul>
  <li> <b>Overview:</b> Describe your project in layman terms.  Think of this as an <a href="https://en.wikipedia.org/wiki/Elevator_pitch">elevator pitch</a>.</li>

  <li> <b>Importance:</b> Describe why this project has personal significance to you.</li>

  <li> <b>Originality:</b> Describe why you believe this project idea is unique and original.</li>

  <li> <b>Methods:</b> Describe how you are planning to apply the data science skills you learned in this class to this project.</li>

  <li> <b>Data:</b> What dataset(s) are you thinking of using?</li>

</ul>

<br>
<h3>Title & Proposal</h3>
<a name="proposal"></a>

The title and proposal serve multiple purposes:  they provide a framework to structure the proposed work, can form the overview on your project website, and make the basis of an elevator pitch when asked in interviews to explain your project from your digital portfolio.

<p>The structure echos that of the pre-proposal; an excellent way to start is to expand your pre-proposal to the template below, incorporating the feedback from the pre-proposal.  Submission is via Blackboard Turnitin, and the file formats accepted include PDF, HTML, DOC, and RTF files as well as plain text.  Your project proposal will be evaluated by the <a href="#peerReview">rubric below</a> by three other students in the course.

<p>Your file should include the following:

  <ul>
      <li><b>Title:</b>  a title that accurately captures your project.
      <li><b>Objective:</b> A paragraph concisely describing the problem you need to solve and your recommended solution. A tl/dr as you will.  Keep it short: 2 - 3 sentences.  More background information and details of the solution are in subsequent sections.  Feel free to use portions of the pre-proposal here.
      <li><b>Background:</b> Why is it important to solve this problem?  Whom is it for?  Give some general context for the reader.
          <ul>
            <li> What is the existing solution(s), if any, and what are the drawbacks?
            <li> Key Term Definitions:  What are some terms specific to your project that someone might not know?  ELI5 them here.
            <li> Background resources: Include a link to any research you have done here.
          </ul>
      <li><b>Solution Overview:</b>
          Give a high-level overview of your proposed solution.  Diagrams are very useful in this section to describe the solution (e.g. how the data flows through the model) but not required.  Also include:
            <ul>
              <li> Dependencies and assumptions:  What version of Python is being used and what additional libraries?
              <li>Inputs:  Include a list of your planned data sources with URL's.  (Note all data must be publicly available.)
          	  <li>Expected Outputs:  What outputs do you plan to generate?  Include planned data visualizations, descriptive statistics, models, and predictions.
              <li> Success Metric: What is defined as a success?  How is that measured?
            </ul>
        <li> <b>Security and Privacy Considerations:</b> Will you   be working with personal identifiable information (PII)?
        Can your model be mis-used for evil, not good?
        How do you plan to mitigate that?

      <li> <b>Appendix:</b> (Optional):  Include miscellaneous
          action items, decisions, links, research, and any open questions still not addressed.
  </ul>


<h3>Peer Review #1</h3>
<a name="peerReview"></a>

  The proposal and titled will be graded following the rubric below by three other students.

  <h4>Grading Rubric for First Peer Review:</h4>
    <ol>
      <li> Does the title accurately capture the planned project?
      <li> Read through the proposal and describe it <i>in your own words</i> in 2 sentences.
      <li> Does the objective section clearly describe the project?  What would you add to make it clearer or more reflective of the project?
      <li> Why is this project important or interesting?  Include the reasons from the proposal.  If you found none, provide two reasons.
      <li> Was the explanation of key terms sufficient?
      <li> Did the links provided work?  Was the data chosen well and sufficient to accomplish the objectives above?
      <li> Are the libraries and dependencies appropriate for the project?
      <li> Are the planned outputs appropriate for the project?
      <li> Were security and privacy considerations handled sufficiently?
      <li> Based on their success metric, do you think the proposed solution will be successful?  Why or why not?
    </ol>

<h3>Check-in #1 (Data Collection)</h3>
<a name="dataCollection"></a>

There are periodic check-ins to make sure that you are making progress on your project.  All ask for the following information:
<ul>
  <li> The title of your project
  <li> Brief description of your project (1-2 sentences)
  <li> Any major changes to the focus of your project and why (e.g. could not find the data needed, or data was more compelling for a related project).
  <li> Progress made towards your final project:  how much have you completed on data collection, analysis, visualization, website and final presentation.
</ul>
In addition, the first one focuses on data collection and includes
<ul>
	<li> a list (with links) of all data sources used,
	<li> for each data source:
		<ul>
			<li> a description of what and why you are using that source,
			<li> how you extracted the data from the source (i.e. detail how you downloaded/scraped the data as well as the file processing needed), and
			<li> any issues you had or forsee with the analysis of the data.
		</ul>
</ul>



<h3>Check-in #2 (Analysis)</h3>
<a name="analysis"></a>

There are periodic check-ins to make sure that you are making progress on your project.  All ask for the following information:
<ul>
  <li> The title of your project
  <li> Brief description of your project (1-2 sentences)
  <li> Any major changes to the focus of your project and why (e.g. could not find the data needed, or data was more compelling for a related project).
  <li> Progress made towards your final project:  how much have you completed on data collection, analysis, visualization, website and final presentation.
</ul>
In addition, the second one focuses on progress you have made on your data analysis and includes the results of the initial analysis for each data set.



<h3>Check-in #3 (Visualization)</h3>
<a name="visualization"></a>

There are periodic check-ins to make sure that you are making progress on your project.  All ask for the following information:
<ul>
  <li> The title of your project
  <li> Brief description of your project (1-2 sentences)
  <li> Any major changes to the focus of your project and why (e.g. could not find the data needed, or data was more compelling for a related project).
  <li> Progress made towards your final project:  how much have you completed on data collection, analysis, visualization, website and final presentation.
</ul>
In addition, the third one focuses on the visualizations and includes:
<ul>
	<li> links to draft images (can be scans of hand drawn sketches with details of what you plan to do) that you plan to use in your project website presentation,
  <li> identify the main theme of your topic (e.g. schools, transportation, health/covid, social justice, etc.).  The peer review of your draft abstract and website will be by others with a similar theme, so, this will determine to which subcourse you will submit the next component.  If you do not choose a theme, we will include you in the "General" group.
</ul>



<h3>Draft Abstract & Website</h3>
<a name="draft"></a>

This is done via Gradescope.  You will be sent an invitation to a subcourse (based on your theme you specified in the last check-in).  If you did not submit a theme, you will be automatically placed in the "general" subcourse.

The following are requested:
<ul>
  <li> Project Title
  <li> Theme:  we will be grouping projects by themes for peer reviews as well as on the course projects page.
  <li> Your preferred version of your name for our projects page.  If you use multiple names, use the one here that you use (or plan to use) for internships and job applications
  <li> Your LinkedIn name (optional, but makes it easier for employers to find you by linking in your project with your digital portfolio)
  <li> Location (URL) of your website
  <li> The final version of the abstract will be listed on the course project page.
</ul>

<h3>Peer Review #2</h3>
<a name="peerReview2"></a>

This is done via Gradescope.  You will be sent an invitation to a subcourse (based on your theme).  The themes are:
  <ul>
    <li> health/covid
    <li> social justice
    <li> urban planning
    <li> transportation
    <li> science & environment
    <li> education & arts
    <li> general
  </ul>

  <p>Your access to the subcourse will be changed to <code class=inline>Reader/TA</code> after the drafts have been submitted.  You need to complete peer reviews for 4 other students.  The system logs all entries and grades will be assigned based on the automatic entries.  The points for this portion are given for each peer review you complete (up to 4).

<p>
For the peer review, as covered in Lecture #25 (video available on Blackboard):
<ol>
  <li> Log into your themed Gradescope course.
  <li> Click on <code class=inline>Draft Abstract & Website</code> under the list of <code class=inline>ACTIVE ASSIGNMENTS</code>:<br>
    <p>
      <img src="peerReview_grading_1.png" height=100>
    <br>
  <li> The next screen shows the Grading Dashboard:<br>
    <p>
       <img src="peerReview_grading_2.png" height=100><br>
  <li> Click on <code class=inline>1: Draft Abstract</code>.  It will show either a list of submissions or a grading screen.  If the former, click on the first name to get to a submission.  On the right hand side of the window, you will see a grading rubric with check boxes for 4 different reviewers.  Scroll down to the first reviewer that's not be started.  In this case, it's Reviewer 2.  If all 4 reviews have been completed for this submission or its the work you submitted, click on the <code class=inline>Next</code> button to see another review.<br>
       <p><img src="peerReview_grading_3.png" height=400><br>
      <li> Read the abstract on the left, and check the corresponding boxes on the right.  Append any comments to the end of the comment box at the bottom of the menu.  Then click the <code class=inline>Next</code> button.

      <li> If you have finished  less than 4 reviews of abstracts, repeat from Step 4.  If you have finished 4 reviews of abstract, go back to the <code class=inline>Grading Dashboard</code> and fill out 4 reviews of websites.
</ol>

    <p> Be kind and constructive with your comments!


<h3>Abstract</h3>
<a name="abstract"></a>

This is the final version of your abstract and associated information.  It should include 2-3 sentences describing the project and your results.

<p>This portion of the project is submitted via Gradescope as a text file.  It will first be screened by an autograder to make sure the required fields are included.  After the submission deadline, each abstract will also be read and the remaining points assigned manually.  The required fields follow those of the earlier draft submission.  For the autograder to find each field, precede your entry by the field name followed by a colon.  For example, for the title, your file should include:

<pre><code class="datablock">
Title: YOURTITLEHERE
</code></pre>

The autograder is expecting the following fields in your text file:
<ul>
  <li> <b>Name:</b> Your name, as it appears in your Gradescope registration.
  <li> <b>Email:</b> The email you are using for Gradescope.
  <li> <b>Resources:</b> A list of any resources used.  This should include the links to your data sources, as well as any additional resources used.  Include classmates and tutors that you worked with, along with any websites or tutorials that you used.
  <li> <b>Title:</b> Include a title that describes the underlying problem/focus.
  <li> <b>Theme:</b> Include 3 or fewer keywords, separated by commas, that classify the overall focus of your project.  At least one of the keywords must be from the list:  health/covid, social justice, urban planning, transportation, science & environment, education & arts, general.
  <li> <b>Abstract:</b> In 3 or fewer sentences, describe your focus and your approach using data science to address it.
  <li> <b>Relevance to NYC:</b> How does this relate to NYC?
  <li> <b>URL:</b> Include a working link to your project website.
  <li> <b>Private:</b> Include this field if you do not want you site included on the course webpage.  If this field is missing, your page will be included by title, along with your name, the abstract, and the theme on the course webpage.
  <li> <b>GitHub:</b> If you would like your Github login to be included in the listing, include it here.
  <li> <b>LinkedIn:</b>  If you would like your LinkedIn login to be included in the listing, include it here.
</ul>


<h3>Complete Project & Website</h3>
<a name="completeSite"></a>

This part of your project is submitted via Gradescope as a .py file that contains in the introductory comments the website (preceded by URL:) and the code you wrote for your project in the body of the file.

<p>
The autograder will check for the python file and that includes the title, the resources, and the URL of your website.  After the submission deadline, the code and the website will be graded.

<p> Your code should include documentation about what each function does and details about the data format and sources.

<p> The project must be submitted as a webpage (use google sites or other pre-built if you're not comfortable writing html).  The project website must include:
<ul>
	<li> a front image and title that summarizes your project,
	<li> an overview paragraph on what you did:  what was your underlying hypothesis?  what data, methods, and tools did you use to test and explore it?
	<li> a data section with a paragraph detailing each data set,
	<li> a techniques section with a paragraph detailing,
	the process and tools you usedeach technique use,
	<li> a citations section with links to all data sources, code sources, publications used.
</ul>

<h3>Presentation Slides</h3>
<a name="presentation"></a>

For the last part of the project, include two slides that serve as a graphical overview ("lightning talk" slides) of your project.  You should submit to Gradescope a pdf file that contains two slides that summarize your project:
<ul>
	<li> Slide 1: the front image and title from website, as well as your name
	<li> Slide 2:  discoveries & conclusions (with images)
</ul>

</div>

<!--


<p>
  <i>More to come...</i>
-->

</div>
</body>
</html>
