<!DOCTYPE doctype PUBLIC "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
  <meta http-equiv="Content-Type"
 content="text/html; charset=iso-8859-1">
  <title>Program 7, CSci 39542: Data Science, Hunter College</title>
</head>
<STYLE>A {text-decoration: none;}
th, td { padding: 5px; }
code {
  background-color: #eeeeee;
}
.inline {
  padding: 1px;
}
.blockcode {
  border: 1px solid #999999;
  display: block;
  padding-left: 10px;
  padding-top : 2px;
  padding-bottom : 2px;
  margin: 5px;
}
.datablock {
  border: 1px solid #eeeeee;
  display: block;
  padding: 7px;
  padding-top : 0px;
  margin: 5px;
}
</STYLE>
<body>


<div style="margin: 15px;width:100%;">
    <span style= "float: left;font-size:larger"><a href="index.html">CSci 39542</a></span>
    <span style= "float: right">
      <a href="syl.html">Syllabus</a>&nbsp;&nbsp;&nbsp;
      <a href="resources.html">Resources</a>&nbsp;&nbsp;&nbsp;
      <a href="work.html">Coursework</a><!--&nbsp;&nbsp;&nbsp;
      <a href="faq.html">FAQ</a>-->
    </span>
</div>

<br>
<br>
<hr>

<div style="margin:50px">


<h2>Program 7:  Commodity Pricing
  <br>CSci 39542: Introduction to Data Science<br>
<a href="http://www.hunter.cuny.edu/csci">Department of Computer Science</a><br>
<a href="https://hunter.cuny.edu">Hunter College</a>, <a href="https://www.cuny.edu">City University of New York</a><br>
Spring 2023<br><br>
</h2>


<hr>
<a href="work.html#cw">Classwork</a>&nbsp;&nbsp;&nbsp;
<a href="work.html#quizzes">Quizzes</a>&nbsp;&nbsp;&nbsp;
<a href="work.html#hw">Homework</a>&nbsp;&nbsp;&nbsp;
<a href="work.html#project">Project</a>&nbsp;&nbsp;&nbsp;
<hr>

<object width=100% height=50% type="text/html" data="generalNotes.html" border="0"
   style="overflow: hidden;">
</object>
<hr>

<h2>Program Description</h2>

<br>
<p><a name="p7"><b>Program 7: Commodity Pricing.</b></b> &emsp; <i>Due 10am, Wednesday, 15 March.
<br>Learning Objective: to build models with polynomial features, employ thresholds to decide model fitness, and use regularization techniques to better fit models.
<br>Available Libraries: pandas, numpy, scikit-learn and core Python 3.6+.  The available packages used for scikit-learn:  sklearn.model_selection, sklearn.preprocessing, sklearn.linear_model, and sklearn.metrics.
<br>Data Sources:  <a href="https://fred.stlouisfed.org/series/USINFO">St. Louis Federal Reserve Bank Online Data (FRED)</a>.
<br>Sample Datasets: 
  <a href="fred_cpi_eggs_oil_cars_INDEXED.csv">fred_cpi_eggs_oil_cars_INDEXED.csv</a>:  Consumer Price Index, Price of Eggs, Heating Oil, and Cars, over last 10 years, indexed to January 1, 2013<br></i>


<p>This program builds models for economic data from <a href="https://fred.stlouisfed.org/">St. Louis Federal Reserve Bank Online Data (FRED)</a>. 
  
<p><a href="https://fred.stlouisfed.org/"><img src="fred_cpi_eggs_oil_cars.png" width="800"></a></p>

<p>This program builds models, using polynomial features and regularization for  economica data.   The pipeline below is general enough to handle actual prices as well as changes over time and price indices.  The first steps are importing the data and splitting it into training and testing sets.  The training set is then used to determine the degree of the polynomial for the model, and once that is determined, regularization, with cross validation, is used to fit a model that can be validated with the testing data and used for predictions.
  

<p>The assignment is broken into the following functions to allow for unit testing:


<ul>
  <li> <code class = "inline">import_data(csv_file)</code>:
  This function takes one input:
    <ul>
      <li> <code class = "inline">csv_file</code>: the name of a CSV file time series data for a commodity from <a href="https://fred.stlouisfed.org/series/USINFO">FRED</a>. 
    </ul>
  The data in the file is read into a DataFrame, and 3,...)   and <code class = "inline">yes</code> is the <code class = "inline">USINFO</code> column of your dataset. The <code class = "inline">xes</code> can be generated by looping through the dataset or using the index as a column (e.g. <code class = "inline">df.index.to_series()</code>).adds an additional column, based on the index.
  The resulting DataFrame is returned.</li><br>


  <li> <code class = "inline">split_data(df, y_col_name, test_size = 0.25, random_state = 21):</code>
  This function takes 4 input parameters:
  <ul>
    <li> <code class = "inline">df</code>: a DataFrame containing with a columns <code class = "inline">units</code>.</li>
    <li>  <code class = "inline">y_col_name</code>: the name of the column of the dependent variable.</li>
    <li>  <code class = "inline">test_size</code>: accepts a float between 0 and 1 and represents the proportion of the data set to use for training.  This parameter has a default value of 0.25.</li>
    <li>  <code class = "inline">random_state</code>:  Used as a seed to the randomization.  This parameter has a default value of 1870.</li>
  </ul>
  Returns the data split into 4 subsets, corresponding to those returned by <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>:
  <code class = "inline">x_train</code>,
  <code class = "inline">x_test</code>,
  <code class = "inline">y_train</code>, and
  <code class = "inline">y_test</code>. where <code class="inline">units</code> is the "x" column and the input parameter, <code class="inline">y_col_name</code> is the "y" column.<br>
  Note:  this is function is very similar to the splitting of data into training and testing sets from <a href="p6.html">Program 6</a>.</li><br>


  <li> <code class = "inline">fit_poly(xes, yes, epsilon=100, verbose=False)</code>:
  This function takes four inputs:
  <ul>
    <li> <code class = "inline">xes</code>: a DataFrame that
    includes the column <code class="inline">units</code>.
    <li> <code class = "inline">yes</code>: a series of the same length as <code class = "inline">xes</code>.
    <li> <code class = "inline">epsilon</code>: the size of the sample.  It has a default value of <code class = "inline">100</code>.
    <li> <code class = "inline">verbose</code>: when <code class = "inline">True</code>, prints out the MSE cost for each degree tried (in format:  <code class="inline">f'MSE cost for deg {deg} poly model: {error:.3f}'</code> for degrees 1, 2, ..., until the error is below <code class = "inline">epsilon</code>, see example below).  It has a default value of <code class = "inline">False</code>.      
  </ul>
  It returns the smallest integer degree >= 1 for which the model yields a MSE of less than the specified epsilon and the coefficients as a vector for <code class = "inline">df["units"]</code> and <code class = "inline">df[y_col]</code>.  If it does not find a model with an error less than <code class="inline">epsilon</code> by degree 5, returns <code class="inline">None</code>. 
  When fitting the linear regression model, the <code class="inline">fit_intercept=False</code>.
  <br>
  Hint:  see the <a href="https://learningds.org/ch/16/ms_overfitting.html">Chapter 16</a> for examples of using <code class="inline">PolynomialFeatures()</code>.</li><br>

  <li> <code class = "inline">fit_model(xes, yes, poly_deg=2 reg = "lasso"): </code>
    <ul>
      <li> <code class = "inline">xes</code>: a series of numeric values.
      <li> <code class = "inline">yes</code>: a series of numeric values.
      <li> <code class = "inline">poly_deg</code>: the degree of the polynomial features to be created.  It has a default value of <code class = "inline">2</code>.
      <li> <code class = "inline">reg</code>: The type of regularization used:  <code class="inline">ridge</code> or <code class = "inline">lasso</code>. It has a default value of <code class = "inline">lasso</code>.
    </ul>
    This function fits a model with polynomial features using Lasso or Ridge regression with cross validation:
    <ol>
      <li>Apply <code class="inline">PolynomialFeatures</code> to the <code class = "inline">xes</code> with degree equal to <code class = "inline">poly_deg</code>.</li>
      <li>If <code class = "inline">reg</code> equals <code class = "inline">ridge</code>, use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html">RidgeCV</a> to instantiate and fit a model to the polynomial features and <code class = "inline">yes</code>.  Otherwise, use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html">LassoCV</a> to to instantiate and fit the model.</li>
      <li>Returns the model as serialized object (i.e. a pickled object).</li>
    </ol>   
  </li><br>


  <li> <code class = "inline">predict_using_trained_model(mod_pkl, poly_xes, yes): </code>

    This function takes three inputs:
    <ul>
      <li> <code class = "inline">mod_pkl</code>: a trained model for the data, stored in pickle format.
      <li> <code class = "inline">poly_xes</code>: an array or  DataFrame of numeric columns with no null values.
      <li> <code class = "inline">yes</code>: an array or DataFrame of numeric columns with no null values.
    </ul>
    Computes and returns the mean squared error and r2 score between the values predicted by the model (<code class = "inline">mod</code> on <code class = "inline">x</code>) and the actual values (<code class = "inline">y</code>).  
    Note that <code class = "inline">sklearn.metrics</code> contains two functions that may be of use:  <code class = "inline">mean_squared_error</code> and <code class = "inline">r2_score</code>.</li><br>

</ul>


<br>
<p>For example, let's use the FRED dataset of the Consumer Price Index, the price of eggs, the price of heating oil, and the price of cars, all indexed to have the same starting value of 100 on January 1, 2013 (<a href="fred_cpi_eggs_oil_cars_INDEXED.csv">fred_cpi_eggs_oil_cars_INDEXED.csv</a>):
<pre><code class="blockcode">csv_file = "fred_cpi_eggs_oil_cars_INDEXED.csv"
df = import_data(csv_file)
print('The DataFrame:')
print(df)
</code></pre>
will print:
<pre><code class="datablock">The DataFrame:
DATE        CPI       EGGS        OIL       CARS  units
0    2013-01-01  100.00000  100.00000  100.00000  100.00000      0
1    2013-02-01  100.54299  101.65546  103.26837  100.30355      1
2    2013-03-01  100.26027   99.58614   95.90818   99.41820      2
3    2013-04-01  100.05093   99.22400   89.38780   99.98735      3
4    2013-05-01  100.09237   96.74082   89.27521  100.44900      4
..          ...        ...        ...        ...        ...    ...
116  2022-09-01  127.99563  150.12933  106.16784   89.24935    116
117  2022-10-01  128.62063  176.87532  135.68469   99.74704    117
118  2022-11-01  128.88436  185.66994  124.54440   93.91640    118
119  2022-12-01  129.05356  219.86549   95.72411   87.74426    119
120  2023-01-01  129.72086  249.50854  100.54942  102.47897    120

[121 rows x 6 columns]</code></pre>

The column names were generated from FRED were rather cryptic, so they have been renamed.  Note that all have been normalized to 100 for the date 2013-01-01.

<p>Let's try the split data function on the CPI data:</p>
<pre><code class="blockcode">y_col_name = "CPI"
print(f'For the x column = "units", y_col = {y_col_name}')
print(df[ ["units", y_col_name] ])
x_train_cpi, x_test_cpi, y_train_cpi, y_test_cpi = split_data(df, y_col_name)

print('\nReturned sets of lengths:')
print(f"x_train_cpi: {len(x_train_cpi)}, x_test_cpi: {len(x_test_cpi)}")
print(f"y_train_cpi: {len(y_train_cpi)}, y_test_cpi: {len(y_test_cpi)}")</code></pre>
would print in a sample run:

<pre><code class="datablock">For the x column = "units", y_col = CPI
  units        CPI
0        0  100.00000
1        1  100.54299
2        2  100.26027
3        3  100.05093
4        4  100.09237
..     ...        ...
116    116  127.99563
117    117  128.62063
118    118  128.88436
119    119  129.05356
120    120  129.72086

[121 rows x 2 columns]

Returned sets of lengths:
x_train_cpi: 90, x_test_cpi: 31
y_train_cpi: 90, y_test_cpi: 31</code></pre>

<p>Let's fit a polynomial model to the CPI:</p>
<pre><code class="blockcode">eps = 5
  print(f'Finding the poly degree for training data with epsilon = {eps}:')
  deg = fit_poly(x_train_cpi.to_frame(),y_train_cpi,epsilon=eps,verbose=True)
  print(f'For epsilon = {eps}, poly has degree {deg}.')</code></pre>
  
would print in a sample run:
<pre><code class="datablock">Finding the poly degree for training data with epsilon = 5:
MSE cost for deg 1 poly model: 7.980
MSE cost for deg 2 poly model: 2.511
For epsilon = 5, poly has degree 2.  
</code></pre>  

We can adjust the sizes of training and testing sets, by using the keyword arguments:
<pre><code class="blockcode">y_col_name = "EGGS"
print(f'For the x column = "units", y_col = {y_col_name}')
print(df[ ["units", y_col_name] ])
x_train, x_test, y_train, y_test = split_data(df, y_col_name,test_size=0.33)

print('\nReturned sets of lengths:')
print(f"x_train: {len(x_train)}, x_test: {len(x_test)}")
print(f"y_train: {len(y_train)}, y_test: {len(y_test)}")</code></pre>
would print in a sample run:
<pre><code class="datablock">For the x column = "units", y_col = EGGS
  units       EGGS
0        0  100.00000
1        1  101.65546
2        2   99.58614
3        3   99.22400
4        4   96.74082
..     ...        ...
116    116  150.12933
117    117  176.87532
118    118  185.66994
119    119  219.86549
120    120  249.50854

[121 rows x 2 columns]

Returned sets of lengths:
x_train: 81, x_test: 40
y_train: 81, y_test: 40</code></pre>  

<p>Let's try the same epsilon for fitting the EGGS model:
</p>
<pre><code class="blockcode">eps = 5
print(f'Finding the poly degree for training data with epsilon = {eps}:')
deg = fit_poly(x_train.to_frame(),y_train,epsilon=eps,verbose=True)
print(f'For epsilon = {eps}, poly has degree {deg}.')</code></pre>
would print in a sample run:
<pre><code class="datablock">Finding the poly degree for training data with epsilon = 150:
MSE cost for deg 1 poly model: 582.143
MSE cost for deg 2 poly model: 404.864
MSE cost for deg 3 poly model: 212.212
MSE cost for deg 4 poly model: 207.756
MSE cost for deg 5 poly model: 192.272
For epsilon = 5, poly has degree None.</code></pre>

<p>Next, let's use cross validation Lasso regularization (the default for our function) to better fit our models for the CPI data:</p>

<pre><code class="blockcode">#Working with CPI:
transformer = PolynomialFeatures(degree=2)
x_train_poly = transformer.fit_transform(x_train_cpi.to_frame())
#Fit model to the poly features
#print(x_train_poly)
cpi_pkl = fit_model(x_train_cpi.to_frame(), y_train_cpi)
mod = pickle.loads(cpi_pkl)
print(mod.get_params())
print(x_train_poly)
print(f'model:  {mod.predict(x_train_poly)}')</code></pre>
would print in a sample run:
<pre><code class="datablock">model:  [115.4081765  100.78877974 103.75105901 105.76705462 104.53105731
116.71103081 100.87106528 103.18877452 107.38533682 123.64873
102.5630616  106.54876721 115.09103434 101.56877805 116.38017439
117.38302935 101.65277786 110.48304436 102.02306277 104.36820053
101.26706442 112.39104021 115.72874723 111.01104321 100.72020846
114.77732073 101.13677899 111.55275632 105.39676972 102.4482047
102.92820366 105.58019789 114.16017922 101.07677912 120.57673669
120.94873588 102.22877661 119.4813105  104.69734267 123.25273086
122.08530483 122.47101828 109.71676032 106.15105379 101.33734998
103.3242028  106.96019489 109.2230471  103.05677481 112.96703896
109.46818943 104.05277264 101.4110641  102.80306107 105.95733992
113.85675131 103.46305964 100.82820823 114.46703569 111.82875572
103.90020154 116.05274653 100.91735089 100.62420867 107.60305063
119.12302556 108.98133334 108.27676345 124.0481577  100.58820875
101.20020742 104.2087723  121.70301995 117.0453158  113.55675196
118.06874214 105.04019906 100.66535144 103.60534504 118.76816919
121.32416363 101.9253487  117.72417146 106.34819622 109.96875977
104.86705658 119.843024   100.58135162 100.96706507 100.5830659 ]</code></pre>

<p>We can see how well this model does with the testing data, as well as predicting for the future:</p>

<pre><code class="blockcode">#Predict with the model on training data:
x_test_poly = transformer.fit_transform(x_test_cpi.to_frame())
print('Built model, testing on testing data:')

mse_cpi, r2_cpi = predict_using_trained_model(cpi_pkl, x_test_poly, y_test_cpi)
print(f'The CPI model has a MSE for the testing set of {mse_cpi:2.2} and r2 score of {r2_cpi:2.2}.')</code></pre>
would print in a sample run:
<pre><code class="datablock">Built model, testing on testing data:
The CPI model has a MSE for the testing set of 2.8 and r2 score of 0.96.</code></pre>
 
Let's visualize the model (red line) with the full data set:
<pre><code class="blockcode">import matplotlib.pyplot as plt
import seaborn as sns

x_poly_all = transformer.fit_transform(df[['units']])
y_predict = mod.predict(x_poly_all)
sns.lineplot(df['units'],y_predict,color='red')
plt.scatter(df['units'],df['CPI'])
plt.title('CPI Quadratic Model')
plt.xlabel('Months since January 2013')
plt.show()</code></pre>
which shows:

<img src="cpi_quadratic_model.png" height="300">

<i>
<p>Notes and Hints:</p>
<ul>
  <li> You should submit a .py file with only the standard comments at the top, the specified functions, and any helper functions you have written. The grading scripts will then import the file for testing.
  If your file includes code outside of functions, either comment the code out before submitting or use a main function that is conditionally executed (see <a href="https://runestone.academy/ns/books/published//thinkcspy/Functions/mainfunction.html">Think CS: Section 6.8</a> for details).</li>
  
  <li> Include only the libraries you need (such as <code class = "inline">pandas</code>) for your functions and none of the ones for plotting (such as <code class = "inline">matplotlib.pyplot</code> and <code class = "inline">seaborn</code>) since the functions submitted are computing and not plotting.  Only the libraries listed in <b>Available Libraries</b> are loaded by the autograder.     

  <li>  See Lecture 3 or <a href="https://learningds.org/ch/09/wrangling_transformations.html">DS 100: Section 9.4</a> for working with dates and times.</li>  
  
</ul>
</i>

    

    
    </div>
    </body>
    </html>
    