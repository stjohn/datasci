<!DOCTYPE doctype PUBLIC "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
  <meta http-equiv="Content-Type"
 content="text/html; charset=iso-8859-1">
  <title>Program 11, CSci 39542: Data Science, Hunter College</title>
</head>
<STYLE>A {text-decoration: none;}
th, td { padding: 5px; }
code {
  background-color: #eeeeee;
}
.inline {
  padding: 1px;
}
.blockcode {
  border: 1px solid #999999;
  display: block;
  padding-left: 10px;
  padding-top : 2px;
  padding-bottom : 2px;
  margin: 5px;
}
.datablock {
  border: 1px solid #eeeeee;
  display: block;
  padding: 7px;
  padding-top : 0px;
  margin: 5px;
}
</STYLE>
<body>


<div style="margin: 15px;width=100%;">
    <span style= "float: left;font-size:larger"><a href="index.html">CSci 39542</a></span>
    <span style= "float: right">
      <a href="syl.html">Syllabus</a>&nbsp;&nbsp;&nbsp;
      <a href="resources.html">Resources</a>&nbsp;&nbsp;&nbsp;
      <a href="work.html">Coursework</a><!--&nbsp;&nbsp;&nbsp;
      <a href="faq.html">FAQ</a>-->
    </span>
</div>

<br>
<br>
<hr>

<div style="margin:50px">


<h2>Program 11: Digit Dimensions
  <br>CSci 39542: Introduction to Data Science<br>
<a href="http://www.hunter.cuny.edu/csci">Department of Computer Science</a><br>
<a href="https://hunter.cuny.edu">Hunter College</a>, <a href="https://www.cuny.edu">City University of New York</a><br>
Spring 2022<br><br>
</h2>


<hr>
<a href="work.html#cw">Classwork</a>&nbsp;&nbsp;&nbsp;
<a href="work.html#quizzes">Quizzes</a>&nbsp;&nbsp;&nbsp;
<a href="work.html#hw">Homework</a>&nbsp;&nbsp;&nbsp;
<a href="work.html#project">Project</a>&nbsp;&nbsp;&nbsp;
<hr>

<object width=100% height=50% type="text/html" data="generalNotes.html" border="0"
   style="overflow: hidden;">
</object>
<hr>

<h2>Program Description</h2>

<br>
   <p><a name="p11"><b>Program 11: Digit Dimensions.</b></b> &emsp; <i>Due noon, Thursday, 29 April.
     <br>Learning Objective: to increase facility with standard linear algebra approaches and strengthen understanding of intrinistic dimensions of data sets via exploration of the classic digits dataset).
     <br>Available Libraries: pandas, numpy, pickle, sklearn, and core Python 3.6+.
     <br>Data Sources: <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset of hand-written digits</a>, available in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">sklearn digits dataset</a>.
     <br>Sample Datasets: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">sklearn digits dataset</a>.<br></i>

<p>
In Lecture 22 and <a href="http://www.textbook.ds100.org/ch/25/pca_in_practice.html">Chapter 25</a>, we applied Principal Components Analysis as a dimensionality reduction technique and used scree plots to provide a visualization of the captured variance.  This assignment continues our analysis of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">sklearn digits dataset</a>, focusing on visualization and intrinsic dimenion of the data.
In addition, we will write a function that allows the user to explore how many dimensions are needed to see the underlying structure of images from the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">sklearn digits dataset</a> (inspired by <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html">Python Data Science Handbook: Section 5.9</a> (PCA)).


<p>As in Program 10, we can view our images as "flattened" 1D arrays of length 64.
As discussed in <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html">Python Data Science Handbook: Section 5.9</a>, we can view the images as sums of the components.
If we let <code class=inline>x1 = [1 0 ... 0]</code>,
<code class=inline>x2 = [0 1 0 ... 0]</code>, ...,
<code class=inline>x64 = [0 ... 0 1]</code> (vectors corresponding to the axis), then we can write our images, <code class=inline>im = [i1 i2 ... i64]</code>, as: <code class="inline">im = x1*i1 + x2*i2 + x3*i3 + ... + x64*i64</code>.
For example, if we take the first entry from the dataset:
<pre><code class="datablock">[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3. 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]</code></pre>
We can write it as:
<pre><code class="datablock">im = x1*i1 + x2*i2 + x3*i3 + ... + x64*i64
   = x1*0  + x2*0  + x3*5  + ... + x64*0</code></pre>
plugging in the values for a given image (in this case the first image in the dataset)  into the equation.

<p>
In a similar fashion, we can represent the image in terms of the axis,<code class=inline>c1, c2, ... c64</code>,  that the PCA analysis returns:
<pre><code class="datablock">im = mean + c1*i1 + c2*i2 + ... + c64*i64</code></pre>
since the axis of PCA are chosen so that the first one captures the most variance, the second the next most, etc.  The later axis capture very little variance and likely add litte to the image.  For technical reasons, we include the mean (the reason is similar to when we "center" multidimensional data at 0).
This can be very useful for reducing the dimension of the data set, for example, here is the first image from above on the left:
<p>
<img src = "../fall21/digit_adding_comps.png" width=90%>
<br>
The next image is the overall mean, and each subsequent image is adding another component to the previous.  For this particular scan, the mean plus its first component is enough to see that it's a 0.

<p>In your program, include the following functions from <a href="p10.html">Program 10</a>.  You may use your earlier function or the Program 10 solution available on Blackboard:
<ul>
  <li> <code class = "inline">select_data(data, target, labels = [0,1]):</code>

    This function takes as three input parameters:
    <ul>
      <li> <code class = "inline">data</code>: a numpy array that
            includes rows of equal size flattened arrays,
      <li> <code class = "inline">target</code> a numpy array that contains the labels for each row in <code class = "inline">data</code>.
      <li> <code class = "inline">labels</code>: the labels from <code class = "inline">target</code> that the rows to be selected.  The default value is [0,1].
    </ul>
    Returns the rows of <code class = "inline">data</code> and
    <code class = "inline">target</code> where the value of <code class = "inline">target</code> is in <code class = "inline">labels</code>.
</ul>

And write the following new functions:


<ul>


  <li> <code class="inline">run_pca(xes):</code>
      This function takes as one input parameter:
      <ul>
          <li> <code class = "inline">xes</code>: a numpy array that
                  includes rows of equal size flattened arrays,
      </ul>
      It fits a model of <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn.decomposition.PCA</a> to the
      <code class = "inline">xes</code>.  The function returns the model and the transformed values.

  <li> <code class="inline">capture_85(mod):</code>
    This function takes as one input parameter:
    <ul>
        <li> <code class = "inline">mod</code>: a model of <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn.decomposition.PCA</a> that has been fitted to a dataset.
    </ul>
    Using the array of the amount of varianace of each component, <code class="inline">explained_variance_</code>,
    computes the captured variance (see <a href="http://www.textbook.ds100.org/ch/26/pca_in_practice.html">Chapter 26.3.1.1 Captured Variance and Scree Plots</a>).
    Returns the number of elements needed to capture more than 85% of the variance.

  <li> <code class="inline">average_eigenvalue(mod):</code>
    This function takes as one input parameter:
    <ul>
        <li> <code class = "inline">mod</code>: a model of <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn.decomposition.PCA</a> that has been fitted to a dataset.
    </ul>
    For the <code class = "inline">mod</code>,
    computes the average of the eigenvalues and returns the number of elements greater than the average.  See <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn.decomposition.PCA</a> for attributes of the model.

  <li> <code class = "inline">approx_digits(mod, img, numComponents=8):</code>
    This function has three inputs:
    <ul>
        <li> <code class = "inline">mod</code>: a model of <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn.decomposition.PCA</a> that has been fitted to a dataset.
        <li> <code class = "inline">img</code>: a flattened image from the dataset.
        <li> <code class = "inline">numComponents</code>: the number of components used in the approximation.  Expecting a value between 0 and 64.  The default value is 8.
    </ul>
    The function transforms the image, <code class = "inline">img</code>, and uses the resulting coefficents to compute an approximation of the image.  The approximation image (flattened array) is the mean plus the sum of the first <code class = "inline">numComponents</code>
    terms (i.e. <code class = "inline">coefficients[i] * components[i]</code> where the <code class = "inline">coefficients</code> are returned from the transform and <code class = "inline">components</code> are an array of the components computed by <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">PCA()</a> analysis).
    Note that the mean of the dataset does not have to be computed since it is stored as an attribute of the models of <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">PCA()</a>.
    Returns the computed approximation image as a flattened array.
    <i>Hint:  see the discussion in <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html">DS Handbook 5.09 (PCA)</a> and in Lecture 21</i>.
            </ul>

    </ul>
    </p>

<br>
<p>
Let's run through a few examples.  We can start with our binary digits from
<a href="p10.html">Program 10</a>:

<pre><code class="blockcode">from sklearn import datasets
digits = datasets.load_digits()
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))
target = digits.target
print(f'The targets for the first 5 entries: {target[:5]}')
bin_dig, bin_tar = select_data(data,digits.target)
print(f'The targets for the first 5 binary entries: {bin_tar[:5]}')</code></pre>
which prints:
<pre><code class="datablock">pca = PCA().fit(digits.data)
The targets for the first 5 binary entries: (0, 1, 0, 1, 0)</code></pre>

<p>
Using our function from <a href="p10.html">Program 10</a>, we can select the binary digits and then fit the PCA model to those.  Our function returns the model as well as the transformed values:

<pre><code class="blockcode">import matplotlib.pyplot as plt
bin_data,bin_target = select_data(data,target)
bin_mod, bin_proj = run_pca(bin_data)
scatter = plt.scatter(bin_proj[:, 0], bin_proj[:, 1], c=bin_target, alpha=0.5,
            cmap=plt.cm.get_cmap('rainbow', 10))
plt.title('PCA for digits')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.legend(*scatter.legend_elements())
plt.show()</code></pre>

The resulting image shows two distinct clusters for the images labeled 0 and those labeled 1:

<p><img src="digits_bin_pca.png" height = 350>

<p>We can fit the model to all the digits:

<pre><code class="blockcode">all_mod, all_proj = run_pca(data)
scatter = plt.scatter(all_proj[:, 0], all_proj[:, 1], c=digits.target, alpha=0.5,
            cmap=plt.cm.get_cmap('rainbow', 10))
plt.title('PCA for digits')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.legend(*scatter.legend_elements())
plt.show()</code></pre>

When looking at the complete data set, the projection to the first coordinates has some grouping of the images by labels:

<p><img src="digits_all_pca.png" height = 400>


<p>Plotting the explained variance ratio of our model:

<pre><code class="blockcode">import numpy as np
plt.plot(np.cumsum(all_mod.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.show()
</code></pre>
The resulting image shows that much of the variance is captured by the first 10 components:

<p><img src="digits_all_pca_ev_ratio.png" height = 400>

<p>We can use our functions to look at ways to measure this, via the captured variance and average eigenvalues:
<pre><code class="blockcode">np.set_printoptions(suppress=True)  #Turn off scientific notation
print(f'The explained variance: {all_mod.explained_variance_}')
print(f'The number of components needed to capture 85% of the variance is {capture_85(all_mod)}.')
print(f'The number of eigenvalues larger than the average is {average_eigenvalue(all_mod)}.')
</code></pre>

prints out the explained variance as well as the numbers computed by our functions:
<pre><code class="datablock">The explained variance: [179.0069301  163.71774688 141.78843909 101.1003752   69.51316559
  59.10852489  51.88453911  44.01510667  40.31099529  37.0117984
  28.51904118  27.32116981  21.90148814  21.32435654  17.63672222
  16.94686385  15.85138991  15.00446022  12.23447318  10.88685932
  10.69356625   9.58259779   9.2264026    8.69036872   8.3656119
   7.16577961   6.91973881   6.19295508   5.88499123   5.1558669
   4.49129656   4.24687799   4.04743883   3.94340334   3.70647245
   3.53165306   3.08457409   2.73780002   2.67210896   2.54170563
   2.28298744   1.90724229   1.81716569   1.68996439   1.4019722
   1.29221888   1.15893419   0.93122001   0.66985059   0.48606522
   0.25235043   0.09915279   0.06313078   0.06073776   0.03966623
   0.01495056   0.00847307   0.00362366   0.00127705   0.00066127
   0.00041222   0.           0.           0.        ]
The number of components needed to capture 85% of the variance is 5.
The number of eigenvalues larger than the average is 14.</code></pre>

<p> Lastly, let's look at how many components are needed to recover a given image:
<pre><code class="blockcode">plt.imshow(all_mod.mean_.reshape(8, 8), cmap='binary', interpolation='nearest', clim=(0, 16))
plt.title("Mean for digits")
plt.show()

print(f'The original image: {data[1068]}.')

approx_answer = approx_digits(all_mod, all_proj[1068], numComponents=4)
print(f'The first 4 components: {approx_answer}')
plt.imshow(approx_answer.reshape(8, 8), cmap='binary', interpolation='nearest', clim=(0, 16))
plt.title("mean + 4 components for digits[1068]")
plt.show()

approx_answer = approx_digits(all_mod, all_proj[1068], numComponents=8)
print(f'The first 8 components: {approx_answer}')
plt.imshow(approx_answer.reshape(8, 8), cmap='binary', interpolation='nearest', clim=(0, 16))
plt.title("mean + 8 components for digits[1068]")
plt.show()

approx_answer = approx_digits(all_mod, all_proj[1068], numComponents=12)
print(f'The first 12 components: {approx_answer}')
plt.imshow(approx_answer.reshape(8, 8), cmap='binary', interpolation='nearest', clim=(0, 16))
plt.title("mean + 12 components for digits[1068]")
plt.show()</code></pre>

Below we have the mean of all the images and the result of adding the first 4 components,
the first 8 components, and the first 12 components for <code class=inline>data[1068]</code>:

    <p>
    <img src="digits_mean.png" height=150>
    <img src="digits_1068_4.png" height=150>
    <img src="digits_1068_8.png" height=150>
    <img src="digits_1068_12.png" height=150>
</p>




</div>
</body>
</html>
