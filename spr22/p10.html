<!DOCTYPE doctype PUBLIC "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
  <meta http-equiv="Content-Type"
 content="text/html; charset=iso-8859-1">
  <title>Program 10, CSci 39542: Data Science, Hunter College</title>
</head>
<STYLE>A {text-decoration: none;}
th, td { padding: 5px; }
code {
  background-color: #eeeeee;
}
.inline {
  padding: 1px;
}
.blockcode {
  border: 1px solid #999999;
  display: block;
  padding-left: 10px;
  padding-top : 2px;
  padding-bottom : 2px;
  margin: 5px;
}
.datablock {
  border: 1px solid #eeeeee;
  display: block;
  padding: 7px;
  padding-top : 0px;
  margin: 5px;
}
</STYLE>
<body>


<div style="margin: 15px;width=100%;">
    <span style= "float: left;font-size:larger"><a href="index.html">CSci 39542</a></span>
    <span style= "float: right">
      <a href="syl.html">Syllabus</a>&nbsp;&nbsp;&nbsp;
      <a href="resources.html">Resources</a>&nbsp;&nbsp;&nbsp;
      <a href="work.html">Coursework</a><!--&nbsp;&nbsp;&nbsp;
      <a href="faq.html">FAQ</a>-->
    </span>
</div>

<br>
<br>
<hr>

<div style="margin:50px">


<h2>Program 10:
  <br>CSci 39542: Introduction to Data Science<br>
<a href="http://www.hunter.cuny.edu/csci">Department of Computer Science</a><br>
<a href="https://hunter.cuny.edu">Hunter College</a>, <a href="https://www.cuny.edu">City University of New York</a><br>
Spring 2022<br><br>
</h2>


<hr>
<a href="work.html#cw">Classwork</a>&nbsp;&nbsp;&nbsp;
<a href="work.html#quizzes">Quizzes</a>&nbsp;&nbsp;&nbsp;
<a href="work.html#hw">Homework</a>&nbsp;&nbsp;&nbsp;
<a href="work.html#project">Project</a>&nbsp;&nbsp;&nbsp;
<hr>

<object width=100% height=50% type="text/html" data="generalNotes.html" border="0"
   style="overflow: hidden;">
</object>

<hr>

<h2>Program Description</h2>

<br>
   <p><a name="p10"><b>Program 10: Classifying Digits.</b></b> &emsp; <i>Due noon, Thursday, 14 April.
     <br>Learning Objective: to enhance model building and comparison skills, using standard packages.
     <br>Available Libraries: pandas, numpy, pickle, sklearn, and core Python 3.6+.
     <br>Data Sources: <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset of hand-written digits</a>, available in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">sklearn digits dataset</a>.
     <br>Sample Datasets: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">sklearn digits dataset</a>.<br></i>

<p>
This program uses the canonical <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset of hand-written digits</a> discussed in Lecture #18 and available in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">sklearn digits dataset</a>:
      <br>
<a href="https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py">
  <img src="../fall21/sklearn_digits.png" height=400>
</a>

<br>
The dataset has 1797 scans of hand-written digits.
Each entry has the digit represented (<code class=inline>target</code>) as well as the 64 values representing the gray scale for the 8 x 8 image.  The first 5 entries are:
<br>
<img src = "../fall21/mnist_first5.png", height = 100>
<br>
The gray scales for the first 5 entries, flattened to one dimensional array:
<pre><code class="datablock">[[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3. 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]
[ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.  3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16. 16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  0. 11. 16. 10.  0.  0.]
[ 0.  0.  0.  4. 15. 12.  0.  0.  0.  0.  3. 16. 15. 14.  0.  0.  0.  0.  8. 13.  8. 16.  0.  0.  0.  0.  1.  6. 15. 11.  0.  0.  0.  1.  8. 13. 15.  1.  0.  0.  0.  9. 16. 16.  5.  0.  0.  0.  0.  3. 13. 16. 16. 11.  5.  0.  0.  0.  0.  3. 11. 16.  9.  0.]
[ 0.  0.  7. 15. 13.  1.  0.  0.  0.  8. 13.  6. 15.  4.  0.  0.  0.  2.  1. 13. 13.  0.  0.  0.  0.  0.  2. 15. 11.  1.  0.  0.  0.  0.  0.  1. 12. 12.  1.  0.  0.  0.  0.  0.  1. 10.  8.  0.  0.  0.  8.  4.  5. 14.  9.  0.  0.  0.  7. 13. 13.  9.  0.  0.]
[ 0.  0.  0.  1. 11.  0.  0.  0.  0.  0.  0.  7.  8.  0.  0.  0.  0.  0.  1. 13.  6.  2.  2.  0.  0.  0.  7. 15.  0.  9.  8.  0.  0.  5. 16. 10.  0. 16.  6.  0.  0.  4. 15. 16. 13. 16.  1.  0.  0.  0.  0.  3. 15. 10.  0.  0.  0.  0.  0.  2. 16.  4.  0.  0.]]</code></pre>

<p>Our goal is to predict what number is represented by a vector in the data set.  For example, the last line contains a handwritten number '4'.   Each entry in the dataset is labeled by the number represented in its gray scale images.  The labels ranges from 0 to 9.
We will first build binary classifers for the data when restricted to entries whose are labeled 0 or 1, and then classify more diverse subsets.
<br>To start, we will focus on entries that represent 0's and 1's.  The first 10 from the dataset are displayed below:
<br>
  <img src = "../fall21/mnist_binary10.png", height = 175>
<br>
Restricting to just 0's and 1's allows us to build binary classifiers:  those distinguishing between two classes.  This program employs some of the canonical techiques implemented in <a href="">sci-kit learn</a>: logistic regression, naive Bayes, support vector machines, and random forests.  We will then extend our classifications to larger sets.
The function specifications are below:

<ul>
    <li> <code class = "inline">select_data(data, target, labels = [0,1]):</code>

      This function takes as three input parameters:
      <ul>
        <li> <code class = "inline">data</code>: a numpy array that
              includes rows of equal size flattened arrays,
        <li> <code class = "inline">target</code> a numpy array that contains the labels for each row in <code class = "inline">data</code>.
        <li> <code class = "inline">labels</code>: the labels from <code class = "inline">target</code> that the rows to be selected.  The default value is [0,1].
      </ul>
      Returns the rows of <code class = "inline">data</code> and
      <code class = "inline">target</code> where the value of <code class = "inline">target</code> is in <code class = "inline">labels</code>.
      <i>Hint:  run through the examples at the end of this program description to see ways to work with this dataset.</i>

    <li> <code class = "inline">split_data(data, target, test_size = 0.25, random_state = 21):</code>
        This function has four inputs:
        <ul>
            <li> <code class = "inline">data</code>: a numpy array that
                  includes rows of equal size flattened arrays,
            <li> <code class = "inline">target</code> a numpy array corresponding to the rows of <code class = "inline">data</code>.
            <li> <code class = "inline">test_size</code>: the size of the test set created when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">0.25</code>.
            <li> <code class = "inline">random_state</code>: the random seed used when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">21</code>.
        </ul>
        Returns the data split into 4 subsets, corresponding to those returned by <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>:
        <code class = "inline">x_train</code>,
        <code class = "inline">x_test</code>,
        <code class = "inline">y_train</code>, and
        <code class = "inline">y_test</code>.
        To ensure good representation across the classes, the <code class = "inline">stratify</code> parameter should be used with the data labels (i.e. <code class = "inline">stratify=target</code> included in the call to <code class = "inline">train_test_split</code>).

    <li> <code class = "inline">fit_model(x_train, y_train, model_type='logreg'):</code>
    This function takes four input parameters:
      <ul>
        <li> <code class = "inline">x_train</code>: the independent variable(s) for the analysis.
        <li> <code class = "inline">y_train</code>: the dependent variable for the analysis.
        <li> <code class = "inline">model_type</code>: the type of model to use.  Possible values are 'logreg', 'svm', 'nbayes', and 'rforest'.  See below for the specified parameters for each model.  The default value for this parameter is 'logreg'.

      </ul>
      Fits the specifed model to the <code class = "inline">x_train</code> and
      <code class = "inline">y_train</code> data, using <a href="https://scikit-learn.org">sklearn</a>.  Additional notes for each model:
      <ul>
          <li> <code class = "inline">logreg</code>: Logistic Regression:  For logistic regression, use the SVM classifier to set up the model, <code class = "inline">sklearn.linear_model.LogisticRegression</code> with solver <code class = "inline">solver = 'saga'</code>, regularization <code class = "inline">penalty='l2'</code>, and max iterations <code class = "inline">max_iter=5000</code> (Note that it's the letter L in 'l2', not a 1.).
          <li> <code class = "inline">nbayes</code>: Naive Bayes:  use the Gaussian Naive Bayes classifier to set up the model, <code class = "inline">sklearn.naive_bayes.GaussianNB</code>.
          <li> <code class = "inline">svm</code>: Support Vector Machine:  use the SVM classifier to set up the model, <code class = "inline">sklearn.svm.SVC</code> with the radial basis function kernel RBF <code class = "inline">kernel='rbf'</code>.
          <li> <code class = "inline">rforest</code>: Random Forest: use the random forest classifier to set up the model, <code class = "inline">sklearn.ensemble.RandomForestClassifier</code> with 100 estimators and the random state set to 0  (i.e. <code class = "inline">n_estimators=100, random_state=0</code>).
      </ul>
      The resulting model should be returned as bytestream, using <a href="https://docs.python.org/3/library/pickle.html">pickle</a>.
      <i>Hint: for more details on setting up each of these models, see Lecture 18 and the associated notebooks and reading.</i>
  <li> <code class = "inline">predict_model(mod_pkl, xes):</code>
      This function takes two input parameters:
          <ul>
              <li> <code class = "inline">mod_pkl</code>: a object serialization of a trained model (i.e. a pickled bytestream).  The possible model approaches are logistic regression, support vector machine, naive Bayes, and random forest.
            <li> <code class = "inline">xes</code>: the independent variable(s) for the analysis with the same dimensions as which the model was trained.
          </ul>
          Returns the values that the model predicts for the inputted independent variables (that is, the "y_estimate" gives the <code class = "inline">xes</code>).

  <li> <code class = "inline">score_model(mod_pkl,xes,yes):</code>
      This function takes three input parameters:
          <ul>
              <li> <code class = "inline">mod_pkl</code>: a object serialization of a trained model.  The possible model approaches are logistic regression, support vector machine, naive Bayes, and random forest.
              <li> <code class = "inline">xes</code>: the independent variable(s) for the analysis with the same dimensions as which the model was trained.
              <li> <code class = "inline">yes</code>: the dependent variable(s) for the analysis with the same dimensions as which the model was trained.
          </ul>
          Returns the confusion matrix for the model.

    <li> <code class = "inline">compare_models(data, target, test_size = 0.25, random_state = 21, models = ['logreg','nbayes','svm','rforest']):</code>
        This function has five inputs:
        <ul>
            <li> <code class = "inline">data</code>: a numpy array that
                  includes rows of equal size flattened arrays,
            <li> <code class = "inline">target</code> a numpy array that takes values 0 or 1 corresponding to the rows of <code class = "inline">data</code>.
            <li> <code class = "inline">test_size</code>: the size of the test set created when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">0.25</code>.
            <li> <code class = "inline">random_state</code>: the random seed used when the data is divided into test and training sets with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>. The default value is <code class = "inline">21</code>.
            <li> <code class = "inline">models</code>: a list of names of models that <code class = "inline">fit_model</code> accepts.  The default value is <code class = "inline">['logreg','nbayes','svm','rforest']</code>.
        </ul>
        This function calls <code class = "inline">split_data</code> with the first four parameters to create four subsets:
        <code class = "inline">x_train</code>,
        <code class = "inline">x_test</code>,
        <code class = "inline">y_train</code>, and
        <code class = "inline">y_test</code>.
        For each of the specified models in <code class = "inline">models</code>, calls the function above to fit each model to the training data, and computes the accuracy (which can be computed from the sum of the diagonal of the confusion matrix, or more simply using the <code class = "inline">score()</code> function of each model).  The function returns the name of the model with highest accuracy score and its accuracy score.  In case of ties for the best score, return the first one that has that value.

      </ul>



<p>
For the examples, we first load in the digits dataset from sklearn:
<pre><code class="blockcode">#Using the digits data set from sklearn:
from sklearn import datasets
digits = datasets.load_digits()
print(digits.target)
print(type(digits.target), type(digits.data))
</code></pre>

As we saw in lecture, the data set is labeled with the digit represented and the types of
these labels and the data is numpy arrays:
<pre><code class="datablock">[0 1 2 ... 8 9 8]
&lt;class 'numpy.ndarray'&gt; &lt;class 'numpy.ndarray'&gt;
</code></pre>

Let's flatten the entries, using the numpy's reshape function:
<pre><code class="blockcode">n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))
print(f'The labels for the first 5 entries: {digits.target[:5]}')
print(data[0:5])
</code></pre>

The labels of the first five elements in our dataset and their flattened representation:
<pre><code class="datablock">The targets for the first 5 entries: [0 1 2 3 4]
[[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.
  15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.
   0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.
   0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]
 [ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.
   3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16.
  16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.
   0.  0.  0.  0.  0. 11. 16. 10.  0.  0.]
 [ 0.  0.  0.  4. 15. 12.  0.  0.  0.  0.  3. 16. 15. 14.  0.  0.  0.  0.
   8. 13.  8. 16.  0.  0.  0.  0.  1.  6. 15. 11.  0.  0.  0.  1.  8. 13.
  15.  1.  0.  0.  0.  9. 16. 16.  5.  0.  0.  0.  0.  3. 13. 16. 16. 11.
   5.  0.  0.  0.  0.  3. 11. 16.  9.  0.]
 [ 0.  0.  7. 15. 13.  1.  0.  0.  0.  8. 13.  6. 15.  4.  0.  0.  0.  2.
   1. 13. 13.  0.  0.  0.  0.  0.  2. 15. 11.  1.  0.  0.  0.  0.  0.  1.
  12. 12.  1.  0.  0.  0.  0.  0.  1. 10.  8.  0.  0.  0.  8.  4.  5. 14.
   9.  0.  0.  0.  7. 13. 13.  9.  0.  0.]
 [ 0.  0.  0.  1. 11.  0.  0.  0.  0.  0.  0.  7.  8.  0.  0.  0.  0.  0.
   1. 13.  6.  2.  2.  0.  0.  0.  7. 15.  0.  9.  8.  0.  0.  5. 16. 10.
   0. 16.  6.  0.  0.  4. 15. 16. 13. 16.  1.  0.  0.  0.  0.  3. 15. 10.
   0.  0.  0.  0.  0.  2. 16.  4.  0.  0.]]</code></pre>

We can restrict the dataset to just binary digits:
<pre><code class="blockcode">binaryDigits = [(d,t) for (d,t) in zip(data, target) if t <= 1]
bd,bt = zip(*binaryDigits)
print(f'The targets for the first 5 binary entries: {bt[:5]}')</code></pre>
which will print out the labels for the first 5 binary entries:
<pre><code class="datablock">The targets for the first 5 binary entries: (0, 1, 0, 1, 0)</code></pre>

Let's do the same with our function:
<pre><code class="blockcode">bin_dig, bin_tar = select_data(data,digits.target)
print(f'The targets for the first 5 binary entries: {bin_tar[:5]}')</code></pre>
which will print out the labels for the first 5 binary entries:
<pre><code class="datablock">The targets for the first 5 binary entries: (0, 1, 0, 1, 0)</code></pre>

We can also select for other sets of labels:
<pre><code class="blockcode">#Selecting on 6's and 7's:
dig67, tar67 = select_data(data, digits.target, labels=[6, 7])
print(f"The targets for the first 5 6's & 7's entries: {tar67[:5]}")
#Selecting on evens:
dig_even, tar_even = select_data(data, digits.target,labels=[0, 2, 4, 6, 8])
print(f"The targets for the first 5 even entries: {tar_even[:5]}")</code></pre>
which will print:
<pre><code class="datablock">The targets for the first 5 6's & 7's entries: (6, 7, 6, 7, 6)
The targets for the first 5 even entries: (0, 2, 4, 6, 8)</code></pre>

<p>Using our functions to restrict the data and targets datasets to 0's and 1's, we can split the data and fit and test the various models:
<pre><code class="blockcode">x_train, x_test, y_train, y_test = split_data(bin_dig, bin_tar, test_size=0.5)
log_pkl = fit_model(x_train, y_train)
y_predict = predict_model(log_pkl, x_train)
log_cmatrix = score_model(log_pkl, x_test, y_test)
print(f'prediction: y_predict\nconfusion matrix:\n log_cmatrix')</code></pre>

will print:
<pre><code class="datablock">prediction: [1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1]
confusion matrix:
[[89  0]
 [ 1 90]]</code></pre>

The logistic regression model does extremely well, making only one wrong prediction.
Let's see how each of the other models does with the same training and testing subsets:

<pre><code class="blockcode">for m in ['nbayes','svm','rforest']:
    log_pkl = fit_model(x_train,y_train, model_type=m)
    log_cmatrix = score_model(log_pkl,x_test,y_test)
    print(f'The confusion matrix for {m} is:\n {log_cmatrix}')
</code></pre>

will print:
<pre><code class="datablock">The confusion matrix for nbayes is:
 [[88  1]
 [ 1 90]]
The confusion matrix for svm is:
 [[89  0]
 [ 0 91]]
The confusion matrix for rforest is:
 [[89  0]
 [ 0 91]]</code></pre>

 <p>All of these models do very well also.  Lowering the training set to just 10% of the available input data, we can see which model does the best with for a dataset of those with labels 6 and 7 (in case of ties, return the first one that has that value).

<pre><code class="blockcode">best_mod, best_score = compare_models(dig67,tar67,test_size=0.9,random_state=22)
print(f"The best model for the 6 and 7's dataset is {best_mod} with score {best_score}.")
</code></pre>

will print:
<pre><code class="datablock">The best model for the 6 and 7's dataset is logreg with score 1.0.</code></pre>

While we have focused on binary datasets, the classifiers from sklearn can also be trained for multiclass datasets.  Let's try the classifiers on all digits and a training set of 10% of the data:

<pre><code class="blockcode">best_mod, best_score = compare_models(data,digits.target,test_size=0.9,random_state=22)
print(f"The best model for the 6 and 7's dataset is {best_mod} with score {best_score}.")
</code></pre>

The SVM classifer did the best with:

<pre><code class="datablock">The best model for the full dataset is svm with score 0.9375772558714462.</code></pre>



</div>
</body>
</html>
